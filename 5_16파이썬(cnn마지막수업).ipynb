{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_16파이썬(cnn마지막수업).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2sOScrNgGG+5MWQx68e1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/5_16%ED%8C%8C%EC%9D%B4%EC%8D%AC(cnn%EB%A7%88%EC%A7%80%EB%A7%89%EC%88%98%EC%97%85).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLU5ThrWfCJ"
      },
      "outputs": [],
      "source": [
        "2022.05.16\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()\n",
        "\n",
        "x_train.shape\n",
        "y_train.shape\n",
        "x_test.shape\n",
        "y_test.shape\n",
        "\n",
        "image = x_train[0]\n",
        "image.shape\n",
        "label = y_train[0]\n",
        "\n",
        "plt.imshow(image,'gray')\n",
        "plt.title(label)\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten # flatten 다차원배열을 1차원으로 평탄화해주는 함수\n",
        "from tensorflow.keras.models import Sequential, load_model # 딥러닝 모델생성, 저장한 모델 불러오기\n",
        "from tensorflow.keras.optimizers import Adam # 옵티마이저(최적화)\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy # 다중분류 손실함수\n",
        "from tensorflow.keras.utils import to_categorical # one hot encoding 변환하는데 사용되는 함수\n",
        "\n",
        "다중분류에서 정답을 one hot encoding 변환해야한다.\n",
        "\n",
        "to_categorical(0,10)\n",
        "0 -> array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n",
        "\n",
        "train_ohe = to_categorical(y_train) # 정답 레이블을 one hot encoding 변환\n",
        "test_ohe = to_categorical(y_test)# 정답 레이블을 one hot encoding 변환\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능\n",
        "history = model.fit(x=x_train,y=train_ohe, batch_size=32,epochs=20)\n",
        "\n",
        "x_train.shape\n",
        "# 1 epochs 시에 전체 데이터 60000 데이터를 32개씩 총 1875번 메모리에 올려서 미분을 구한다.\n",
        "# batch_size는 보편적으로 작게한다. epochs도 어느정도 필요(아주조금할경우 underfitting발생가능)\n",
        "\n",
        "print(history.history['loss']) # 시각화할 때 그래프로 loss값이 어떻게 바뀌었는지 표현\n",
        "print(history.history['accuracy']) # 시각화할 때 그래프로 loss값이 어떻게 바뀌었는지 표현\n",
        "\n",
        "# 테스트 데이터를 이용해서 모델 성능 검증\n",
        "score = model.evaluate(x_test,test_ohe,batch_size=64)\n",
        "print('loss: ',score[0])\n",
        "print('accuracy : ',score[1])\n",
        "\n",
        "# 모델 저장\n",
        "model.save('c:/data/my_mnist_model.h5')\n",
        "\n",
        "# 모델 불러오기\n",
        "from tensorflow.keras.models import Sequential, load_model # 딥러닝 모델생성, 저장한 모델 불러오기\n",
        "new_model = load_model('c:/data/my_mnist_model.h5')\n",
        "new_model.summary()\n",
        "\n",
        "score = new_model.evaluate(x_test,test_ohe,batch_size=64)\n",
        "print('loss: ',score[0])\n",
        "print('accuracy : ',score[1])\n",
        "\n",
        "plt.imshow(x_test[0],'gray')\n",
        "\n",
        "# 직접 그림판에서 만든 숫자 적용\n",
        "(base) C:\\WINDOWS\\system32>pip install opencv-python\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread(\"c:/data/3.png\",cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(255-img,'gray') # 색상이 반대로 나와서 255를 빼주기\n",
        "\n",
        "img_3 = cv2.resize(255-img,(28,28)) \n",
        "plt.imshow(img_3,'gray')\n",
        "\n",
        "img_3.shape\n",
        "28,28 -> 1,28,28,1\n",
        "predict = new_model.predict(img_3.reshape(-1,28,28,1)) # shape 조절 후 예측\n",
        "import numpy as np\n",
        "np.argmax(predict) # 예측된 값\n",
        "\n",
        "\n",
        "img = cv2.imread(\"c:/data/0.png\",cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(255-img,'gray') # 색상이 반대로 나와서 255를 빼주기\n",
        "\n",
        "img_0 = cv2.resize(255-img,(28,28)) \n",
        "plt.imshow(img_3,'gray')\n",
        "\n",
        "predict = new_model.predict(img_0.reshape(-1,28,28,1)) # shape 조절 후 예측\n",
        "np.argmax(predict) # 예측된 값\n",
        "\n",
        "\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()\n",
        "# 검증 데이터를 이용해서 학습 수행\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x,valid_x,train_y,valid_y = train_test_split(x_train,y_train,test_size=0.15)\n",
        "train_x.shape\n",
        "train_y.shape\n",
        "valid_x.shape\n",
        "valid_y.shape\n",
        "\n",
        "train_ohe = to_categorical(train_y)\n",
        "valid_ohe = to_categorical(valid_y)\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능\n",
        "history = model.fit(x=train_x,y=train_ohe,validation_data = (valid_x,valid_ohe),batch_size=32,epochs=20)\n",
        "\n",
        "test_ohe = to_categorical(y_test)\n",
        "model.evaluate(x_test,test_ohe,batch_size=32) # 테스트\n",
        "\n",
        "img = cv2.imread(\"c:/data/0.png\",cv2.IMREAD_GRAYSCALE) # 새 이미지로 테스트\n",
        "img_3 = cv2.resize(255-img,(28,28)) \n",
        "plt.imshow(img_3,'gray')\n",
        "img_3.shape\n",
        "predict = model.predict(img_3.reshape(-1,28,28,1)) # shape 조절 후 예측\n",
        "np.argmax(predict) # 예측된 값\n",
        "\n",
        "\n",
        "callback\n",
        "1. ModelCheckpoint\n",
        "- 특정 조건에 맞춰서 모델의 weight값을 파일로 저장\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "mcp_cb = ModelCheckpoint(filepath='c:/data/mnist/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "                monitor='val_loss',mode='min',\n",
        "                save_best_only=True,save_weights_only=True,period=3,verbose=1)  # val_accuracy는 max로, verbose=1은 화면에 출력해주는 것, period=3 세번마다 체크해서 기존 loss값보다 줄었으면 weight값을 저장하는 것(모델이 개선된 validation score를 도출해낼 때마다 weight를 중간 저장)\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능, Adam(학습률) 학습률(learning rate)에 따라 효율성이 달라질 수 있음 .\n",
        "history = model.fit(x=train_x,y=train_ohe,validation_data = (valid_x,valid_ohe),batch_size=32,epochs=20,\n",
        "                    callbacks=[mcp_cb])\n",
        "\n",
        "\n",
        "# 기존에 만들어놓은 weight 값을 새로운 모델에 적용하기\n",
        "    # 모델\n",
        "input_size = 28\n",
        "mnist_model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "mnist_model.summary()\n",
        "mnist_model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능\n",
        "\n",
        "    # 만들어놓은 weight 값을 불러와서 새로운 모델에 적용하기\n",
        "mnist_model.load_weights(\"c:/data/mnist/weights.15-0.17.hdf5\")\n",
        "mnist_model.evaluate(x_test,test_ohe,batch_size=32) # 테스트\n",
        "\n",
        "\n",
        "2.ReduceLROnPlateau\n",
        "epochs 횟수 동안 성능이 개선되지 않을 경우 learning rate 을 동적으로 감소시키는 기능\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "rlp_cb = ReduceLROnPlateau(monitor='val_loss',mode='min',patience=1,factor=0.3,verbose=1) # patience=1은 1번씩 체크, factor값은 learning rate를 줄이는 값(learning rate * factor), 대부분 검증데이터(valid)를 가지고 loss를 봄\n",
        "learning_rate = learning_rate * factor\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능, Adam(학습률) 학습률(learning rate)에 따라 효율성이 달라질 수 있음 .\n",
        "history = model.fit(x=train_x,y=train_ohe,validation_data = (valid_x,valid_ohe),batch_size=32,epochs=20,\n",
        "                    callbacks=[rlp_cb])\n",
        "\n",
        "# 콜백했을 때랑 안 했을 때랑 비교해보기!!!!\n",
        "\n",
        "3.EarlyStopping\n",
        "- epochs 동안 성능이 개선되지 않은 경우 학습을 조기에 중단하는 방법\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "es_cb = EarlyStopping(monitor='val_loss',mode='min',patience=1,verbose=1)\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능, Adam(학습률) 학습률(learning rate)에 따라 효율성이 달라질 수 있음 .\n",
        "history = model.fit(x=train_x,y=train_ohe,validation_data = (valid_x,valid_ohe),batch_size=32,epochs=20,\n",
        "                    callbacks=[es_cb])\n",
        "\n",
        "\n",
        "4. 위의 3가지를 한꺼번에 수행\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "mcp_cb = ModelCheckpoint(filepath='c:/data/mnist/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "                monitor='val_loss',mode='min',\n",
        "                save_best_only=True,save_weights_only=True,period=3,verbose=1)  # val_accuracy는 max로, verbose=1은 화면에 출력해주는 것, period=3 세번마다 체크해서 기존 loss값보다 줄었으면 weight값을 저장하는 것(모델이 개선된 validation score를 도출해낼 때마다 weight를 중간 저장)\n",
        "es_cb = EarlyStopping(monitor='val_loss',mode='min',patience=1,verbose=1)\n",
        "rlp_cb = ReduceLROnPlateau(monitor='val_loss',mode='min',patience=1,factor=0.3,verbose=1) # patience=1은 1번씩 체크, factor값은 learning rate를 줄이는 값(learning rate * factor), 대부분 검증데이터(valid)를 가지고 loss를 봄\n",
        "\n",
        "input_size = 28\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[input_size,input_size]),\n",
        "    Dense(100,activation='relu'),\n",
        "    Dense(30,activation='relu'),\n",
        "    Dense(10,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능, Adam(학습률) 학습률(learning rate)에 따라 효율성이 달라질 수 있음 .\n",
        "history = model.fit(x=train_x,y=train_ohe,validation_data = (valid_x,valid_ohe),batch_size=32,epochs=20,\n",
        "                    callbacks=[mcp_cb,rlp_cb,es_cb])\n",
        "\n",
        "#---------------------------------------------------------\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import os\n",
        "\n",
        "1. binary\n",
        "train_path = 'c:/data/cats_dogs/train'\n",
        "validation_path = 'c:/data/cats_dogs/validation'\n",
        "\n",
        "training_datagen = ImageDataGenerator( # 학습 ( 학습하기 위해서 사용되는 이미지의 다양한 변환을 적용 )\n",
        "    preprocessing_function=preprocess_input, # VGG16의 모델에서 사용한 스케일링 함수\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # 검증\n",
        "\n",
        "training_generator = training_datagen.flow_from_directory(train_path,\n",
        "                                     batch_size=32,\n",
        "                                     target_size=(128,128),\n",
        "                                     class_mode='categorical') # class_mode='binary' : 이진 분류(0,1),'sparse' : 레이블인코딩,'categorical' : 원-핫 인코딩\n",
        "\n",
        "training_generator.classes # 레이블 된 것을 보여줌\n",
        "training_generator.class_indices # 어떻게 레이블이 적용됐는지 딕셔너리로 보여줌\n",
        "\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_path, # flow_from_directory함수로 numpy array iterator 객체를 만들어줌(배치크기, 데이터셔플유무,image/label array등 설정해줌)\n",
        "                                     batch_size=32, # training이랑 틀려도 됨\n",
        "                                     shuffle=True, # 섞기\n",
        "                                     target_size=(128,128),\n",
        "                                     class_mode='categorical')\n",
        "\n",
        "validation_generator.classes # 레이블 된 것을 보여줌\n",
        "validation_generator.class_indices # 어떻게 레이블이 적용됐는지 딕셔너리로 보여줌\n",
        "\n",
        "#VGG16_MODEL = VGG16(input_shape=(224,224,3)) # 출력된 사이트에서 weight값을 가져오는거(vgg16)\n",
        "#VGG16_MODEL.summary()\n",
        "vgg_model = VGG16(input_shape=(128,128,3),include_top=False,weights='imagenet') # include_top=False : top부분은 빼고 수행,weights='imagenet' : weight값 그대로 사용하겠다.\n",
        "vgg_model.summary()\n",
        "vgg_model.trainable=False # weight값을 업데이트(학습)하지 않고 vgg16에서 쓰고있는 weight값을 바로 사용하겠다., 기본값은 True(학습하면서 업데이트)\n",
        "\n",
        "model = Sequential([\n",
        "    vgg_model,\n",
        "    Flatten(),\n",
        "    Dense(100,activation='relu'), # Dense층은 맘대로 설정 가능\n",
        "    Dense(2,activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "rlp_cb = ReduceLROnPlateau(monitor='val_loss',mode='min',patience=1,factor=0.3,verbose=1) # patience=1은 1번씩 체크, factor값은 learning rate를 줄이는 값(learning rate * factor), 대부분 검증데이터(valid)를 가지고 loss를 봄\n",
        "model.compile(optimizer=Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy']) # CategoricalCrossentropy 임포트 하는 것 대신 'categorical_crossentropy'로 대체가능, Adam(학습률) 학습률(learning rate)에 따라 효율성이 달라질 수 있음 .\n",
        "history = model.fit(training_generator,validation_data = validation_generator,batch_size=32,epochs=20,\n",
        "                    callbacks=[rlp_cb])\n",
        "\n",
        "model.save('c:/data/my_vgg_cat_dog_model.h5') # 모델 저장\n",
        "\n",
        "from tensorflow import keras\n",
        "new_model = keras.models.load_model('c:/data/my_vgg_cat_dog_model.h5') # 모델 불러오기\n",
        "new_model.summary()\n",
        "\n",
        "# 새 이미지 가져와서 테스트\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "img = load_img('c:/img/dog_1.jpg') \n",
        "x = img_to_array(img)\n",
        "x.shape\n",
        "x = tf.image.resize(x,[128,128])\n",
        "x.shape\n",
        "x = np.array([x])\n",
        "x.shape\n",
        "predict = model.predict(preprocess_input(x))\n",
        "list(training_generator.class_indices.keys())[np.argmax(predict)]\n",
        "\n",
        "predict = new_model.predict(preprocess_input(x))\n",
        "list(training_generator.class_indices.keys())[np.argmax(predict)]\n",
        "\n",
        "\n",
        "# 내가 만든 모델이 잘 안나올 때 vgg같은 모델들을 가져와서 적용한다\n",
        "# vgg16의 구성\n",
        "    - 13 Convolution Layers + 3 Fully-connected Layers\n",
        "    - 3x3 convolution filters\n",
        "    - stride: 1 & padding: 1\n",
        "    - 2x2 max pooling(stride:2)\n",
        "    - ReLU\n",
        "    \n",
        "    - 사용된 최적화 알고리즘\n",
        "        - optimizing multinomial logistic regression\n",
        "        - mini-batch gradient descent # 미니배치 경사하강법\n",
        "        - Momentum(0.9) # 옵티마이져\n",
        "        - Weight Decay(L2 Norm)\n",
        "        - Dropout(0.5) # 반절은 버리기\n",
        "        - Learning rate 0.01로 초기화 후 서서히 줄임\n",
        "    \n",
        "    - 가중치 초기화\n",
        "        - 상대적으로 얕은 11-Layer 네트워크를 우선적 학습, 가중치는 정규분포를 따르도록 임의의 값으로 초기화\n",
        "        - 어느정도 학습이 완료되면 입력층 부분의 4개 층과 마지막 3개의 fully-connected layer의 weight를 학습할 네트워크의 초기값으로 사용\n",
        "    \n",
        "    - 학습이미지 크기\n",
        "        - 입력이미지의 크기는 모두 224x224로 고정\n",
        "        - 학습 이미지는 각 이미지에 대해 256x256~512x512 내에서 임의의 크기로 변환하고, 크기가 변환된 이미지에서 개체(Object)의 일부가 포함된 224x224 이미지를 Crop하여 사용\n",
        "        "
      ]
    }
  ]
}