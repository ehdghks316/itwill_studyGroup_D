{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_9파이썬(tensorflow3,keras,다중분류,onehotencoding).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPexlDw7vr0d2+MVTze7fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/5_9%ED%8C%8C%EC%9D%B4%EC%8D%AC(tensorflow3%2Ckeras%2C%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98%2Conehotencoding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWPffjqLYDtV"
      },
      "outputs": [],
      "source": [
        "2022.5.9\n",
        "\n",
        "[문제] XOR Logistic regression 을 이용해서 분류해주세요. 단 신경망을 이용하세요.\n",
        "x1 x2 y\n",
        "0  0  0\n",
        "0  1  1\n",
        "1  0  1\n",
        "1  1  0\n",
        "\n",
        "y_hat = w1*x1 + w2 * w2+ b\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
        "y_data = [[0],[1],[1],[0]]\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([2,1]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rate=0.1 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.sigmoid(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis) + (1-y)*tf.math.log(1-hypothesis)))\n",
        "        w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "        w.assign_sub(learning_rate*w_grad)\n",
        "        b.assign_sub(learning_rate*b_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight:{}\\n bias:{}\\n coas:{}'.format(i,w.numpy(),b.numpy(),cost.numpy()))\n",
        "        \n",
        "\n",
        "# 여러 뉴런을 가진 은닉층 추가\n",
        "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
        "y_data = [[0],[1],[1],[0]]\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "\n",
        "# 은닉층\n",
        "w1 = tf.Variable(tf.random.normal([2,4]),name='weight') # 2는 입력층의 개수 4는 뉴런의 개수\n",
        "b1 = tf.Variable(tf.random.normal([4]),name='bias')\n",
        "\n",
        "# 출력층\n",
        "w2 = tf.Variable(tf.random.normal([4,1]),name='weight') # 4는 은닉층의 뉴런수 1은 출력\n",
        "b2 = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rate=0.1 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(10000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        layer1 = tf.nn.relu(tf.matmul(x,w1) + b1)\n",
        "        hypothesis = tf.sigmoid(tf.matmul(layer1,w2) + b2)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis) + (1-y)*tf.math.log(1-hypothesis))) # binary cross entropy\n",
        "        w1_grad,w2_grad,b1_grad,b2_grad = tape.gradient(cost,[w1,w2,b1,b2])\n",
        "        w1.assign_sub(learning_rate*w1_grad)\n",
        "        b1.assign_sub(learning_rate*b1_grad)\n",
        "        w2.assign_sub(learning_rate*w2_grad)\n",
        "        b2.assign_sub(learning_rate*b2_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight2:{}\\n bias2:{}\\n cost:{}'.format(i,w2.numpy(),b2.numpy(),cost.numpy()))\n",
        "        \n",
        "hypothesis > 0.5\n",
        "\n",
        "predict = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = np.mean(predict == y)\n",
        "\n",
        "\n",
        "# keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,input_dim=2,activation='relu')) # layer1 / 8은 뉴런, input_dim은 x_data의 리스트의 한 값당 2열, activation은 relu를 사용 \n",
        "model.add(Dense(4,activation='relu')) # layer2 / 위에서 input_dim을 지정하면 다음 은닉층에서는 사용안해도 됨\n",
        "model.add(Dense(1,activation='sigmoid')) # 출력층\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
        "model.fit(x,y,epochs=1000) # 모델에 적용\n",
        "\n",
        "y_predict = model.predict(x) > 0.5\n",
        "y_predict.astype(np.int32) # bool 형식 int로\n",
        "\n",
        "model.evaluate(x,y) # 모델 평가()\n",
        "\n",
        "\n",
        "★ multinomial classification\n",
        "\n",
        "softmax function\n",
        "- 0 ~ 1 사이의 값으로 출력된다.\n",
        "- 확률값으로 출력\n",
        "- 모든 출력의 합은 반드시 1이 되어야 한다\n",
        "\n",
        "x = 100\n",
        "np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "target = [0,0,1]\n",
        "y_hat = [0.6,0.1,0.3]\n",
        "y_hat = [0.1,0.1,0.8]\n",
        "\n",
        "-(0*np.log(0.6) + 0*np.log(0.1) + 1*np.log(0.3))\n",
        "\n",
        "-(0*np.log(0.1) + 0*np.log(0.1) + 1*np.log(0.8))\n",
        "\n",
        "\n",
        "- 다중분류 cost function\n",
        "cross entropy\n",
        "\n",
        "-np.sum(y*np.log(x))\n",
        "\n",
        "def cross_entropy_function(x,y):\n",
        "    delta = 1e-7\n",
        "    return -np.sum(y*np.log(x) + delta)\n",
        "\n",
        "target = [0,0,1]\n",
        "y_hat = [0.6,0.1,0.3]\n",
        "cross_entropy_function(np.array(y_hat),np.array(target))\n",
        "\n",
        "target = [0,0,1]\n",
        "y_hat = [0.01,0.01,0.98]\n",
        "cross_entropy_function(np.array(y_hat),np.array(target))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "\n",
        "x_data = iris.iloc[:,0:-1] # 독립변수, 입력변수, 설명변수 : 수치형\n",
        "y_data = iris.Name # 종속변수, 결과변수, 목표변수, : 범주형 -> 수치형(one hot encoding)\n",
        "y_data.unique()\n",
        "\n",
        "labels = {\n",
        "'Iris-setosa' : [1,0,0],\n",
        "'Iris-versicolor' : [0,1,0], \n",
        "'Iris-virginica': [0,0,1]}\n",
        "\n",
        "labels[y_data[0]] 'Iris-setosa' -> [1,0,0]\n",
        "labels[y_data[1]] \n",
        "\n",
        "# onehotencoding\n",
        "'''\n",
        "1.\n",
        "y_data.map({'Iris-setosa' : [1,0,0],\n",
        "'Iris-versicolor' : [0,1,0], \n",
        "'Iris-virginica': [0,0,1]})\n",
        "2.\n",
        "list(map(lambda v : labels[v],y_data))\n",
        "3.\n",
        "pd.get_dummies(y_data)\n",
        "4.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one = OneHotEncoder()\n",
        "one2 = one.fit_transform(y_data)\n",
        "'''\n",
        "y_one_hot = list(map(lambda v : labels[v],y_data)) # one hot encoding\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_one_hot,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([4,3]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([3]),name='bias')\n",
        "\n",
        "learning_rate=0.01 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.nn.softmax(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis),axis=1)) # categorical_crossentropy\n",
        "        w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "        w.assign_sub(learning_rate*w_grad)\n",
        "        b.assign_sub(learning_rate*b_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight2:{}\\n bias2:{}\\n cost:{}'.format(i,w.numpy(),b.numpy(),cost.numpy()))\n",
        "\n",
        "np.mean(tf.argmax(hypothesis,axis=1) ==tf.argmax(y,axis=1))\n",
        "\n",
        "predict = tf.argmax(hypothesis,axis=1) # 예측\n",
        "real = tf.argmax(y,axis=1) # 실제\n",
        "\n",
        "tf.equal(predict,real)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict,real),dtype=tf.float32))\n",
        "accuracy.numpy()\n",
        "\n",
        "# weight값과 bias값을 구했으면 새로운 값을 입력해서 꽃 분류예측\n",
        "new = tf.constant(np.array([5.9,3.,5.1,1.8]),shape=(1,4),dtype=tf.float32)\n",
        "new_predict = tf.nn.softmax(tf.matmul(new,w) + b)\n",
        "np.argmax(new_predict)\n",
        "\n",
        "\n",
        "\n",
        "# onehot encoding 방법2\n",
        "\n",
        "import pandas as pd\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "\n",
        "x_data = iris.iloc[:,0:-1] # 독립변수, 입력변수, 설명변수 : 수치형\n",
        "# y_data = iris.Name # 종속변수, 결과변수, 목표변수, : 범주형 -> 수치형(one hot encoding)\n",
        "#y_data.unique()\n",
        "\n",
        "# 문자형 데이터를 수치형으로 변환\n",
        "iris.Name = iris.Name.map({'Iris-setosa' : 0,'Iris-versicolor' : 1, 'Iris-virginica': 2})\n",
        "iris.Name.unique()\n",
        "\n",
        "# 수치형 데이터를 one hot encoding 변환\n",
        "0 -> [1,0,0]\n",
        "1 -> [0,1,0]\n",
        "2 -> [0,0,1]\n",
        "\n",
        "y_one_hot = tf.one_hot(iris.Name,3) # tensor에서 제공해주는 one hot encoding\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_one_hot,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([4,3]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([3]),name='bias')\n",
        "\n",
        "learning_rate=0.01 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.nn.softmax(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis),axis=1))\n",
        "        w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "        w.assign_sub(learning_rate*w_grad)\n",
        "        b.assign_sub(learning_rate*b_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight2:{}\\n bias2:{}\\n cost:{}'.format(i,w.numpy(),b.numpy(),cost.numpy()))\n",
        "\n",
        "np.mean(tf.argmax(hypothesis,axis=1) ==tf.argmax(y,axis=1))\n",
        "\n",
        "predict = tf.argmax(hypothesis,axis=1) # 예측\n",
        "real = tf.argmax(y,axis=1) # 실제\n",
        "\n",
        "tf.equal(predict,real)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict,real),dtype=tf.float32))\n",
        "accuracy.numpy()\n",
        "\n",
        "# weight값과 bias값을 구했으면 새로운 값을 입력해서 꽃 분류예측\n",
        "new = tf.constant(np.array([5.9,3.,5.1,1.8]),shape=(1,4),dtype=tf.float32)\n",
        "new_predict = tf.nn.softmax(tf.matmul(new,w) + b)\n",
        "np.argmax(new_predict)\n",
        "\n",
        "\n",
        "# onehot encoding 방법3\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "x_data = iris.iloc[:,0:-1] # 독립변수, 입력변수, 설명변수 : 수치형\n",
        "\n",
        "문자형을 one hot encoding 변환하는 단계\n",
        "1. 문자형 데이터를 수치형으로 변환 : LabelEncoder\n",
        "2. 수치형 데이터를 one hot encoding 변환 : OneHotEncoder\n",
        "\n",
        "1. 문자형 데이터를 수치형으로 변환 : LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_integer = le.fit_transform(iris.Name)\n",
        "y_integer\n",
        "le.inverse_transform([0])\n",
        "le.inverse_transform([1])\n",
        "le.inverse_transform([2])\n",
        "le.inverse_transform(y_integer) # 수치형 다시 원본값 확인\n",
        "\n",
        "2. 수치형 데이터를 one hot encoding 변환 : OneHotEncoder\n",
        "0 -> [1,0,0]\n",
        "1 -> [0,1,0]\n",
        "2 -> [0,0,1]\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False) # sparse = True   matrix로 변환, sparse=False   array로 변환\n",
        "y_one_hot = ohe.fit_transform(y_integer.reshape(-1,1)) # resahpe(-1,1) -1은 모든 행 // ohe.fit_transform(y_integer).reshape(-1,1)\n",
        "le.inverse_transform([np.argmax(y_one_hot[0])])\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_one_hot,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([4,3]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([3]),name='bias')\n",
        "\n",
        "learning_rate=0.01 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.nn.softmax(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis),axis=1))\n",
        "        w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "        w.assign_sub(learning_rate*w_grad)\n",
        "        b.assign_sub(learning_rate*b_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight2:{}\\n bias2:{}\\n cost:{}'.format(i,w.numpy(),b.numpy(),cost.numpy()))\n",
        "\n",
        "np.mean(tf.argmax(hypothesis,axis=1) ==tf.argmax(y,axis=1))\n",
        "\n",
        "predict = tf.argmax(hypothesis,axis=1) # 예측\n",
        "real = tf.argmax(y,axis=1) # 실제\n",
        "\n",
        "tf.equal(predict,real)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict,real),dtype=tf.float32))\n",
        "accuracy.numpy()\n",
        "\n",
        "# weight값과 bias값을 구했으면 새로운 값을 입력해서 꽃 분류예측\n",
        "new = tf.constant(np.array([5.9,3.,5.1,1.8]),shape=(1,4),dtype=tf.float32)\n",
        "new_predict = tf.nn.softmax(tf.matmul(new,w) + b)\n",
        "np.argmax(new_predict)\n",
        "\n",
        "\n",
        "\n",
        "# onehot encoding 방법4\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "x_data = iris.iloc[:,0:-1] # 독립변수, 입력변수, 설명변수 : 수치형\n",
        "\n",
        "# 문자형 데이터를 수치형으로 변환\n",
        "iris.Name = iris.Name.map({'Iris-setosa' : 0,'Iris-versicolor' : 1, 'Iris-virginica': 2})\n",
        "iris.Name.unique()\n",
        "\n",
        "# 수치형 데이터를 one hot encoding 변환\n",
        "0 -> [1,0,0]\n",
        "1 -> [0,1,0]\n",
        "2 -> [0,0,1]\n",
        "\n",
        "\n",
        "y_one_hot = to_categorical(iris.Name,num_classes=3)\n",
        "y_one_hot\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_one_hot,tf.float32)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(4,input_dim=4,activation='relu')) # layer1 # input_dim입력층의 특징이 4개\n",
        "model.add(Dense(4,activation='relu')) # layer2\n",
        "model.add(Dense(3,activation='softmax')) # 출력층 다중이기때문에 softmax , 1개는 sigmoid\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x,y,epochs=1000)\n",
        "\n",
        "# weight값과 bias값을 구했으면 새로운 값을 입력해서 꽃 분류예측\n",
        "new = tf.constant(np.array([5.9,3.,5.1,1.8]),shape=(1,4),dtype=tf.float32)\n",
        "new_predict = model.predict(new)\n",
        "np.argmax(new_predict)\n",
        "\n",
        "\n",
        "[문제] 유방암 데이터를 신경망을 이용해서 분류해주세요.\n",
        "wisc = pd.read_csv('c:/data/wisc_bc_data.csv')\n",
        "wisc.info()\n",
        "wisc.diagnosis\n",
        "\n",
        "1. keras 사용\n",
        "x_data = wisc.iloc[:,2:] # 입력변수\n",
        "\n",
        "# 문자형 데이터를 수치형으로 변환\n",
        "wisc.diagnosis.unique()\n",
        "wisc.diagnosis = wisc.diagnosis.map({'B':0,'M':1})\n",
        "# 수치형 데이터 one hot encoding\n",
        "y_one_hot = to_categorical(wisc.diagnosis,num_classes=2)\n",
        "\n",
        "x = tf.constant(x_data,tf.float32) # float형식의 상수변환\n",
        "y = tf.constant(y_one_hot,tf.float32) # float형식의 상수변환\n",
        "\n",
        "x.shape # 형태 확인\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30,input_dim=30,activation='relu')) # layer1 / 입력층 30개\n",
        "model.add(Dense(30,activation='relu')) # layer2\n",
        "model.add(Dense(2,activation='softmax')) # 출력층 2개\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x,y,epochs=2000)\n",
        "\n",
        "# 새로운값으로 예측\n",
        "new = tf.constant(np.array([0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]),shape=(1,30),dtype=tf.float32)\n",
        "np.argmax(model.predict(new))\n",
        "\n",
        "# 피쳐들의 크기가 다르기때문에 피쳐스케일링 하고 작업도 해보기\n",
        "# 로지스틱으로 했을 때랑 비교해보기\n",
        "\n",
        "\n",
        "#- 강사님 답\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import scale\n",
        "data = pd.read_csv('c:/data/wisc_bc_data.csv')\n",
        "data.info()\n",
        "data.describe()\n",
        "data = data.iloc[:,1:]\n",
        "\n",
        "data.diagnosis.unique()\n",
        "data.diagnosis = data.diagnosis.map({\"B\":0,\"M\":1})\n",
        "Counter(data.diagnosis)\n",
        "\n",
        "y_one_hot = tf.one_hot(data.diagnosis,2)\n",
        "y_one_hot\n",
        "\n",
        "x = tf.constant(scale(data.iloc[:,1:]),tf.float32) # scale 적용하고 넣기\n",
        "y = tf.constant(y_one_hot,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([30,2]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([2]),name='bias')\n",
        "\n",
        "learning_rate=0.01 # 중요한 값(이 값을 찾는게 제일 중요)\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.nn.softmax(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis),axis=1))\n",
        "        w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "        w.assign_sub(learning_rate*w_grad)\n",
        "        b.assign_sub(learning_rate*b_grad)\n",
        "    if i%1000 == 0 :\n",
        "        print('>>#{}\\n weight2:{}\\n bias2:{}\\n cost:{}'.format(i,w.numpy(),b.numpy(),cost.numpy()))\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('c:/data/wisc_bc_data.csv')\n",
        "data.info()\n",
        "data.describe()\n",
        "data = data.iloc[:,1:]\n",
        "\n",
        "# 데이터 표준화\n",
        "data_scale = StandardScaler()\n",
        "data_scale.fit(data.iloc[:,1:])\n",
        "data_scale.mean_\n",
        "data_scale.scale_\n",
        "\n",
        "x_scale = data_scale.transform(data.iloc[:,1:])\n",
        "x_scale.shape\n",
        "\n",
        "# 정답 one hot encoding 변환\n",
        "data.diagnosis.unique()\n",
        "data.diagnosis = data.diagnosis.map({\"B\":0,\"M\":1})\n",
        "Counter(data.diagnosis)\n",
        "y_one_hot = to_categorical(data.diagnosis,num_classes=2)\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_scale,y_one_hot,test_size=0.2)\n",
        "x_train.shape\n",
        "y_train.shape\n",
        "\n",
        "\n",
        "x = tf.constant(x_train,tf.float32)\n",
        "y = tf.constant(y_train,tf.float32)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64,input_dim=30,activation='relu')) # layer1 \n",
        "model.add(Dense(32,activation='relu')) # layer2\n",
        "model.add(Dense(2,activation='softmax')) # 출력층 다중이기때문에 softmax , 1개는 sigmoid\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x,y,epochs=10, batch_size=32) # batch_size =32 : 455개의 데이터를 32개씩 나누어서 작업 / 성능이 더 좋음(epochs만 높게 하는 것보다 )\n",
        "\n",
        "1번 epochs 시에 데이터를 32개씩 15번 올려서 weight 값과 bias갑을 계산한다.\n",
        "455/32\n",
        "\n",
        "score = model.evaluate(x_test,y_test)\n",
        "print(\"loss : \",score[0])\n"
      ]
    }
  ]
}