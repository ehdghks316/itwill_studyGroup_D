{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.31파이썬(정규표현식, re모듈,웹스크래핑,크롤링,urllib,BeautifulSoup).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMY5Jemd2NLgXIsMNMKiiqW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/3_31%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D%2Cre%EB%AA%A8%EB%93%88%2C%EC%9B%B9%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91%2C%ED%81%AC%EB%A1%A4%EB%A7%81%2Curllib%2CBeautifulSoup).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYDap9k6QICP"
      },
      "outputs": [],
      "source": [
        "★ 정규표현식(Regular Expression)\n",
        "- 특정 패턴과 일치하는 문자열을 검색, 치환, 제거하는 기능을 제공한다.\n",
        "\n",
        "★ 메타문자(meta characters)\n",
        "- 원래 문자가 가진 뜻이 아닌 특별한 용도로 사용되는 문자\n",
        "\n",
        "[] : []사이의 문자들과 매치\n",
        "[sql] : s또는 q또는 l\n",
        "[a-z] : [] 안의 두 문자 사이에 -를 사용하면 두 문자사이의 범위를 의미\n",
        "[a-zA-Z]\n",
        "[가-힣ㄱ-ㅎ]\n",
        "[!@#$%^&*()]\n",
        "[0-9]\n",
        "\n",
        "a.b :.위치에 모든 문자를 의미(줄바꿈 문자 \\n 제외)\n",
        "a\\.b : .을 문자로 인식\n",
        "a[.]b : .을 문자로 인식\n",
        "\n",
        "a*b : *바로 앞의 문자가 0번 이상을 찾는다. b, ab,aaab\n",
        "a+b : +바로 앞의 문자가 1번 이상을 찾는다. ab, aab,aaab\n",
        "ab?c : ?바로 앞의 문자가 0번, 1번을 찾는다. ac,abc\n",
        "\n",
        "a{2}b : {n} 바로 앞의 문자가 n번 반복을 찾는다. aab\n",
        "a{2,3}b : {n,m} 바로 앞의 문자가 n번 또는 m번 반복을 찾는다. aab,aaab\n",
        "a{2,}b :{n,} 바로 앞의 문자가 n번 이상 반복을 찾는다. aab,aaab,aaaab\n",
        "\n",
        "a|b : a 또는 b\n",
        "[^a] : [^] not 을 의미, a는 제외\n",
        "\n",
        "^ : 시작\n",
        "$ : 끝\n",
        "\n",
        "\\d : [0-9] 숫자와 매치\n",
        "\\D : [^0-9] 숫자가 아닌것과 매치\n",
        "\\s : 공백문자\n",
        "\\S : 공백문자가 아닌것\n",
        "\\w : 문자, 숫자 [a-zA-Z0-9_]\n",
        "\\w : 문자, 숫자아닌 것 [^a-zA-Z0-9_]\n",
        "\n",
        "import re\n",
        "\n",
        "# re.match : 문자열의 처음부터 정규식과 매치되는지를 찾는 함수\n",
        "re.match('\\w','a') # match(정규표현식,문자열) 찾은 위치를 알려줌\n",
        "bool(re.match('\\w','a'))\n",
        "re.match('\\w','1')\n",
        "bool(re.match('\\w','1'))\n",
        "re.match('\\w','_') \n",
        "bool(re.match('\\w','_'))\n",
        "re.match('\\w','파이썬') \n",
        "bool(re.match('\\w','파이썬'))\n",
        "re.match('\\w','-') \n",
        "bool(re.match('\\w','-'))\n",
        "\n",
        "re.match('c*a','cat') \n",
        "bool(re.match('c*a','cat'))\n",
        "re.match('c*a','ccat') \n",
        "bool(re.match('c*a','ccat'))\n",
        "re.match('c*a','at') \n",
        "bool(re.match('c*a','at'))\n",
        "\n",
        "re.match('c+a','cat') \n",
        "bool(re.match('c+a','cat'))\n",
        "re.match('c+a','ccat') \n",
        "bool(re.match('c+a','ccat'))\n",
        "re.match('c+a','at') \n",
        "bool(re.match('c+a','at'))\n",
        "\n",
        "re.match('c?a','cat') \n",
        "bool(re.match('c?a','cat'))\n",
        "re.match('c?a','ccat') \n",
        "bool(re.match('c?a','ccat'))\n",
        "re.match('c?a','at') \n",
        "bool(re.match('c?a','at'))\n",
        "\n",
        "re.match('c{2}a','cat') \n",
        "bool(re.match('c{2}a','cat'))\n",
        "re.match('c{2}a','ccat') \n",
        "bool(re.match('c{2}a','ccat'))\n",
        "re.match('c{2}a','at') \n",
        "bool(re.match('c{2}a','at'))\n",
        "\n",
        "re.match('c{2,3}a','cat') \n",
        "bool(re.match('c{2,3}a','cat'))\n",
        "re.match('c{2,3}a','ccat') \n",
        "bool(re.match('c{2,3}a','ccat'))\n",
        "re.match('c{2,3}a','cccat') \n",
        "bool(re.match('c{2,3}a','cccat'))\n",
        "\n",
        "bool(re.match('c|a','cccat'))\n",
        "bool(re.match('c|a','at'))\n",
        "bool(re.match('c|a','ct'))\n",
        "bool(re.match('a','cat'))\n",
        "\n",
        "bool(re.match('[0-9]th','21th'))\n",
        "bool(re.match('[0-9][0-9]th','21th'))\n",
        "bool(re.match('[0-9]*th','21th'))\n",
        "bool(re.match('[0-9]+th','21th'))\n",
        "bool(re.match('[0-9]?th','21th'))\n",
        "bool(re.match('[0-9]{2}th','21th'))\n",
        "bool(re.match('\\d\\dth','21th'))\n",
        "bool(re.match('\\d{2}th','21th'))\n",
        "\n",
        "bool(re.match('Da','Data Science'))\n",
        "bool(re.match('da','Data Science'))\n",
        "bool(re.match('da','Data Science',re.I)) # re.I옵션 : 대소문자 무시하고 찾을 수 있다.\n",
        "\n",
        "m = re.match('Data','Data Science')\n",
        "m.group() # match된 문자열만 리턴\n",
        "m.span() # 매치된 문자열의 (시작,끝)을 리턴\n",
        "m.start() # m.span()[0] 매치된 시작위치\n",
        "m.end() # m.span()[1]  매치된 끝 위치를 리턴\n",
        "\n",
        "'Data Science'[0:4]\n",
        "'Data Science'[m.start():m.end()]\n",
        "'Data Science'.startswith('Data')\n",
        "'Data Science'.endswith('Science')\n",
        "\n",
        "re.match('Science','Data Science')\n",
        "\n",
        "'Data Science'.find('Science')\n",
        "'Data Science'.index('Science')\n",
        "\n",
        "'Data Science'.find('science')\n",
        "'Data Science'.index('science')\n",
        "\n",
        "'Science' in 'Data Science'\n",
        "'science' in 'Data Science'\n",
        "'Data Science' in 'Science'\n",
        "\n",
        "from pandas import Series, DataFrame\n",
        "\n",
        "Series('Data Science').str.contains('Science')\n",
        "Series('Data Science').str.contains('Data')\n",
        "Series('Data Science').str.startswith('Data')\n",
        "Series('Data Science').str.endswith('Science')\n",
        "Series('Data Science').str.find('Science')\n",
        "Series('Data Science').str.index('Science')\n",
        "\n",
        "re.match('Science','Data Science')\n",
        "\n",
        "# search : 패턴에 해당하는 문자열을 어디서든지 찾는다.\n",
        "m = re.search('Science','Data Science')\n",
        "m.group()\n",
        "m.span()\n",
        "m.start()\n",
        "m.end()\n",
        "\n",
        "re.search('Science','Data Science Data Science') # 한 번만 찾음\n",
        "\n",
        "# findall : 패턴의 일치하는 문자열을 리스트로 반환\n",
        "re.findall('Science','Data Science Data Science') \n",
        "\n",
        "re.findall('a','Data Science')\n",
        "re.findall('a.','Data Science')\n",
        "re.findall('a.*','Data Science')\n",
        "re.findall('a.+','Data Science')\n",
        "re.findall('a.?','Data Science')\n",
        "\n",
        "[문제170] data 변수에서 숫자 패턴을 찾아주세요.\n",
        "data = \"오늘은 2022년 3월31일 입니다.\"\n",
        "re.findall('\\d',data)\n",
        "re.findall('\\d+',data)\n",
        "re.findall('[0-9]+',data)\n",
        "\n",
        "[문제171] data 변수에서 문자 패턴을 찾아주세요.\n",
        "data = \"오늘은 2022년 3월31일 입니다.\"\n",
        "re.findall('\\w+',data)\n",
        "\n",
        "re.findall('\\D+',data)\n",
        "re.findall('[^0-9]+',data)\n",
        "re.findall('[가-힣]+',data)\n",
        "\n",
        "[문제172] 전화번호를 출력해주세요.\n",
        "message = '''안녕하세요. 전화번호는 02-123-4567 입니다.\n",
        "문의사항이 있으면 031-1234-0000 으로 연락주시기 바랍니다.\n",
        "폰 번호는 010-1234-1004 고객센터 전화번호 1588-3600  대표전화 : 031)777-1140'''\n",
        "\n",
        "re.findall('[\\d+-|)]*\\d+-\\d+',message) \n",
        "\n",
        "re.findall('\\d{0,3}[-)]*\\d{3,4}-\\d{4}',message)\n",
        "re.findall('\\d*[-)]*\\d*-\\d*',message)\n",
        "\n",
        "[문제173] 이메일 주소를 출력해주세요.\n",
        "message = '''담당자 이메일주소는 webmaster@itwill.co.kr  \n",
        "           이메일 주소는 happy.o@gmail.com   \n",
        "           이메일 주소는 happy123@naver.com 입니다. info_search@joins.com'''\n",
        "\n",
        "message1 = '''담당자 이메일주소는 webmaster@itwill.co.kr  \n",
        "           이메일 주소는 happy.o@gmail.com   \n",
        "           이메일 주소는 happy123@naver.com 입니다. info_search@joins.com'''\n",
        "\n",
        "re.findall('[^\\s]+@[^\\s]+',message)\n",
        "\n",
        "re.findall('[a-zA-Z0-9._]+@[a-zA-Z.]+',message)\n",
        "re.findall('[A-z0-9._]+@[A-z.]+',message)\n",
        "re.findall('([A-z0-9._]+)@([A-z.]+)',message) # () : 그룹의미로 묶어서 이메일에 아이디만 추출할 때 사용하면 유용\n",
        "re.findall('[가-힣]+',message)\n",
        "\n",
        "# re.compile : 정규식을 컴파일 해놓는 것(정규식을 반복해서 사용해야할 때)\n",
        "mail_compile = re.compile('[A-z0-9._]+@[A-z.]+')\n",
        "mail_compile.findall(message)\n",
        "mail_compile.findall(message1)\n",
        "\n",
        "x = '오늘 하루도 우리 힘내서 공부해봐요..\\ngo for it!!'\n",
        "print(x)\n",
        "\n",
        "# r: raw string escape 문자열을 그냥 문자로 표현\n",
        "x = r'오늘 하루도 우리 힘내서 공부해봐요..\\ngo for it!!'\n",
        "print(x)\n",
        "\n",
        "\n",
        "# re.sub:패턴 치환하기 \n",
        "source = 'Data Science'\n",
        "source.replace('Science','Scientist')\n",
        "re.sub('Science','Scientist',source)\n",
        "\n",
        "[문제174] html 변수에서 title tag에 있는 문자열을 추출해주세요.\n",
        "html = \"<head> 안녕하세요 <title> itwill 학원에 오실걸 환영합니다. </title> </head>\"\n",
        "#1.\n",
        "title = str(re.findall('<title>[\\w+\\s]+.*</title>',html))\n",
        "title = re.sub('</*title>','',title)\n",
        "title = re.sub(\"\\[|]\",\"\",title)\n",
        "title = re.sub(\"\\'\",\"\",title)\n",
        "title.strip() # 양쪽 공백 제거\n",
        "\n",
        "#2.\n",
        "title = re.search('<title.*/title>',html)\n",
        "text = title.group()\n",
        "\n",
        "re.findall('<.+?>',text)\n",
        "re.sub('<.+?>','',text).strip()\n",
        "\n",
        "\n",
        "# 문자열 분리\n",
        "'python.programming'.split('.')\n",
        "\n",
        "re.split('.','python.programming') # 안나옴\n",
        "re.split('\\.','python.programming')\n",
        "re.split('[.]','python.programming')\n",
        "\n",
        "re.split('\\.|:','python.programming:R') # . 또는 : 을 기준으로 split\n",
        "re.split('[:.]','python.programming:R') # . 또는 : 을 기준으로 split\n",
        "\n",
        "re.split('[:. ]','python.programming:R SQL') # . 또는 : 또는 공백\n",
        "re.split('[:.\\s]','python.programming:R SQL') # . 또는 : 또는 공백\n",
        "\n",
        "\n",
        "########################################\n",
        "\n",
        "★ web scraping, web crawling\n",
        "- 웹사이트에서 원하는 정보를 수집하는 기술\n",
        "\n",
        "# 스크래핑을 할 때 타임슬립을 주면서 스크래핑 \n",
        "# 웹사이트 robot을 확인하기\n",
        "# 과도한 스크래핑으로 ip차단당할 수 있다.\n",
        "# 동종업계면 허락을 받고 하는게 좋다. 무단 사용시 법정싸움까지 갈 수 있다.\n",
        "\n",
        "★ urllib\n",
        "- 파이썬에서 인터넷 데이터를 받아오는 기능들이 들어 있는 패키지\n",
        "\n",
        "import urllib\n",
        "url = 'https://www.naver.com/'\n",
        "\n",
        "# 인터넷주소에 응답요청을 객체로 받아옴\n",
        "response = urllib.request.Request(url)\n",
        "response.host # url 확인\n",
        "response.full_url # full url확인\n",
        "response.type # 프로토콜 확인\n",
        "\n",
        "\n",
        "from urllib.request import urlopen\n",
        "url = 'https://www.naver.com/'\n",
        "html = urlopen(url) # 요청한 주소에서 객체로 받아오기\n",
        "byte_data = html.read() # byte 단위로 읽어오기\n",
        "text_data = byte_data.decode() # decoding (해독)\n",
        "text_data\n",
        "\n",
        "★ beautifulsoup\n",
        "- 데이터를 추출하는데 필요한 기능이 들어 있는 라이브러리\n",
        "- parsing 라이브러리\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "html = \"\"\"\n",
        "<html>\n",
        "<body>\n",
        "<h1> 스크래핑 </h1>\n",
        "<p> 웹페이지 분석하기 </p>\n",
        "<p> 데이터 정제 작업하기 1 </p>\n",
        "<p> 데이터 정제 작업하기 2 </p>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser') # parser작업해주는 기능\n",
        "h1 = soup.html.body.h1 # 태그 명 사용\n",
        "h1.string # 태그안에 있는 텍스트만 확인\n",
        "h1.text # 태그안에 있는 텍스트만 확인\n",
        "\n",
        "p1 = soup.html.body.p #첫번째 p태그\n",
        "p1.text\n",
        "\n",
        "p2 = p1.next_sibling.next_sibling #두 번째 p태그\n",
        "p2.text\n",
        "\n",
        "p3 = p2.next_sibling.next_sibling #세 번째 p태그\n",
        "p3.text\n",
        "\n",
        "\n",
        "html = \"\"\"\n",
        "<html>\n",
        "<body>\n",
        "<h1 id = 'title'> 스크래핑 </h1>\n",
        "<p id = 'subtitle'> 웹페이지 분석하기 </p>\n",
        "<p> 데이터 정제 작업하기 1 </p>\n",
        "<p> 데이터 정제 작업하기 2 </p>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.html.body.h1 # 태그를 순서대로 들어가서 찾기\n",
        "soup.find('h1') # 태그를 바로 찾기\n",
        "soup.find(id='title').string # id값으로 찾기(태그가 많으면 찾기 어려우니까 id값이 있는 경우 사용)\n",
        "soup.find(id='title').text\n",
        "soup.html.body.p # 처음으로 나오는 p태그 찾기\n",
        "soup.find('p') # 처음으로 나오는 p태그 찾기\n",
        "soup.find(id='subtitle') #  처음으로 나오는 p태그 찾기\n",
        "soup.find(id='subtitle').string\n",
        "soup.find(id='subtitle').text\n",
        "\n",
        "soup.find_all('p') # p태그 모두 찾기(리스트[]형식으로 출력)\n",
        "soup.find_all('p').string # 리스트형식으로 텍스트만 출력 불가 for문사용\n",
        "for i in soup.find_all('p'):\n",
        "    print(i.string)\n",
        "for i in soup.find_all('p'):\n",
        "    print(i.text)\n",
        "\n",
        "# string과 text 차이 \n",
        "    - string : 뽑아낼 태그안에 다른 테그가 있으면 출력되지 않음  ( 자식 테그가 없을 때 사용하는것이 일반적 )\n",
        "    - text : 뽑아낼 태그안에 다른 테그가 있어도 출력됨 ( 태그이름 빼고 text 모두 추출, 자식테그들의 텍스트 모두 추출)\n",
        "soup.html.body.string\n",
        "soup.html.body.text\n",
        "\n",
        "html = \"\"\"\n",
        "<html>\n",
        "    <body>\n",
        "        <h1 id = 'title'> 스크래핑 </h1>\n",
        "        <p id = 'subtitle'> 웹페이지 분석하기 </p>\n",
        "        <p> 데이터 정제 작업하기 1 </p>\n",
        "        <p> 데이터 정제 작업하기 2 </p>\n",
        "        <ul>\n",
        "            <li> <a href='https://www.itwill.co.kr/'> 아이티윌 </a> </li>\n",
        "            <li> <a href='https://www.naver.com'> 네이버 </a> </li>\n",
        "        </ul>\n",
        "    </body>\n",
        "</html>\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "ul = soup.html.body.ul\n",
        "ul.string\n",
        "ul.text\n",
        "\n",
        "a1 = soup.html.body.ul.li.a\n",
        "a1.string\n",
        "a1.text\n",
        "\n",
        "# attrs : 속성 추출(속성을 확인하는 방법)\n",
        "a1.attrs\n",
        "'href' in a1.attrs\n",
        "'id' in a1.attrs\n",
        "\n",
        "a1['href']\n",
        "a1.attrs['href']\n",
        "\n",
        "soup.find('a')\n",
        "soup.find('a')['href']\n",
        "\n",
        "soup.find_all('a')\n",
        "soup.find_all('a')[0]['href']\n",
        "soup.find_all('a')[1]['href']\n",
        "\n",
        "for i in soup.find_all('a'):\n",
        "    print(i.attrs['href'])\n",
        "    print(i.string)\n",
        "    \n",
        "# find_all = findAll\n",
        "soup.findAll('a')\n",
        "soup.findAll('a')[0]['href']\n",
        "soup.findAll('a')[1]['href']\n",
        "\n",
        "for i in soup.findAll('a'):\n",
        "    print(i.attrs['href'])\n",
        "    print(i.string)\n",
        "\n",
        "for i in soup.findAll('p'):\n",
        "    print(i.string)\n",
        "        \n",
        "    \n",
        "html = \"\"\"\n",
        "<html>\n",
        "<body>\n",
        "<h1 id = 'title'> 스크래핑 </h1>\n",
        "<p id = 'subtitle'> 웹페이지 분석하기 </p>\n",
        "<p> 데이터 정제 작업하기 1 </p>\n",
        "<p> 데이터 정제 작업하기 2 </p>\n",
        "<ul>\n",
        "<li> <a href='https://www.itwill.co.kr/' class='cafe1' id='link1'> 아이티윌 </a> </li>\n",
        "<li> <a href='https://www.naver.com' class='cafe2' id='link2'> 네이버 </a> </li>\n",
        "</ul>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "soup.find(id='link1')\n",
        "soup.find(id='link2')\n",
        "\n",
        "soup.find(class_ = 'cafe1') # class속성을 찾을 때는 _표시 필요\n",
        "soup.find(class_ = 'cafe2')\n",
        "\n",
        "soup.find('a',id='link1')\n",
        "soup.find('a',id='link2')\n",
        "\n",
        "soup.find('a',{'id':'link1'}) # key, value 형태로 찾기\n",
        "soup.find('a',{'id':'link2'})\n",
        "\n",
        "soup.find('a',class_='cafe1') \n",
        "soup.find('a',class_='cafe2')\n",
        "\n",
        "soup.find('a',{'class':'cafe1'})  # key, value에서는 class속성에 _표시 제거\n",
        "soup.find('a',{'class':'cafe2'})\n",
        "\n",
        "\n",
        "from urllib.request import urlopen\n",
        "url = 'https://www.donga.com/news/article/all/20220331/112632650/1'\n",
        "html = urlopen(url) \n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.find('div',class_='article_txt').text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[문제175] 동아일보 '인공지능' 뉴스기사 검색을 통해 본문기사 내용을 수집하세요. # for문 돌릴때마다 time.sleep 넣어주기\n",
        "# 1. 한 페이지에 있는 url모두 추출\n",
        "url = 'https://www.donga.com/news/search?p=1&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "s_p = soup.find_all('p',{'class':'tit'})\n",
        "\n",
        "s_p[0].find('a')['href']\n",
        "s_p[1].find('a')['href']\n",
        "\n",
        "url_save = [] # 각 p테그안에 a테그에서 url추출해서 저장하는 변수\n",
        "for i in s_p:\n",
        "    url_save.append(i.find('a')['href'])\n",
        "\n",
        "url_save # url이 저장된 변수(저장 완료)\n",
        "\n",
        "# 2. url마다 text추출\n",
        "url_text = [] # url마다 text추출하여 저장해놓는 변수\n",
        "for i in url_save:\n",
        "    url = i\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    text = soup.find('div',class_='article_txt').text\n",
        "    url_text.append(text)\n",
        "\n",
        "url_text # 각url마다 text를 추출하여 저장한 변수(저장 완료)\n",
        "\n",
        "# 3. 각 페이지 마다 url 추출\n",
        "1페이지\n",
        "https://www.donga.com/news/search?p=1&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1 \n",
        "2페이지\n",
        "https://www.donga.com/news/search?p=16&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\n",
        "3페이지\n",
        "https://www.donga.com/news/search?p=31&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\n",
        "다른 문자들은 모두 같고 p=1 , p= 15, p= 31\n",
        "\n",
        "import time\n",
        "url_save = []\n",
        "for i in range(1,52,15):\n",
        "    url = 'https://www.donga.com/news/search?p='+str(i)+'&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    s_p = soup.find_all('p',class_='tit')\n",
        "    for j in s_p:\n",
        "        url_save.append(j.find('a')['href'])\n",
        "    time.sleep(2)\n",
        "\n",
        "url_save        \n",
        "\n",
        "# 4.url에서 text추출 ( 2.이랑 코드는 같음)\n",
        "url_text = []\n",
        "for i in url_save:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    text = soup.find('div',class_='article_txt').text\n",
        "    url_text.append(text)\n",
        "    time.sleep(1)\n",
        "    \n",
        "url_text\n",
        "\n",
        "# 5. list를 하나의 텍스트로 변환\n",
        "one_url_text = ''\n",
        "for i in url_text:\n",
        "    one_url_text = one_url_text + i + '\\n'\n",
        "one_url_text\n",
        "\n",
        "# 6. 파일로 저장해놓기 ( 1리스트저장 2하나의텍스트로저장 )\n",
        "1.리스트로 저장\n",
        "with open(\"c:/data/donga_news_ai_20220331_list.txt\",'w',encoding='utf-8') as file:\n",
        "    for text in url_text:\n",
        "        file.write(text + '\\n')\n",
        "2.하나의 텍스트로 저장\n",
        "with open('c:/data/donga_news_ai_20220331.txt','w',encoding='utf-8') as file:\n",
        "    file.write(one_url_text)\n",
        "\n",
        "######################### 강사님 답\n",
        "'''    \n",
        "url = 'https://www.donga.com/news/search?p=1&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "p = soup.findAll('p',{'class':'tit'})\n",
        "\n",
        "url = []\n",
        "for i in p:\n",
        "    url.append(i.find('a')['href'])\n",
        "\n",
        "html = urlopen(url[0])\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.find('div',{'class':'article_txt'}).text\n",
        "\n",
        "news_url = []\n",
        "\n",
        "for i in range(1,32,15):\n",
        "    url = 'https://www.donga.com/news/search?p='+str(i)+'&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    p = soup.findAll('p',{'class':'tit'})\n",
        "    for j in p:\n",
        "        news_url.append(j.find('a')['href'])\n",
        "    time.sleep(2)\n",
        "\n",
        "import time\n",
        "\n",
        "news_text = ''\n",
        "\n",
        "for u in news_url:\n",
        "    html = urlopen(u)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    news_text = news_text + soup.find('div',{'class':'article_txt'}).text + '\\n'\n",
        "    time.sleep(2)\n",
        "\n",
        "with open(\"c:/data/news_20220331.txt\", 'w', encoding='utf-8') as file:\n",
        "    file.write(news_text)\n",
        "'''\n",
        "######################## 여기까지\n",
        "\n",
        "[스스로문제]저장한 데이터를 열어서 전처리작업 해보기\n",
        "1. 파일 열기\n",
        "# 파일 열기(with문은 close를 수행 안 해도됨)\n",
        "with open('c:/data/donga_news_ai_20220331.txt','r',encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "    \n",
        "data\n",
        "2.이메일 추출 \n",
        "re.findall('[A-z가-힣0-9._]+@[a-z.]+',data)\n"
      ]
    }
  ]
}