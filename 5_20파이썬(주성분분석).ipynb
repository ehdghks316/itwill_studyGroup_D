{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_20파이썬(주성분분석).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhOxpkUpossgNe//XjR+aV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/5_20%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jURZdlfvFK2f"
      },
      "outputs": [],
      "source": [
        "차원축소(dimension reduction)\n",
        "- 변수를 줄이는 것\n",
        "\n",
        "고전, 국어, 프랑스어, 수학, 물리, IQ(정답)\n",
        "# 변수들이 많다고 예측을 잘 하는것이 아니다\n",
        "\n",
        "고전,국어,프랑스 -> 언어지능\n",
        "수학,물리 -> 수리지능\n",
        "\n",
        "- 상관계수가 높은 변수들 끼리 묶어 보는게 중요하다.\n",
        "#공분산은 2개의 확률변수간의 선형관계, 공분산을 표준화하는게 상관계수\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.head()\n",
        "\n",
        "# 그래프 그려보기\n",
        "markers = ['^','s','o']\n",
        "\n",
        "for i,marker in enumerate(markers):\n",
        "    x_data = iris[iris['Name'] == iris.Name.unique()[i]]['SepalLength']\n",
        "    y_data = iris[iris['Name'] == iris.Name.unique()[i]]['SepalWidth']\n",
        "    plt.scatter(x_data,y_data,marker=marker, label=iris.Name.unique()[i])\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('SepalLength')\n",
        "plt.ylable('SepalWidth')\n",
        "plt.show()\n",
        "\n",
        "markers = ['^','s','o']\n",
        "\n",
        "for i,marker in enumerate(markers):\n",
        "    x_data = iris[iris['Name'] == iris.Name.unique()[i]]['PetalLength']\n",
        "    y_data = iris[iris['Name'] == iris.Name.unique()[i]]['PetalWidth']\n",
        "    plt.scatter(x_data,y_data,marker=marker, label=iris.Name.unique()[i])\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('PetalLength')\n",
        "plt.ylabel('PetalWidth')\n",
        "plt.show()\n",
        "\n",
        "# 표준화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "iris_s = StandardScaler().fit(iris.iloc[:,:-1])\n",
        "iris_s.mean_\n",
        "iris_s.scale_\n",
        "\n",
        "iris_scaled = iris_s.transform(iris.iloc[:,:-1])\n",
        "iris_scaled\n",
        "iris_scaled.shape\n",
        "\n",
        "#주성분 분석, PCA(Principal Component Analysis)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2) # 하이퍼파라미터 값( 몇 개로 성분을 분류할 것인가)\n",
        "pca.fit(iris_scaled)\n",
        "\n",
        "print(pca.explained_variance_ratio_) #설명력을 출력하는 함수, pca_component_1이 72%로 설명할 수 있고 pca_component_2는 23%로 설명할 수 있다. 첫번째가 설명력이 좋다고 볼 수 있다.\n",
        "\n",
        "iris_pca = pca.transform(iris_scaled)\n",
        "iris_pca # 4개의 피쳐를 2개의 피쳐로 변환 함, 첫번째의 열을 기준으로 보는 것이 좋다.\n",
        "\n",
        "pca_columns = ['pca_component_1','pca_component_2']\n",
        "iris_pca = pd.DataFrame(iris_pca,columns=pca_columns)\n",
        "iris_pca['Name'] = iris.Name\n",
        "iris_pca # 2개의 피처로 name을 분류하는 지\n",
        "\n",
        "markers = ['^','s','o']\n",
        "\n",
        "for i,marker in enumerate(markers):\n",
        "    x_data = iris_pca[iris_pca['Name'] == iris_pca.Name.unique()[i]]['pca_component_1']\n",
        "    y_data = iris_pca[iris_pca['Name'] == iris_pca.Name.unique()[i]]['pca_component_2']\n",
        "    plt.scatter(x_data,y_data,marker=marker, label=iris_pca.Name.unique()[i])\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('pca_component_1')\n",
        "plt.ylabel('pca_component_2')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score # \n",
        "rfc = RandomForestClassifier()\n",
        "score = cross_val_score(rfc,iris.iloc[:,:-1],iris.Name,scoring='accuracy',cv=3)\n",
        "score # 학습완료 됨, iris.Name.unique() 순서대로 나옴\n",
        "\n",
        "model = RandomForestClassifier(criterion='entropy',n_estimators=100,oob_score=True)\n",
        "model.fit(iris.iloc[:,:-1],iris.Name)\n",
        "\n",
        "iris.iloc[149]\n",
        "\n",
        "y_pred = model.predict([[5.9,3.0,5.1,1.8]])\n",
        "y_pred\n",
        "\n",
        "# 주성분분석으로 나온 것도 체크해보기 \n",
        "rfc = RandomForestClassifier()\n",
        "score = cross_val_score(rfc,iris_pca.iloc[:,:-1],iris_pca.Name,scoring='accuracy',cv=3)\n",
        "score # 학습완료 됨, iris.Name.unique() 순서대로 나옴, 주성분분석 안 했을 때보다 더 낮게 나옴 그래서 무조건 주성분 분석이 좋은 것이 아니다라는 것\n",
        "\n",
        "model = RandomForestClassifier(criterion='entropy',n_estimators=100,oob_score=True)\n",
        "model.fit(iris_pca.iloc[:,:-1],iris_pca.Name)\n",
        "\n",
        "1. 새로운 값을 표준화 수행\n",
        "test = [[5.9,3.0,5.1,1.8]] # scaling을 해야함\n",
        "test_s = iris_s.transform(test)  # 표준화\n",
        "test_s\n",
        "\n",
        "2. 표준화 데이터를 pca값으로 변환\n",
        "test_pca = pca.transform(test_s)\n",
        "test_pca\n",
        "\n",
        "3. 새로운 pca값으로 모델에 적용\n",
        "y_pred = model.predict(test_pca) # \n",
        "y_pred\n",
        "\n",
        "\n",
        "Autoencoder\n",
        "- 비지도학습(unsupervisor)\n",
        "- 차원축소된 이미지가 나옴\n",
        "- 오토인코더는 입력값과 출력값을 같게 하는 신경망\n",
        "- encoder : 입력값을 받아 특징값으로 변환하는 신경망\n",
        "- latent : 신경망 내부에서 추출된 특징되는 값들이 모여있는 곳\n",
        "- decoder : 특징값을 출력값으로 변환하는 신경망\n",
        "# 화질이 않좋은 이미지를 좋게 만드는 작업도 함\n",
        "# cnn에서 적용해보기\n",
        "\n",
        "from tensorflow.keras.layers import Dense,Input,Conv2D,Conv2DTranspose, Flatten,Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import datasets\n",
        "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data() # 정답라벨인 y는 필요없음\n",
        "x_train.shape\n",
        "x_train = x_train.astype('float32')/255 # scaling\n",
        "x_test = x_test.astype('float32')/255 # scaling\n",
        "\n",
        "▶방법1\n",
        "Inputs = Input(shape=(28,28,1))\n",
        "x = Flatten()(Inputs)\n",
        "latent = Dense(10,activation='relu')(x)\n",
        "x = Dense(784)(latent)\n",
        "x = Reshape((28,28))(x)\n",
        "model = Model(Inputs,x)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        "model.fit(x_train,x_train,epochs=10,batch_size=32,validation_data=(x_test,x_test)) # batch_size는 8~64 그이상은 x\n",
        "\n",
        "predict_img = model.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(2,n,i+1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title('original')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False) # x축 출력x\n",
        "    ax.get_xaxis().set_visible(False) # y축 출력 x \n",
        "    \n",
        "    ax = plt.subplot(2,n,i+1+n)\n",
        "    plt.imshow(predict_img[i])\n",
        "    plt.title('autoencoder')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False) # x축 출력x\n",
        "    ax.get_xaxis().set_visible(False) # y축 출력 x \n",
        "\n",
        "\n",
        "▶방법2\n",
        "Inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(32,3,2,activation='relu',padding='same')(Inputs)\n",
        "x = Conv2D(64,3,2,activation='relu',padding='same')(x)\n",
        "x = Flatten()(x)\n",
        "latent = Dense(10)(x)\n",
        "x = Dense(7*7*64)(latent)\n",
        "x = Reshape((7,7,64))(x)\n",
        "x = Conv2DTranspose(64,3,2,activation='relu',padding='same')(x)\n",
        "x = Conv2DTranspose(32,3,2,activation='relu',padding='same')(x)\n",
        "x = Conv2DTranspose(1,3,1,activation='relu',padding='same')(x) \n",
        "model = Model(Inputs,x)\n",
        "model.summary() # 모델은 계속 체크해보기\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        "model.fit(x_train,x_train,epochs=10,batch_size=32,validation_data=(x_test,x_test)) # batch_size는 8~64 그이상은 x\n",
        "\n",
        "predict_img = model.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(2,n,i+1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title('original')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False) # x축 출력x\n",
        "    ax.get_xaxis().set_visible(False) # y축 출력 x \n",
        "    \n",
        "    ax = plt.subplot(2,n,i+1+n)\n",
        "    plt.imshow(predict_img[i])\n",
        "    plt.title('autoencoder')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False) # x축 출력x\n",
        "    ax.get_xaxis().set_visible(False) # y축 출력 x \n",
        "\n",
        "\n",
        "▶방법3\n",
        "encoder_inputs = Input(shape=(28,28,1))\n",
        "x = Conv2D(32,3,1,activation='relu',padding='same')(encoder_inputs)\n",
        "x = Conv2D(64,3,1,activation='relu',padding='same')(x)\n",
        "x = Flatten()(x)\n",
        "encoder_output = Dense(2)(x)\n",
        "encoder = Model(encoder_inputs,encoder_output)\n",
        "encoder.summary()\n",
        "\n",
        "decoder_inputs = Input(shape=(2,))\n",
        "x = Dense(7*7*64)(decoder_inputs)\n",
        "x = Reshape((7,7,64))(x)\n",
        "x = Conv2DTranspose(64,3,2,activation='relu',padding='same')(x)\n",
        "x = Conv2DTranspose(32,3,2,activation='relu',padding='same')(x)\n",
        "decoder_output = Conv2DTranspose(1,3,1,activation='relu',padding='same')(x) \n",
        "decoder = Model(decoder_inputs,decoder_output)\n",
        "decoder.summary() # 모델은 계속 체크해보기\n",
        "\n",
        "# encoder 모델과 decoder 모델을 연결하는 작업\n",
        "encoder_in = Input(shape=(28,28,1))\n",
        "x = encoder(encoder_in)\n",
        "decoder_out = decoder(x)\n",
        "\n",
        "model_auto = Model(encoder_in,decoder_out)\n",
        "model_auto.summary()\n",
        "\n",
        "model_auto.compile(optimizer='adam',loss='mse')\n",
        "model_auto.fit(x_train,x_train,epochs=10,batch_size=32,validation_data=(x_test,x_test)) # batch_size는 8~64 그이상은 x\n",
        "\n",
        "x_img = encoder.predict(x_test)\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.scatter(x=x_img[:,0],y=x_img[:,1],c=y_test,cmap=plt.get_cmap('Paired'),s=4)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "통계에서 가장 중요한 것은 분산!\n",
        "\n",
        "cnn에서 convolution층에서 특징을 추출하는 것\n",
        "\n",
        "\n"
      ]
    }
  ]
}