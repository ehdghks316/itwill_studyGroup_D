{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_6파이썬(tensorflow회귀분석).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwBGKiN6LJIwHYD9BshXVH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/5_6%ED%8C%8C%EC%9D%B4%EC%8D%AC(tensorflow%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YJgSgQX7jCQ"
      },
      "outputs": [],
      "source": [
        "2022.5.6\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "텐서 자료구조\n",
        "- 텐서는 텐서플로의 기본 자료구조\n",
        "- 텐서는 다차원배열\n",
        "\n",
        "1차원텐서\n",
        "a = np.array([1,2,3])\n",
        "a.shape\n",
        "a.ndim\n",
        "a.dtype\n",
        "\n",
        "# numpy array -> tensor로 변환\n",
        "tf.convert_to_tensor(a,dtype='int32')\n",
        "tf.constant(a,tf.float64)\n",
        "t = tf.convert_to_tensor(a,dtype=tf.int32)\n",
        "t.ndim\n",
        "t.shape\n",
        "t.dtype\n",
        "t1 = tf.constant(a)\n",
        "t1.ndim\n",
        "t1.shape\n",
        "t1.dtype\n",
        "\n",
        "2차원 텐서\n",
        "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "b = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
        "\n",
        "t1 = tf.constant(a)\n",
        "t2 = tf.constant(b)\n",
        "\n",
        "tf.math.add(t1,t2)\n",
        "tf.matmul(t1,t2) # 행렬의 곱\n",
        "np.dot(a,b) # 행렬의 곱\n",
        "\n",
        "3차원 텐서\n",
        "x = np.array([[1,2],[3,4],[5,6],[7,8]])\n",
        "x.shape\n",
        "x.ndim\n",
        "\n",
        "x1 = tf.constant(x)\n",
        "x1\n",
        "x1.shape\n",
        "x1.get_shape()\n",
        "x1.ndim\n",
        "\n",
        "[문제]\n",
        "x변수는 1행 3열 모양의 1,2,3\n",
        "w변수는 3행 1열 모양의 2,2,2\n",
        "y변수는 x와 w를 행렬의 곱을 이용한 결과를 수행하는 프로그램을 작성하세요.\n",
        "\n",
        "x = np.array([[1,2,3]])\n",
        "w = np.array([[2],[2],[2]])\n",
        "x.shape\n",
        "w.shape\n",
        "tf.matmul(x,w)\n",
        "np.dot(x,w)\n",
        "\n",
        "x = tf.constant([[1,2,3]])\n",
        "w = tf.constant([[2],[2],[2]])\n",
        "y = tf.matmul(x,w)\n",
        "y\n",
        "\n",
        "# 함수를 생성하는 방법\n",
        "1. 0의 값으로 텐서를 생성\n",
        "tf.zeros_like([1,2,3],dtype=tf.int32,name='zeros')\n",
        "tf.zeros([4,5]) # 2차원행렬(행,열)\n",
        "tf.zeros([1,4,5]) # 3차원행렬(면,행,열)\n",
        "\n",
        "2. 1의 값으로 텐서를 생성\n",
        "tf.ones([3,3]) # 2차원\n",
        "tf.ones([2,3,3]) # 3차원\n",
        "\n",
        "3. 특정한 값으로 텐서를 생성\n",
        "tf.fill([3,3],2) # 행렬, 채워야할 값\n",
        "tf.constant(7,shape=[3,3]) # 채울 값, 행렬\n",
        "tf.constant(7)\n",
        "tf.constant([1,2,3,4],shape=[2,2])\n",
        "\n",
        "4. 정규분포난수\n",
        "x = tf.random.normal([3,3])\n",
        "np.mean(x)\n",
        "np.std(x)\n",
        "\n",
        "x = tf.random.normal([3,3],mean=0,stddev=1)\n",
        "np.mean(x)\n",
        "np.std(x)\n",
        "\n",
        "5. 균등분포난수\n",
        "tf.random.uniform([2,3],minval=1,maxval=3)\n",
        "tf.random.uniform([5,5],minval=1,maxval=10)\n",
        "\n",
        "6. 시퀀스\n",
        "tf.linspace(10,12,5)\n",
        "\n",
        "tf.range(start=1,limit=10,delta=1)\n",
        "tf.range(start=1,limit=10,delta=1.5)\n",
        "\n",
        "# 주어진 값을 shuffle\n",
        "x = tf.constant([1,2,3,4,5,6],shape=[3,2])\n",
        "tf.random.shuffle(x)\n",
        "\n",
        "# 핼, 열의 모습을 수정\n",
        "tf.reshape(x,shape=[2,3])\n",
        "\n",
        "\n",
        "y = weight * x + bias\n",
        "\n",
        "x_data = np.array([1,2,3,4,5,6])\n",
        "y_data = np.array([2,4,6,8,10,12])\n",
        "\n",
        "x = tf.constant(x_data,tf.float32) # 입력값\n",
        "y = tf.constant(y_data,tf.float32) # 종속변수,정답\n",
        "\n",
        "w = tf.Variable(tf.random.normal([1]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(200):\n",
        "    with tf.GradientTape() as tape: # GradienTape() : 미분을 대신 해주는 함수? with문 필요\n",
        "        hypothesis = w*x+b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis-y)) # mse\n",
        "    w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "    w.assign_sub(learning_rate*w_grad) # w= w - learning_rate*(∂cost(loss) / ∂w) # weight 값의 수정\n",
        "    b.assign_sub(learning_rate*b_grad) # b= b - learning_rate*(∂cost(loss) / ∂b) # bias 값의 수정\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        print(\"step:{},weight:{}, bias:{}, cost:{}\".format(i,float(w),float(b),cost))\n",
        "        \n",
        "\n",
        "[문제] linear regression 학습을 통해서 입력값에 대한 예측값을 출력하세요. \n",
        "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "\n",
        "x1 x2 x3  y(예측할값)\n",
        "-------------\n",
        "73 80 75  152\n",
        "93 88 93  185\n",
        "89 91 90  180\n",
        "96 98 100 196\n",
        "73 66 70  142\n",
        "\n",
        "x1_data = [73,93,89,96,73]\n",
        "x2_data = [80,88,91,98,66]\n",
        "x3_data = [75,93,90,100,70]\n",
        "y_data = [152,185,180,196,142]\n",
        "x1 = tf.constant(x1_data,tf.float32)\n",
        "x2 = tf.constant(x2_data,tf.float32)\n",
        "x3 = tf.constant(x3_data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "\n",
        "w1 = tf.Variable(tf.random.normal([1]),name='weight1')\n",
        "w2 = tf.Variable(tf.random.normal([1]),name='weight2')\n",
        "w3 = tf.Variable(tf.random.normal([1]),name='weight3')\n",
        "b = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rante = 0.000001 # 수정하면서\n",
        "\n",
        "for i in range(3000):\n",
        "    with tf.GradientTape() as tape: # GradienTape() : 미분을 대신 해주는 함수? with문 필요\n",
        "        hypothesis = w1*x1 + w2*x2 + w3*x3 +b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis-y)) # mse\n",
        "    w1_grad,w2_grad,w3_grad,b_grad = tape.gradient(cost,[w1,w2,w3,b])\n",
        "    w1.assign_sub(learning_rate*w1_grad) # w= w - learning_rate*(∂cost(loss) / ∂w) # weight 값의 수정\n",
        "    w2.assign_sub(learning_rate*w2_grad)\n",
        "    w3.assign_sub(learning_rate*w3_grad)\n",
        "    b.assign_sub(learning_rate*b_grad) # b= b - learning_rate*(∂cost(loss) / ∂b) # bias 값의 수정\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        print(\"step:{},weight1:{},weight2:{},weight3:{}, bias:{}, cost:{}\".format(i,float(w1),float(w2),float(w3),float(b),cost))\n",
        "        \n",
        "\n",
        "#------------------------------------------\n",
        "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "\n",
        "x1 x2 x3  y(예측할값)\n",
        "-------------\n",
        "73 80 75  152\n",
        "93 88 93  185\n",
        "89 91 90  180\n",
        "96 98 100 196\n",
        "73 66 70  142\n",
        "\n",
        "x1_data = np.array([73,93,89,96,73]).reshape(-1,1) # 5행 1열\n",
        "x2_data = np.array([80,88,91,98,66]).reshape(-1,1)\n",
        "x3_data = np.array([75,93,90,100,70]).reshape(-1,1)\n",
        "y_data = np.array([152,185,180,196,142]).reshape(-1,1)\n",
        "\n",
        "\n",
        "data = np.hstack((x1_data, x2_data, x3_data)) # 열로 합치는 함수( numpy.hstack)\n",
        "# np.vstack((x1_data, x2_data, x3_data)) # 행으로 합치는 함수 한개의 열로 출력됨\n",
        "\n",
        "x = tf.constant(data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([3,1]),name='weight') # 3행 1열(x가 5행3열로 행렬곱을 수행하기위해서)\n",
        "b = tf.Variable(tf.random.normal([1,]),name='bias') # 1차원 1개의 값\n",
        "\n",
        "learning_rate = 0.00001 # 수정하면서\n",
        "\n",
        "for i in range(30000):\n",
        "    with tf.GradientTape() as tape: # GradienTape() : 미분을 대신 해주는 함수? with문 필요\n",
        "        hypothesis = tf.matmul(x,w) + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis-y)) # mse\n",
        "    w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "    w.assign_sub(learning_rate*w_grad) # w= w - learning_rate*(∂cost(loss) / ∂w) # weight 값의 수정\n",
        "    b.assign_sub(learning_rate*b_grad) # b= b - learning_rate*(∂cost(loss) / ∂b) # bias 값의 수정\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        print(\"step:{},weight:{}, bias:{}, cost:{}\".format(i,w.numpy,float(b),cost))\n",
        "\n",
        "\n",
        "new = np.array([73,93,72]).reshape(1,3)\n",
        "new.shape\n",
        "new = tf.constant(new,tf.float32)\n",
        "new\n",
        "predict = tf.matmul(new,w) + b # 예측 ( 위에서 구한 w와 b를 이용)\n",
        "predict\n",
        "\n",
        "# 회귀분석\n",
        "독립변수, 입력변수, 설명변수 : 수치형 자료\n",
        "종속변수, 결과변수, 목표변수 : 수치형 자료(연속형)\n",
        "\n",
        "예측값(연속형) = weight * 입력값 + bias\n",
        "\n",
        "LOGIT(Logistic Regression), binary classification\n",
        "- 이분법을 기준으로 분류\n",
        "- 로지스틱 회귀분석은 값이 연속적인 값을 가지더라도 로지스틱함수의 결과값은 0 ~ 1 사이의 값을 갖도록 설계되어 있기 때문에 이분법적인 분류 문제를 해결할 수 있다.\n",
        "\n",
        "\n",
        "독립변수, 입력변수, 설명변수 : 수치형 자료\n",
        "종속변수, 결과변수, 목표변수 : 범주형 자료\n",
        "\n",
        "예측값(0,1) = sigmoid(weight * 입력값 + bias) # 0 또는 1사이의 값에서 기준점(보통0.5)을 잡고 기준점보다 크면 1 작으면 0 \n",
        "\n",
        "# sigmoid\n",
        "- 0과 1사이의 실수값으로 전달하는 함수\n",
        "- e 자연상수 : 2.7182.....\n",
        "\n",
        "def sigmoid(arg):\n",
        "    return 1/(1+np.exp(-arg))\n",
        "\n",
        "sigmoid(100)\n",
        "sigmoid(-1000)\n",
        "sigmoid(0.5)\n",
        "sigmoid(2)\n",
        "sigmoid(10000)\n",
        "\n",
        "목표 1 : 학습된 결과는 0이 나왔다. 틀린정답을 수행?\n",
        "이를 해결하기 위해서 weight 값, bias를 조정해야 한다.\n",
        "\n",
        "binary classification 에서는 어떤 cost 함수를 사용해야 하는지?\n",
        "binary crossentropy cost function을 사용한다\n",
        "\n",
        "z = 1\n",
        "z = 0\n",
        "target = 1 : -target * np.log(z) # 실제값이 1일때 분류 오차 함수\n",
        "target = 0 : -(1-target) * np.log(1-z) # 실제값이 0일때 분류 오차 함수\n",
        "\n",
        "target = 1\n",
        "z = 1\n",
        "- target * np.log(z)\n",
        "\n",
        "target = 1\n",
        "z = 1\n",
        "- target * np.log(z)\n",
        "\n",
        "target = 0\n",
        "z = 0\n",
        "-(1-target) * np.log(1-z)    \n",
        "\n",
        "target = 0\n",
        "z = 1\n",
        "-(1-target) * np.log(1-z)    \n",
        "\n",
        "\n",
        "h = sigmoid(weight * x + bias)\n",
        "\n",
        "target = 1\n",
        "h = 0.1\n",
        "\n",
        "if target == 1:\n",
        "    -np.log(z)\n",
        "else:\n",
        "    -np.log(1-z)\n",
        "\n",
        "target = 1\n",
        "h = 0.1\n",
        "(-target * np.log(h)) - ((1-target) * np.log(1-h))\n",
        "\n",
        "target = 1\n",
        "h = 0.999\n",
        "(-target * np.log(h)) - ((1-target) * np.log(1-h))\n",
        "\n",
        "target = 0\n",
        "h = 0.001\n",
        "(-target * np.log(h)) - ((1-target) * np.log(1-h))\n",
        "\n",
        "target = 0\n",
        "h = 0.9\n",
        "(-target * np.log(h)) - ((1-target) * np.log(1-h))\n",
        "\n",
        "\n",
        "# binary crossentropy cost function\n",
        "tf.reduce_mean(-tf.reduce_sum(target * tf.math.log(h) + (1-target) * tf.math.log(1-h)))\n",
        "\n",
        "# 분류\n",
        "x_data = [[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]]\n",
        "y_data = [[0],[0],[0],[0],[0],[1],[1],[1],[1],[1]]\n",
        "\n",
        "x = tf.constant(x_data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([1,1]),name='weight') # 2차원\n",
        "b = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rate = 0.01 # 수정하면서\n",
        "\n",
        "for i in range(20000):\n",
        "    with tf.GradientTape() as tape: # GradienTape() : 미분을 대신 해주는 함수? with문 필요\n",
        "        hypothesis = tf.sigmoid(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis) + (1-y) * tf.math.log(1-hypothesis))) # binary crossentropy cost function\n",
        "    w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "    w.assign_sub(learning_rate*w_grad) # w= w - learning_rate*(∂cost(loss) / ∂w) # weight 값의 수정\n",
        "    b.assign_sub(learning_rate*b_grad) # b= b - learning_rate*(∂cost(loss) / ∂b) # bias 값의 수정\n",
        "    \n",
        "    if i % 1000 == 0 :\n",
        "        print(\"step:{},weight:{}, bias:{}, cost:{}\".format(i,float(w),float(b),cost))\n",
        "\n",
        "\n",
        "def sigmoid(arg):\n",
        "    return 1/(1+np.exp(-arg))\n",
        "\n",
        "predict = sigmoid(float(w) * 10 + float(b)) # 예측, 10을 입력시 나오는 값 ( 정답은 1 ) #tf.sigmoid(float(w) * 10 + float(b))\n",
        "predict \n",
        "\n",
        "predict = sigmoid(float(w) * 3 + float(b)) # 3을 입력했을 때 나오는 값 ( 정답은  0 )\n",
        "predict \n",
        " \n",
        "[문제] XOR Logistic regression 을 이용해서 분류해주세요. 단 신경망을 이용하세요.\n",
        "x1 x2 y\n",
        "0  0  0\n",
        "0  1  1\n",
        "1  0  1\n",
        "1  1  0\n",
        "\n",
        "x1_data = np.array([0,0,1,1]).reshape(-1,1)\n",
        "x2_data = np.array([0,1,0,1]).reshape(-1,1)\n",
        "y_data = np.array([0,1,1,0]).reshape(-1,1)\n",
        "\n",
        "data = np.hstack((x1_data, x2_data))\n",
        "\n",
        "x = tf.constant(data,tf.float32)\n",
        "y = tf.constant(y_data,tf.float32)\n",
        "w = tf.Variable(tf.random.normal([2,1]),name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]),name='bias')\n",
        "\n",
        "learning_rate = 0.01 # 수정하면서\n",
        "\n",
        "for i in range(2000):\n",
        "    with tf.GradientTape() as tape: # GradienTape() : 미분을 대신 해주는 함수? with문 필요\n",
        "        hypothesis = tf.sigmoid(tf.matmul(x,w) + b)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(hypothesis) + (1-y) * tf.math.log(1-hypothesis))) # binary crossentropy cost function\n",
        "    w_grad,b_grad = tape.gradient(cost,[w,b])\n",
        "    w.assign_sub(learning_rate*w_grad) # w= w - learning_rate*(∂cost(loss) / ∂w) # weight 값의 수정\n",
        "    b.assign_sub(learning_rate*b_grad) # b= b - learning_rate*(∂cost(loss) / ∂b) # bias 값의 수정\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        print(\"step:{},weight:{}, bias:{}, cost:{}\".format(i,float(w),float(b),cost))\n"
      ]
    }
  ]
}