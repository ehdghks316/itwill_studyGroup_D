{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_21파이썬(결정트리DecisionTree,).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZlXbHZzlJ7BKGPn6kRUvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_21%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%ACDecisionTree%2C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqExz0omaFgo"
      },
      "outputs": [],
      "source": [
        "★ Decision Tree(결정트리, 의사결정나무)\n",
        "- 머신러닝 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘\n",
        "- 데이터 있는 규칙을 학습을 통해 자동으로 찾아내는 트리기반의 분류규칙을 만든다.\n",
        "- if else 기반\n",
        "- 지도학습\n",
        "- 스무고개 세임과 유사하며 룰 기반의 프로그램\n",
        "- CART(Classification And Regression Trees)  : 지니지수(Gini index)\n",
        "- C4.5, C5.0 : 엔트로피(Entropy index)\n",
        "- 분류알고리즘\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "from collections import Counter\n",
        "import math\n",
        "성별 = ['남','남','남','남','남','남','여','여','여','여']\n",
        "결혼 = ['기혼','미혼','기혼','기혼','미혼','미혼','미혼','기혼','미혼','기혼']\n",
        "구매 = ['예','예','예','아니요','예','예','아니요','아니요','아니요','아니요']\n",
        "customer = DataFrame({'성별':성별,'결혼':결혼,'구매':구매})\n",
        "customer\n",
        "\n",
        "  성별  결혼   구매(분류의 정답)\n",
        "0  남  기혼    예\n",
        "1  남  미혼    예\n",
        "2  남  기혼    예\n",
        "3  남  기혼  아니요\n",
        "4  남  미혼    예\n",
        "5  남  미혼    예\n",
        "6  여  미혼  아니요\n",
        "7  여  기혼  아니요\n",
        "8  여  미혼  아니요\n",
        "9  여  기혼  아니요\n",
        "\n",
        "result1 = pd.crosstab(customer['구매'],customer['성별'])\n",
        "result1\n",
        "\n",
        "result2 = pd.crosstab(customer['구매'],customer['결혼'])\n",
        "result2\n",
        "\n",
        "if  성별 == '남':\n",
        "    if 결혼 == '미혼':\n",
        "        구매 = '예'\n",
        "    else:\n",
        "        구매 = '예','아니요'\n",
        "else:\n",
        "    구매 = '아니요'\n",
        "    \n",
        "CART(Classification And Regression Trees)  : 지니지수(Gini index)\n",
        "- 불확실성을 의미\n",
        "- 지니지수는 얼마나 불확실한가(얼마나 많이 섞여 있는가?)\n",
        "- 지니지수 0이라는 것은 불확실성이 0이라는 것으로 같은 특성을 가진 데이터끼리 잘 모여 있다는 의미( 지니지수가 크면 불확실성이 커서 잘 모여있지 않다는 의미)\n",
        "- 엔트로피와 거의 동일하지만 훨씬 더 빨리 계산할 수 있다. # 지니와 엔트로피중 지니를 사용하는 이유는 계산이 빠르기 때문 ( 엔트로피는 로그 때문에 더 오래걸림 )\n",
        "식\n",
        "1 - ∑P²\n",
        "\n",
        "성별 지니지수\n",
        "G(상위) = 1 - (5/10)**2 - (5/10)**2 = 0.5\n",
        "G(남자) =  1 - (5/6)**2 - (1/6)**2 = 0.278\n",
        "G(여자) = 1 - (0/4)**2 - (4/4)**2 = 0\n",
        "G(성별) = (6/10)*0.278 + (4/10)*0 = 0.1668 # 0.5 -> 0.1668 불확실성이 감소되었다.\n",
        "\n",
        "결혼 지니지수\n",
        "G(상위) = 1 - (5/10)**2 - (5/10)**2 = 0.5\n",
        "G(기혼) =  1 - (2/5)**2 - (3/5)**2 = 0.48\n",
        "G(미혼) = 1 - (3/5)**2 - (2/5)**2 = 0.48\n",
        "G(결혼여부) = (5/10)*0.48 + (5/10)*0.48 = 0.48 # 0.5 -> 0.48 불확실성이 별로 감소하지 않았다.\n",
        "\n",
        "\n",
        "C4.5, C5.0 : 엔트로피(Entropy index)\n",
        "- Entropy는 주어진 데이터 집합의 혼잡도를 의미한다.\n",
        "- 서로 다른 값이 섞여있으면 엔트로피가 높고 같은 값이 섞여 있으면 엔트로피는 낮다.\n",
        "식\n",
        "-∑P*log2(P)\n",
        "\n",
        "성별 엔트로피\n",
        "G(상위) = -(5/10)*math.log2(5/10) - (5/10)*math.log2(5/10) = 1\n",
        "G(남자) = -(5/6)*math.log2(5/6) - (1/6)*math.log2(1/6) = 0.65\n",
        "G(여자) = -(0/4)*math.log2(0/4) - (4/4)*math.log2(4/4) = 0\n",
        "G(성별) = (6/10)*0.65 + (4/10)*0 = 0.39 # 1 -> 0.39 불확실성이 감소되었다.\n",
        "IG(성별) = E(상위) - E(성별) = 1-0.39 = 0.61 # 0.61만큼 감소했다\n",
        "IG(Information Gain) : 정보의 가치\n",
        "정보의 가치가 크면 좋다.\n",
        "\n",
        "결혼 엔트로피\n",
        "G(상위) = -(5/10)*math.log2(5/10) - (5/10)*math.log2(5/10) = 1\n",
        "G(기혼) =   -(2/5)*math.log2(2/5) - (3/5)*math.log2(3/5) = 0.97\n",
        "G(미혼) =-(3/5)*math.log2(3/5) - (2/5)*math.log2(2/5) = 0.97\n",
        "G(결혼여부) = (5/10)*0.97 + (5/10)*0.97 = 0.8215 # 1 -> 0.97 \n",
        "IG(결혼) = E(상위) - E(결혼) = 1 - 0.97 = 0.03 # 0.03 만큼만 감소했다. 즉 구분하지 못했다.\n",
        "\n",
        "\n",
        "성별 = ['남','남','남','남','남','남','여','여','여','여']\n",
        "결혼 = ['기혼','미혼','기혼','기혼','미혼','미혼','미혼','기혼','미혼','기혼']\n",
        "구매 = ['예','예','예','아니요','예','예','아니요','아니요','아니요','아니요']\n",
        "customer = DataFrame({'성별':성별,'결혼':결혼,'구매':구매})\n",
        "customer\n",
        "\n",
        "※카테고리형데이터(categorical data)를 수치형데이터(numerical data)로 변환\n",
        "1.\n",
        "customer['성별'].map({'남':0,'여':1}) # 수치로 구분\n",
        "2.\n",
        "customer['성별'].astype('category').cat.codes #카테고리형태로 바꾸고 cat.codes를 수행하면 자동으로 수치형으로 바꿔줌\n",
        "3. 레이블 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "gender_le = LabelEncoder()\n",
        "gender = gender_le.fit_transform(customer['성별'])\n",
        "gender\n",
        "gender_le.classes_\n",
        "gender_le.inverse_transform(gender) # 역변환\n",
        "\n",
        "marry_le = LabelEncoder()\n",
        "marry = marry_le.fit_transform(customer['결혼'])\n",
        "marry_le.classes_\n",
        "marry_le.inverse_transform(marry)\n",
        "\n",
        "4. one hot encoding\n",
        "pd.get_dummies(customer['성별']) # 각 성별이 따로 컬럼을 만듬\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot_en = OneHotEncoder()\n",
        "gender_onehot = onehot_en.fit_transform(customer[['성별']])\n",
        "gender_onehot.toarray()\n",
        "DataFrame(gender_onehot.toarray().astype('int'))\n",
        "\n",
        "df = DataFrame([gender,marry]).T\n",
        "df.columns = ['성별','결혼']\n",
        "df['구매'] = customer['구매']\n",
        "df\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# model = DecisionTreeClassifier(criterion='gini',max_depth=2) # default\n",
        "model = DecisionTreeClassifier(criterion='entropy',max_depth=1) # max_depth=파라미터값 -> 트리의 depth를 몇번할지\n",
        "model.fit(df[['성별','결혼']],df['구매'])\n",
        "y_predict = model.predict(df[['성별','결혼']])\n",
        "(y_predict == df['구매']).mean()\n",
        "model.score(df[['성별','결혼']],df['구매'])\n",
        "\n",
        "model.classes_ \n",
        "model.feature_importances_ # 피쳐의 중요도\n",
        "\n",
        "model.predict([[0,1]])[0]\n",
        "model.predict([[0,0]])[0]\n",
        "model.predict([[1,1]])[0]\n",
        "model.predict([[1,0]])[0]\n",
        "\n",
        "★ 의사결정나무 시각화\n",
        "1. 다운로드 받아서 설치, 환경path 설정 꼭 체크(2번째 것)\n",
        "https://graphviz.gitlab.io/_pages/Download/Download_windows.html \n",
        "\n",
        "2. 아나콘다(관리자권한)에서 설치 수행\n",
        "(base) C:\\WINDOWS\\system32>pip install graphviz\n",
        "\n",
        "import graphviz # 결정트리 시각화 라이브러리\n",
        "from sklearn.tree import export_graphviz \n",
        "\n",
        "dot_data = export_graphviz(model,out_file=None,feature_names=['성별','결혼'],class_names=model.classes_,\n",
        "                           filled=True,rounded=True,special_characters=True)\n",
        "graphviz.Source(dot_data)\n",
        "\n",
        "\n",
        "# iris데이터 활용\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "iris.iloc[:,0:-1]\n",
        "iris.Name\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.iloc[:,0:-1],iris.Name,test_size=0.2)\n",
        "x_train.shape\n",
        "\n",
        "from collections import Counter\n",
        "Counter(y_train)\n",
        "Counter(y_test)\n",
        "\n",
        "\n",
        "iris_model = DecisionTreeClassifier(max_depth=3) # max_depth를 적당히 하지 않으면 과적합에 빠질 수 있음(너무 많이도, 너무 적게도 x), 테스트 데이터 정확도를 보면서 확인\n",
        "iris_model.fit(x_train,y_train)\n",
        "iris_model.classes_\n",
        "iris_model.feature_importances_ # 주로 사용된 중요 피처(여기서는 컬럼)확인\n",
        "\n",
        "y_pred = iris_model.predict(x_test)\n",
        "(y_pred==y_test).mean()\n",
        "\n",
        "iris_model.score(x_test,y_test)\n",
        "\n",
        "dot_data = export_graphviz(iris_model,out_file=None,feature_names=iris.columns[:-1],class_names=iris_model.classes_,\n",
        "                           filled=True,rounded=True,special_characters=True)\n",
        "graphviz.Source(dot_data)\n",
        "\n",
        "for i in range(0,4):\n",
        "    print('{} : {}'.format(iris.columns[:-1][i],iris_model.feature_importances_[i]))\n",
        "\n",
        "for i , j in zip(iris.columns[:-1], iris_model.feature_importances_):\n",
        "    print('{0} : {1:.3f}'.format(i,j)) # 포맷의 순서 {0},{1},...안에 순서를 넣어줄 수 있고 생략가능\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar(iris.columns[:-1],iris_model.feature_importances_)\n",
        "plt.barh(iris.columns[:-1],iris_model.feature_importances_)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.barplot(x=iris.columns[:-1],y = iris_model.feature_importances_) # 수직\n",
        "sns.barplot(x=iris_model.feature_importances_,y = iris.columns[:-1]) # 수평\n",
        "\n",
        "\n",
        "\n",
        "# 최적의 파라미터(max_depth 찾기) -> grid사용\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.iloc[:,0:-1],iris.Name,test_size=0.2)\n",
        "iris_model = DecisionTreeClassifier()\n",
        "parameters = {'max_depth' : [1,2,3,4,5,6,7,8,9,10]} # 파라미터값은 딕셔너리형식으로 값은 1개라도 무조건 리스트로\n",
        "grid_de = GridSearchCV(estimator=iris_model, param_grid=parameters,cv=5,refit=True,return_train_score=True)\n",
        "grid_de.fit(x_train,y_train)\n",
        "pd.DataFrame(grid_de.cv_results_)[['params','mean_test_score','rank_test_score']]\n",
        "\n",
        "grid_de.best_params_ # 최적의 파라미터 값\n",
        "grid_de.best_score_ # 최고 정확도\n",
        "\n",
        "de_estimator = grid_de.best_estimator_ # GridSearchVC의 refit으로 이미 학습된 estimator를 반환 / 만든 최적의 모델(사용될)\n",
        "y_pred = de_estimator.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "\n",
        "\n",
        "# 강사님의 최적의 파라미터 찾기\n",
        "#iris_model = DecisionTreeClassifier(min_samples_split=6) # 샘플 수가 최소 6개 이상일 때만 split하겠다는 옵션\n",
        "#iris_model = DecisionTreeClassifier(min_samples_leaf=10) # 말단이 10이하일때 끝내기\n",
        "#iris_model = DecisionTreeClassifier(max_leaf_nodes=3) # leaf의 최대개수 3개\n",
        "#iris_model = DecisionTreeClassifier(max_features='log2')\n",
        "iris_model = DecisionTreeClassifier()\n",
        "iris_model.fit(x_train,y_train)\n",
        "iris_model.score(x_train,y_train)\n",
        "iris_model.score(x_test,y_test)\n",
        "dot_data = export_graphviz(iris_model,out_file=None,feature_names=iris.columns[:-1],class_names=iris_model.classes_,\n",
        "                           filled=True,rounded=True,special_characters=True)\n",
        "graphviz.Source(dot_data)\n",
        "\n",
        "※ max_depth : 트리의 최대 깊이, 기본값 None\n",
        "※ min_samples_split : 노드로 분할하기 위한 최소한의 샘플데이터 수, 기본값 2\n",
        "min_samples_leaf : leaf가 되기 위한 최소한의 샘플데이터 수\n",
        "max_leaf_nodes : leaf의 최대개수\n",
        "max_features : 분할하기 위해서 최대로 사용할 수 있는 피처 개수, \n",
        "                기본값 None(모든 feature를 다 사용)\n",
        "                'sqrt' : sqrt(피처의 수),\n",
        "                'auto' : sqrt와 동일\n",
        "                'log' : log2(피처의 수)\n",
        "                \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "iris.info()\n",
        "iris.iloc[:,0:-1]\n",
        "iris.Name\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.iloc[:,0:-1],iris.Name,test_size=0.2)\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "parameters = {'max_depth':[3,4,5],'min_samples_split':[3,4]}\n",
        "grid_tree = GridSearchCV(dtree,param_grid=parameters,cv=5,refit=True,return_train_score=True)\n",
        "grid_tree.fit(x_train,y_train)\n",
        "\n",
        "score_df = pd.DataFrame(grid_tree.cv_results_)\n",
        "score_df[['params','mean_test_score','rank_test_score']]\n",
        "\n",
        "print('최적 파라미터',grid_tree.best_params_)\n",
        "print('최고 정확도',grid_tree.best_score_)\n",
        "\n",
        "y_pred = grid_tree.predict(x_test)\n",
        "accuracy_score(y_pred,y_test)\n",
        "\n",
        "tree_model = grid_tree.best_estimator_\n",
        "accuracy_score(tree_model.predict(x_test),y_test)\n",
        "\n",
        "tree_model.feature_importances_\n"
      ]
    }
  ]
}