{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_1파이썬(스크래핑,크롤링2).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQU0PShVsXkrsnOwAgXPz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_1%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91%2C%ED%81%AC%EB%A1%A4%EB%A7%812).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POBK0SZib1F1"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<ul id = '조선왕'>\n",
        "    <li id = '태조'> '이성계'</li>\n",
        "    <li id = '정종'> '이방과'</li>\n",
        "    <li id = '태종'> '이방원'</li>\n",
        "    <li id = '세종'> '이도'</li>\n",
        "    <li class = '문종'> '이향'</li>\n",
        "</ul>\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.find(id='세종').text\n",
        "soup.find(id='세종').string\n",
        "soup.find('li',id='세종').text\n",
        "soup.find('li',{'id':'세종'}).text\n",
        "soup.find(class_='문종').text\n",
        "soup.find('li',class_='문종').text\n",
        "soup.find('li',{'class':'문종'}).text\n",
        "\n",
        "soup.find('li')\n",
        "soup.find_all('li')\n",
        "soup.findAll('li')\n",
        "\n",
        "for i in soup.find_all('li'):\n",
        "    print(i.string)\n",
        "for i in soup.findAll('li'):\n",
        "    print(i.string)\n",
        "    \n",
        "\n",
        "# css(cascading style sheets) 선택자\n",
        "id => #\n",
        "class => .\n",
        "\n",
        "soup.select_one('li')\n",
        "soup.select_one('ul li')\n",
        "soup.select_one('ul li#세종')\n",
        "soup.select_one('li#정종')\n",
        "soup.select_one('li#정종').text\n",
        "soup.select_one('li[id=\"세종\"]') # 속성표현\n",
        "soup.select_one('li[class=\"문종\"]') # 속성표현\n",
        "soup.select_one('li.문종')\n",
        "soup.select_one('ul#조선왕 li.문종')\n",
        "soup.select_one('#조선왕 li.문종')\n",
        "soup.select_one('#조선왕 > li.문종')\n",
        "soup.select_one('#조선왕 > li.문종').text\n",
        "soup.select_one('#조선왕 > li.문종').string\n",
        "soup.select_one('#조선왕 > li.문종').get_text()\n",
        "\n",
        "soup.select_one('ul li:nth-of-type(1)').text # 위치를 이용해서 찾기\n",
        "soup.select_one('ul li:nth-of-type(2)').text\n",
        "soup.select_one('ul li:nth-of-type(3)').text\n",
        "soup.select_one('ul li:nth-of-type(4)').text\n",
        "soup.select_one('ul li:nth-of-type(5)').text\n",
        "\n",
        "html = \"\"\"\n",
        "<div class = 'king of choson'>\n",
        "    <p>'이성계'</p>\n",
        "    <span>'태조'</span>\n",
        "    <p>'이방과'</p>\n",
        "    <span>'정종'</span>\n",
        "    <p>'이방원'</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.select_one('.king of choson')# 속성값이 여러단어일 때는 바로표현 불가능\n",
        "soup.select_one('div[class=\"king of choson\"]') \n",
        "\n",
        "soup.select_one('div[class=\"king of choson\"] p') \n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-of-type(1)').text # p테그의 id값이 없을 때 원하는 위치에 있는 태그 출력\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-of-type(2)').text\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-of-type(3)').text\n",
        "\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-child(1)').text\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-child(2)').text # 오류 2번째태그는 span이기 대문\n",
        "soup.select_one('div[class=\"king of choson\"] span:nth-child(2)').text\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-child(3)').text\n",
        "soup.select_one('div[class=\"king of choson\"] span:nth-child(4)').text\n",
        "soup.select_one('div[class=\"king of choson\"] p:nth-child(5)').text\n",
        "\n",
        "soup.select_one('div[class=\"king of choson\"] span:nth-of-type(2)').text\n",
        "soup.select_one('div[class=\"king of choson\"] span:nth-of-type(4)').text\n",
        "\n",
        "soup.select('div[class=\"king of choson\"] p')  # find_all\n",
        "\n",
        "for i in soup.select('div[class=\"king of choson\"] p'):\n",
        "    print(i.string)\n",
        "for i in soup.select('div[class=\"king of choson\"] p'):\n",
        "    print(i.text)\n",
        "for i in soup.select('div[class=\"king of choson\"] p'):\n",
        "    print(i.get_text())  \n",
        "\n",
        "[문제176] 동아일보 '4차혁명' 뉴스기사 검색을 한 후 제목, 뉴스내용, 메일주소 정보를 데이터프레임으로 저장한 후 파일로 생성해주세요.\n",
        "css 선택자를 이용해서 스크래핑해주세요.\n",
        "from urllib.request import urlopen\n",
        "import urllib\n",
        "from pandas import Series, DataFrame\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "url = 'https://www.donga.com/news/article/all/20220216/111836516/1'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "# 제목\n",
        "title = soup.select_one('h1[class=\"title\"]').text\n",
        "\n",
        "# 뉴스내용\n",
        "text = soup.select_one('div.article_txt').text\n",
        "\n",
        "# 메일주소 정보\n",
        "mail = re.search('[A-z0-9.]+@[A-z0-9.]+',text)\n",
        "\n",
        "\n",
        "\n",
        "# url 추출\n",
        "url_total = []\n",
        "for i in range(1,17,15):\n",
        "    url = 'https://www.donga.com/news/search?check_news='+str(i)+'&more=1&sorting=1&range=1&search_date=&v1=&v2=&query=4%EC%B0%A8%ED%98%81%EB%AA%85'\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')    \n",
        "    s_p = soup.select('p.tit')\n",
        "    for j in s_p:\n",
        "        url_total.append(j.find('a')['href'])\n",
        "    time.sleep(1)\n",
        "\n",
        "# Datafram에 제목, 뉴스내용, 메일주소 넣기\n",
        "title = []\n",
        "text = []\n",
        "mail = []\n",
        "for i in url_total:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    \n",
        "    title_ = soup.select_one('h1[class=\"title\"]').text\n",
        "    text_ = soup.select_one('div.article_txt').text\n",
        "    try:\n",
        "        mail_ = re.search('[A-z0-9.]+@[A-z0-9.]+',text_).group()\n",
        "    except:\n",
        "        mail_ = ''\n",
        "    title.append(title_)\n",
        "    text.append(text_)\n",
        "    mail.append(mail_)\n",
        "    \n",
        "    time.sleep(1)\n",
        "        \n",
        "# DataFrame(데이터프레임 뼈대)\n",
        "df = DataFrame({'제목' : title,\n",
        "           '내용' : text,\n",
        "           '메일' : mail})\n",
        "\n",
        "##########강사님\n",
        "urllib.parse.quote('4차혁명') # '4차혁명' 이 단어를 컴퓨터언어로 parse\n",
        "'4%EC%B0%A8%ED%98%81%EB%AA%85'\n",
        "urllib.parse.unquote('4%EC%B0%A8%ED%98%81%EB%AA%85')\n",
        "\n",
        "new_url = []\n",
        "for i in range(1,17,15):\n",
        "    url = 'https://www.donga.com/news/search?check_news='+str(i)+'&more=1&sorting=1&range=1&search_date=&v1=&v2=&query=4%EC%B0%A8%ED%98%81%EB%AA%85'\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    p = soup.select('p[class=\"tit\"]')\n",
        "    \n",
        "    for u in p:\n",
        "        new_url.append(u.select_one('a')['href'])\n",
        "    time.sleep(2)\n",
        "\n",
        "new_url\n",
        "\n",
        "contents = DataFrame(columns=['title','news'])\n",
        "for i in new_url:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    title = soup.select_one('h1[class=\"title\"]').string.strip()\n",
        "    news = soup.select_one('div[class=\"article_txt\"]').text.strip()\n",
        "    contents = contents.append({'title':title, 'news':news},ignore_index=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "contents\n",
        "\n",
        "contents['news'][1]\n",
        "re.findall('[a-zA-Z0-9._]+@[a-zA-Z.]+',contents['news'][0])\n",
        "re.findall('[a-zA-Z0-9._]+@[a-zA-Z.]+',contents['news'][1])\n",
        "...\n",
        "\n",
        "contents['mail'] = contents['news'].apply(lambda x : re.findall('[a-zA-Z0-9._]+@[a-zA-Z.]+',x))\n",
        "contents['mail'] = contents['news'].apply(lambda x : ','.join(re.findall('[a-zA-Z0-9._]+@[a-zA-Z.]+',x)))\n",
        "contents['mail']\n",
        "\n",
        "contents.to_csv('c:/data/contents.csv',index=False) # 파일로 만들기\n",
        "\n",
        "new_url[0]\n",
        "html = urlopen(new_url[0])\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "news = soup.select_one('div[class=\"article_txt\"]')\n",
        "news.select('div')\n",
        "news.select_one('script').extract() # 수행시 즉시 제거됨\n",
        "news.select_one('script')\n",
        "\n",
        "for i in news.select('div'): # div태그에 있는 값들 전부 제거\n",
        "    i.extract()\n",
        "news.text\n",
        "\n",
        "\n",
        "new_url[:3]\n",
        "\n",
        "# url에 없는 url들어올 때 \n",
        "from urllib.error import URLError, HTTPError\n",
        "\n",
        "url = ['https://www.donga.com/news/article/all/20220216/111836516/1',\n",
        " 'https://www.do.com/news/article/all/20210524/107074586/1', # 없는 url 들어왔을 때\n",
        " 'https://finance.daum.net/api/search/ranks?limit=10', # 서버에서 막는 웹( 403forbidden)\n",
        " 'https://www.donga.com/news/article/all/20201104/103789830/1' ]\n",
        "\n",
        "contents = DataFrame(columns=['title','news'])\n",
        "failed_url = []\n",
        "httperror_url = []\n",
        "\n",
        "for i in url:\n",
        "    try:\n",
        "        html = urlopen(i)\n",
        "        soup = BeautifulSoup(html,'html.parser')\n",
        "        title = soup.select_one('h1[class=\"title\"]').string.strip()\n",
        "        news = soup.select_one('div[class=\"article_txt\"]').text.strip()\n",
        "        contents = contents.append({'title':title, 'news':news},ignore_index=True)\n",
        "        time.sleep(2)\n",
        "    \n",
        "    except HTTPError as error: # HTTPError와 URLError의 위치를 바꾸면URLError로만 실행됨 ( 우선순위문제 )\n",
        "        print('HTTPError Code : {}'.format(error.code))\n",
        "        httperror_url.append((i,error.code))\n",
        "        \n",
        "    except URLError as error:\n",
        "        print('url error reason',error)\n",
        "        failed_url.append(i)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        print('http status code : {}'.format(html.getcode()))\n",
        "\n",
        "#  'https://finance.daum.net/api/search/ranks?limit=10', # 서버에서 막는 웹( 403forbidden) 이 웹을 사요할 수 있게 요청하기\n",
        "# 아나콘다 프롬프트 관리자권한으로 실행 후 밑에 코드 실행        \n",
        "pip install fake_useragent\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "from urllib.request import urlopen\n",
        "import urllib.request as req\n",
        "ua = UserAgent() # (첫번째는 오류남)오류나도 다시 실행하면 됨\n",
        "print(ua.ie)\n",
        "print(ua.msie)\n",
        "print(ua.chrome)\n",
        "print(ua.safari)\n",
        "print(ua.random)\n",
        "headers = {'User-Agent' : ua.ie, # ua.ie(...다른것도 상관없음)\n",
        "            'referer' : 'https://finance.daum.net/' }\n",
        "\n",
        "\n",
        "url = 'https://finance.daum.net/api/search/ranks?limit=10'\n",
        "res = req.urlopen(req.Request(url,headers=headers)).read().decode('utf-8')\n",
        "res\n",
        "\n",
        "import json\n",
        "top10 = DataFrame(json.loads(res)['data']) # value값만 추출\n",
        "top10\n",
        "top10.info()\n",
        "\n",
        "[문제177] 다음사이트에서 코스피200 편입된 종목을 수집하세요.\n",
        "headers = {'User-Agent' : ua.random,\n",
        "          'referer' : 'https://finance.daum.net/domestic/kospi200'}\n",
        "\n",
        "url = 'https://finance.daum.net/api/trend/included_stocks?page=1&perPage=10&fieldName=changeRate&order=desc&market=KOSPI_200&pagination=true'\n",
        "res = req.urlopen(req.Request(url,headers=headers)).read().decode('utf-8')\n",
        "res\n",
        "\n",
        "kospi200 = DataFrame()\n",
        "failed_url= []\n",
        "for i in  range(1,22):\n",
        "    try:\n",
        "        url = 'https://finance.daum.net/api/trend/included_stocks?page='+str(i)+'&perPage=10&fieldName=changeRate&order=desc&market=KOSPI_200&pagination=true'\n",
        "        res = req.urlopen(req.Request(url,headers=headers)).read().decode('utf-8')\n",
        "        kospi200 =kospi200.append(DataFrame(json.loads(res)['data']),ignore_index=True)\n",
        "        time.sleep(2)\n",
        "    except URLError as error:\n",
        "        failed.url.append(url)\n",
        "        time.sleep(2)\n",
        "        \n",
        "kospi200\n",
        "\n",
        "[문제178] 네이버 영화 리뷰정보를 수집하세요.(평점, 리뷰, 작성자, 작성일), dataframe으로 저장\n",
        "# 최종본\n",
        "movie = DataFrame()\n",
        "for j in range(1,11):\n",
        "    url = 'https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=190991&target=after&page='+str(j)\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    for i in range(1,11):\n",
        "        tr_test = soup.select_one('tbody > tr:nth-of-type('+str(i)+')')\n",
        "        \n",
        "        # 평점\n",
        "        score = tr_test.select_one('em').text\n",
        "        \n",
        "        # 리뷰\n",
        "        review = tr_test.select_one('td.title')\n",
        "        review.select_one('div').extract()\n",
        "        review.select_one('a').extract()\n",
        "        review.select_one('a').extract()\n",
        "        review = review.text.strip()\n",
        "        \n",
        "        # 작성자\n",
        "        writer = tr_test.select_one('td[class=\"num\"] > a').text\n",
        "        \n",
        "        # 날짜\n",
        "        date = tr_test.select_one('td[class=\"num\"]')\n",
        "        date.select_one('a').extract()\n",
        "        date = date.text\n",
        "        \n",
        "        movie = movie.append({'score':score, 'review':review,'writer':writer,'date':date},ignore_index=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "# 강사님\n",
        "movie = DataFrame(columns=['day','id','review','point'])\n",
        "for j in range(1,11):\n",
        "        \n",
        "    url = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=190695&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='+str(j) \n",
        "    # 영화 code만 바꾸면 다른 영화들의 평도 다 가져올 수 있다\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    \n",
        "    review = []\n",
        "    for i in soup.select('div.score_reple > p'):\n",
        "        if i.select_one('span.ico_viewer') == None:\n",
        "            review.append(i.text.strip())\n",
        "        else:\n",
        "            i.select_one('span.ico_viewer').extract()\n",
        "            review.append(i.text.strip())\n",
        "    \n",
        "    point = []\n",
        "    for i in soup.select('div.star_score > em'):\n",
        "        point.append(i.text)\n",
        "    \n",
        "    id = []\n",
        "    for i in soup.select('div.score_reple > dl > dt > em > a > span'):\n",
        "        id.append(i.text)\n",
        "    \n",
        "    day = []\n",
        "    for i in soup.select('div.score_reple > dl > dt > em:nth-of-type(2)'):\n",
        "        day.append(i.text)\n",
        "    \n",
        "    \n",
        "    movie = movie.append(DataFrame({'day':day,'id':id,'review':review,'point':point}))\n",
        "    \n",
        "    time.sleep(3)"
      ]
    }
  ]
}