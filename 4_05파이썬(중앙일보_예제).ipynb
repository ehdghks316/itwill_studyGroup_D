{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.05파이썬(중앙일보 예제).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOylZXIwKnYIa/H3yCJIWdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_05%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EC%A4%91%EC%95%99%EC%9D%BC%EB%B3%B4_%EC%98%88%EC%A0%9C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkph1GtHj3Ze"
      },
      "outputs": [],
      "source": [
        "[문제]다음 강아지 이미지 다운\n",
        "#  강사님\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "url  = 'https://search.daum.net/search?w=img'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "\n",
        "#element = driver.find_element(By.CLASS_NAME,'tf_keyword')\n",
        "#element = driver.find_element(By.ID,'q')\n",
        "#element = driver.find_element(By.NAME,'q')\n",
        "#element = driver.find_element(By.XPATH,'//*[@id=\"q\"]')\n",
        "element = driver.find_element(By.CSS_SELECTOR,'div.inner_searchbar > input')\n",
        "\n",
        "element.clear()\n",
        "element.send_keys('강아지')\n",
        "time.sleep(2)\n",
        "#element.submit() # = enter\n",
        "element.send_keys(Keys.ENTER)\n",
        "\n",
        "cnt = 0\n",
        "while True:    \n",
        "    for i in range(4):\n",
        "        driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n",
        "        time.sleep(2)\n",
        "    \n",
        "    driver.find_element(By.CLASS_NAME,'open').click()\n",
        "    time.sleep(2)\n",
        "    cnt += 1\n",
        "    if cnt == 5:\n",
        "        break\n",
        "\n",
        "html = driver.page_source     \n",
        "driver.quit()\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "img_url = []\n",
        "\n",
        "for i in soup.select('img.thumb_img'):\n",
        "    img_url.append(i.attrs['src'])\n",
        "    \n",
        "len(img_url)\n",
        "img_url[0]\n",
        "\n",
        "★ pickle\n",
        "- 변수형태를 그래프로 유지해서 파일에 저장시키고 부러올 수 있는 모듈이다.\n",
        "- 바이너리 형태로 저장\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"c:/data/dog_img_url.txt\",\"wb\") as file: # 파일 쓰기\n",
        "    pickle.dump(img_url,file)\n",
        "    \n",
        "with open(\"c:/data/dog_img_url.txt\",\"rb\") as file: # 파일 읽기\n",
        "    dog_img_url = pickle.load(file)\n",
        "\n",
        "len(dog_img_url)\n",
        "\n",
        "import urllib.request as req\n",
        "from urllib.error import URLError, HTTPError\n",
        "\n",
        "httperror_url = []\n",
        "urlerror_url = []\n",
        "j = 1\n",
        "for i in dog_img_url[:10]:\n",
        "    try:\n",
        "        req.urlretrieve(i,'c:/img/python2/dog_'+str(j)+'.jpg')\n",
        "        j += 1\n",
        "        time.sleep(2)\n",
        "    except HTTPError as error:\n",
        "        print('HTTPError Code : {}'.format(eeor.code))\n",
        "        httperror_url.append((i,error.code))\n",
        "    \n",
        "    except URLError as error:\n",
        "        print('URL error reason',error)\n",
        "        urlerror_url.append(i)\n",
        "\n",
        "#############        \n",
        "        \n",
        "url  = 'https://search.daum.net/search?w=img'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "\n",
        "#element = driver.find_element(By.CLASS_NAME,'tf_keyword')\n",
        "#element = driver.find_element(By.ID,'q')\n",
        "#element = driver.find_element(By.NAME,'q')\n",
        "#element = driver.find_element(By.XPATH,'//*[@id=\"q\"]')\n",
        "element = driver.find_element(By.CSS_SELECTOR,'div.inner_searchbar > input')\n",
        "\n",
        "element.clear()\n",
        "element.send_keys('강아지')\n",
        "time.sleep(2)\n",
        "#element.submit() # = enter\n",
        "element.send_keys(Keys.ENTER)\n",
        "\n",
        "# 현재 스크롤 좌표\n",
        "driver.execute_script(\"return window.pageYOffset\") \n",
        "\n",
        "# 지정 좌표로 스크롤 이동\n",
        "driver.execute_script(\"window.scrollBy(0,3000)\")  \n",
        "\n",
        "# 현재 스크롤 좌표\n",
        "driver.execute_script(\"return window.pageYOffset\") \n",
        "\n",
        "# 현재 스크롤 전체 길이\n",
        "driver.execute_script(\"return document.body.scrollHeight\") \n",
        "\n",
        "# 지정 좌표로 스크롤 이동\n",
        "driver.execute_script(\"window.scrollBy(0,6827)\")  \n",
        "driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")  \n",
        "\n",
        "\n",
        "# 다음 강아지 스크롤 끝날때까지 내리기 \n",
        "last = driver.execute_script(\"return document.body.scrollHeight\") \n",
        "\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")  \n",
        "    time.sleep(5)\n",
        "    \n",
        "    current = driver.execute_script(\"return document.body.scrollHeight\") \n",
        "    if current == last:    \n",
        "        try:\n",
        "            driver.find_element(By.CLASS_NAME,'open').click()\n",
        "            current = driver.execute_script(\"return document.body.scrollHeight\") \n",
        "            time.sleep(2)\n",
        "        \n",
        "        except:\n",
        "            break\n",
        "        \n",
        "    last = current\n",
        "    \n",
        "\n",
        "[문제185] 중앙일보 인공지능 기사 검색을 통해 데이터 수집해 주세요.\n",
        "# 중앙일보 인공지능 검색한 사이트에서 동적(selenium)으로 기사 더보기 페이지(4번) 클릭하여 웹페이지소스 가져오기\n",
        "url = 'https://www.joongang.co.kr/search?keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "\n",
        "driver.execute_script(\"return document.body.scrollHeight\") # 스크롤 전체길이\n",
        "driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
        "\n",
        "driver.find_element(By.CLASS_NAME,'btn_full').click()\n",
        "i = 1\n",
        "while i < 4:\n",
        "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
        "    time.sleep(3)\n",
        "    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_UP)\n",
        "    time.sleep(1)\n",
        "    \n",
        "    current = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if current == last:\n",
        "        try:\n",
        "            driver.find_element(By.CLASS_NAME,'btn_outline_gray').click()\n",
        "            current = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        except:\n",
        "            break\n",
        "    last = current\n",
        "    i += 1\n",
        "    \n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# url만 추출하기\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "news_url = []\n",
        "for i in soup.select('h2.headline > a'):\n",
        "    news_url.append(i.attrs['href'])\n",
        "\n",
        "# 한개 기사 수집 (테스트)\n",
        "from urllib.request import urlopen\n",
        "\n",
        "test = urlopen(news_url[0])\n",
        "soup2 = BeautifulSoup(test,'html.parser')\n",
        "soup2.select_one('div.article_body').text\n",
        "\n",
        "# 모든 url 기사 수집\n",
        "news = []\n",
        "for i in news_url:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    news.append(soup.select_one('div.article_body').text)\n",
        "    time.sleep(2)\n",
        "\n",
        "news = ' '.join(news)\n",
        "\n",
        "# 파일로 저장\n",
        "with open('c:/data/joongang1_ai_news.txt','w',encoding='utf-8') as file:\n",
        "    file.write(news)\n",
        "\n",
        "[문제186] 중앙일보 인공지능 기사 검색을 통해 이메일 정보를 수집해 주세요.\n",
        "import re\n",
        "\n",
        "# 파일 읽어오기\n",
        "with open('c:/data/joongang1_ai_news.txt','r', encoding='utf-8') as file:\n",
        "    news = file.read()\n",
        "news\n",
        "\n",
        "# 이메일 \n",
        "email = re.findall('[A-z0-9.]+@[A-z.]+',news)\n",
        "\n",
        "[문제187] 중앙일보 인공지능 기사 검색을 통해 (내용),[내용],<내용> 정보를 수집해 주세요.\n",
        "# (내용)\n",
        "x = re.findall('\\(.+?\\)',news)\n",
        "# [내용]\n",
        "x1 = re.findall('\\[.+?\\]',news)\n",
        "# <내용>\n",
        "x2 = re.findall('\\<.+?\\>',news)\n",
        "\n",
        "[문제188] 중앙일보 인공지능 기사 정보를 전처리 하기 전과 후에 워드클라우드를 생성해주세요.\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pylab as plt\n",
        "#from matplotlib import font_manager,rc\n",
        "#font_name = font_manager.FontProperties(fname='C:/Windows/Fonts/HMKMAMI.TTF').get_name()\n",
        "#rc('font',family=font_name)\n",
        "\n",
        "#import numpy as np\n",
        "#from PIL import Image\n",
        "#bear_mask = np.array(Image.open(\"c:/data/bear.jpg\"))\n",
        "\n",
        "\n",
        "## > 전처리 전\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              colormap = 'twilight').generate(news)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "## > 전처리 후\n",
        "\n",
        "# 이메일 제거 \n",
        "for i in email:\n",
        "    news = news.replace(i,'')\n",
        "re.findall('[A-z0-9.]+@[A-z.]+',news)  #확인\n",
        "\n",
        "# [내용],(내용),<내용> 제거\n",
        "for i in x:\n",
        "    news = news.replace(i,'')\n",
        "\n",
        "for i in x1:\n",
        "    news = news.replace(i,'')\n",
        "\n",
        "for i in x2:\n",
        "    news = news.replace(i,'')\n",
        "\n",
        "news = news.replace('\\n','')    \n",
        "news = news.replace('\\xa0',' ')    \n",
        "news = news.replace('\\\\','')\n",
        "news = news.strip()\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              colormap = 'twilight').generate(news)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "w.to_file('c:/data/news_ai.jpg') #워드클라우드 이미지파일을 내 컴퓨터로 다운\n",
        "##################################################강사님\n",
        "# > 데이터 수집\n",
        "url = 'https://www.joongang.co.kr/'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "driver.find_element(By.CLASS_NAME,'btn_search').click()\n",
        "\n",
        "input= driver.find_element(By.CLASS_NAME,'form_control')\n",
        "input.clear()\n",
        "input.send_keys('인공지능')\n",
        "input.send_keys(Keys.ENTER)\n",
        "\n",
        "# 팝업창 컨트롤( 알림기능 안 뜨게 함 )\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "opt = Options()\n",
        "opt.add_experimental_option('prefs', {'profile.default_content_setting_values.notifications':1}) \n",
        "\n",
        "url = 'https://www.joongang.co.kr/'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe',options=opt)\n",
        "driver.get(url)\n",
        "driver.find_element(By.CLASS_NAME,'btn_search').click()\n",
        "\n",
        "input= driver.find_element(By.CLASS_NAME,'form_control')\n",
        "input.clear()\n",
        "input.send_keys('인공지능')\n",
        "input.send_keys(Keys.ENTER)\n",
        "\n",
        "# 창크기\n",
        "previous = driver.get_window_size()\n",
        "previous['width']\n",
        "previous['height']\n",
        "driver.maximize_window() # 최대창\n",
        "driver.minimize_window() # 최소화\n",
        "driver.set_window_size(1500,1000) # 원하는 창 크기로 변경\n",
        "driver.set_window_size(previous['width'],previous['height']) # 원래창으로\n",
        "\n",
        " # btn이 있는 위치까지 스크롤을 내리는 작업( 창 크기가 최대화 되어 있어야 수행 가능함)\n",
        "from selenium.webdriver import ActionChains\n",
        "\n",
        "btn = driver.find_element(By.CLASS_NAME,'btn.btn_full')\n",
        "action = ActionChains(driver)\n",
        "action.move_to_element(btn).perform() \n",
        "driver.find_element(By.CLASS_NAME,'btn.btn_full').click() # 버튼 클릭\n",
        "\n",
        "for i in range(5):    \n",
        "    btn = driver.find_element(By.CLASS_NAME,'btn.btn_outline_gray')\n",
        "    action = ActionChains(driver)\n",
        "    action.move_to_element(btn).perform() \n",
        "    driver.find_element(By.CLASS_NAME,'btn.btn_outline_gray').click() # 버튼 클릭\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source # 페이지 html source 추출\n",
        "driver.close()\n",
        "\n",
        "ai_url = []\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "for i in soup.select('ul.story_list > li.card > div.card_body > h2.headline'):\n",
        "    ai_url.append(i.select_one('a')['href'])\n",
        "ai_url    \n",
        "\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "\n",
        "contents = DataFrame(columns=['day','title','news','reporter'])\n",
        "ai_url[0]\n",
        "html = req.urlopen(ai_url[0])\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "day = soup.select_one('div.time_bx > p.date').text.replace('입력','').strip()\n",
        "title =  soup.select_one('h1.headline').text.strip()\n",
        "reporter = soup.select_one('div.ab_byline').text.strip()\n",
        "\n",
        "soup.select_one('div.ab_byline').extract()\n",
        "for i in soup.select('div.ab_photo.photo_center'): # 리스트, 여러개를 한번에 extract할 수는 없기에 for문을 사용\n",
        "    i.extract()\n",
        "\n",
        "news = soup.select_one('div#article_body').text.strip()\n",
        "contents = contents.append({'day':day,'title':title,'news':news,'reporter':reporter},ignore_index=True)\n",
        "contents\n",
        "\n",
        "# 데이터수집 최종\n",
        "contents = DataFrame(columns=['day','title','news','reporter'])\n",
        "for i in ai_url:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    day = soup.select_one('div.time_bx > p.date').text.replace('입력','').strip()\n",
        "    title = soup.select_one('h1.headline').text.strip()\n",
        "    \n",
        "    try: # 기자가 없는 기사도 존재하기에 기사가 없는 reporter값은 '' \n",
        "        reporter = soup.select_one('div.ab_byline').text.strip()\n",
        "        soup.select_one('div.ab_byline').extract() # news 변수에 들어갈 내용중 필요없는 내용 제거\n",
        "    except:\n",
        "        reporter = ''\n",
        "    \n",
        "    for j in soup.select('div.ab_photo.photo_center'): # news변수에 들어갈 내용 중 필요없는 사진에 있는 text 제거\n",
        "        j.extract()\n",
        "        \n",
        "    for j in soup.select('div.ad_wrap'):  # news변수에 들어갈 내용 중 필요없는 광고글 제거\n",
        "        j.extract()\n",
        "    \n",
        "    news = soup.select_one('div.article_body').text.strip()\n",
        "    contents = contents.append({'day':day,'title':title,'news':news,'reporter':reporter},ignore_index=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "contents.to_csv('c:/data/joongang_ai_news_df.csv')\n",
        "\n",
        "## > 전처리 전\n",
        "text = ' '.join(contents['news'])\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              colormap = 'twilight').generate(text)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "## > 전처리 후\n",
        "\n",
        "# 이메일 제거 (1개있음)\n",
        "\n",
        "text = text.replace(re.search('[A-z0-9.]+@[A-z.]+',text).group(),'')\n",
        "re.search('[A-z0-9.]+@[A-z.]+',text)  #확인\n",
        "\n",
        "# [내용],(내용),<내용> 제거\n",
        "\n",
        "for i in re.findall('\\(.+?\\)',text):\n",
        "    text = text.replace(i,'')\n",
        "\n",
        "for i in re.findall('\\[.+?\\]',text):\n",
        "    text = text.replace(i,'')\n",
        "\n",
        "for i in re.findall('\\<.+?\\>',text):\n",
        "    text = text.replace(i,'')\n",
        "\n",
        "re.findall('이다|등|를|을|있다|있는|등을|하는|말했다|및|같은',text)\n",
        "re.findall('\\xa0',text)\n",
        "text = text.replace('\\n','')    \n",
        "text = text.replace('\\xa0',' ')    \n",
        "text = text.replace('\\\\','')\n",
        "text = text.replace('\\W','')\n",
        "text = text.replace('\\u3000','')\n",
        "text = text.replace('\\'','')\n",
        "text = text.replace('●','')\n",
        "text = text.replace('‘|’|·|:|\\?','')\n",
        "text = text.replace('이다|등|를|을|있다|있는|등을|하는|말했다|및|같은','')\n",
        "text = text.strip()\n",
        "text\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              colormap = 'twilight').generate(text)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "''' pass\n",
        "[186] \n",
        "mailcompile = re.compile('[A-z0-9._]+@[A-z.]+')\n",
        "contents['news'].apply(lambda x : mailcompile.findall(x))\n",
        "contents['reporter'].apply(lambda x : ''.join(mailcompile.findall(x)))\n",
        "\n",
        "[187]\n",
        "p = re.compile('[\\<\\[\\(][\\w\\s=.]+[\\)\\]\\>]')\n",
        "contents['news'].apply(lambda x : p.findall(x))\n",
        "\n",
        "[188]\n",
        "p = re.compile('[\\[\\(][\\w\\s=.]+[\\)\\]]')\n",
        "contents['news'].apply(lambda x : p.findall(x))\n",
        "contents['clean_news'] = contents['news'].apply(lambda x : p.sub(' ',x))\n",
        "contents['clean_news'].apply(lambda x : p.findall(x))                                                \n",
        "'''\n",
        "\n",
        "\n",
        "v"
      ]
    }
  ]
}