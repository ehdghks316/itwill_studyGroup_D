{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_20파이썬(연관성분석/apriori algorithm, 영어품사태깅/nltk).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP53Z/kYWTR3lkgEuRV98sq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_20%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EC%97%B0%EA%B4%80%EC%84%B1%EB%B6%84%EC%84%9D_apriori_algorithm%2C_%EC%98%81%EC%96%B4%ED%92%88%EC%82%AC%ED%83%9C%EA%B9%85_nltk).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylZdUbpfeOxn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "teens = pd.read_csv('c:/data/snsdata.csv')\n",
        "teens.info()\n",
        "\n",
        "# 결측값\n",
        "teens.isnull().sum()\n",
        "\n",
        "teens['gender'].value_counts()\n",
        "teens['gender'].value_counts(dropna=True)\n",
        "teens['gender'].value_counts(dropna=False) # 결측값까지 춫력\n",
        "teens['gender'].isnull().sum()\n",
        "teens['gender'].describe()\n",
        "teens['gender'].value_counts(dropna=False).plot(kind='bar') # 결측값까지 춫력\n",
        "\n",
        "teens_df = pd.DataFrame(teens['gender'].value_counts(dropna=False)).reset_index()\n",
        "teens_df.columns = ['gender','freq']\n",
        "teens_df\n",
        "teens_df.loc[teens_df.gender.isnull(),'gender'] = 'NA'\n",
        "\n",
        "import seaborn as sns\n",
        "sns.barplot(x='gender',y='freq', data=teens_df)\n",
        "\n",
        "teens['age'].isnull().sum()\n",
        "teens['age'].describe()  # 숫자데이터들을 활용할 때 describe함수로 확인해보기\n",
        "teens.describe()\n",
        "\n",
        "teens.boxplot(column='age')\n",
        "sns.boxplot(x=teens['age'])\n",
        "sns.boxplot(y=teens['age'])\n",
        "\n",
        "\n",
        "len([i for i in teens['age'] if (i < 13) | (i >= 20)])\n",
        "teens['age']=[np.nan if (i < 13) | (i >= 20) else i for i in teens['age']]\n",
        "teens['age'].describe()\n",
        "teens['age'].isnull().sum()\n",
        "\n",
        "# 졸업년도를 기준으로 평균나이\n",
        "teens[['gradyear','age']]\n",
        "gradage = teens['age'].groupby(teens['gradyear']).mean()\n",
        "gradage.index\n",
        "\n",
        "teens['agemean'] = [gradage.loc[i] for i in teens['gradyear']]\n",
        "teens\n",
        "\n",
        "teens['age'] = np.where(teens['age'].isnull(),teens['agemean'],teens['age']) # np.where(조건,조건이true일때 수행, false일때 수행)\n",
        "teens['age'].isnull().sum()\n",
        "teens['age'].describe()\n",
        "\n",
        "teens['female'] = [1 if i == 'F' else 0 for i in teens['gender']] # 수치화\n",
        "teens['female'].value_counts()\n",
        "\n",
        "teens['no_gender'] = [1 if pd.isnull(i) else 0 for i in teens['gender']]\n",
        "teens['no_gender'].value_counts()\n",
        "\n",
        "teens.info()\n",
        "data = teens.iloc[:,4:40]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=5)\n",
        "model.fit(data)\n",
        "model.labels_\n",
        "model.cluster_centers_\n",
        "model.cluster_centers_.shape\n",
        "teens['cluster'] = model.labels_\n",
        "teens.loc[1:5,['cluster','gender','age','friends']]\n",
        "\n",
        "teens['age'].groupby(teens['cluster']).mean()\n",
        "teens['female'].groupby(teens['cluster']).mean()\n",
        "teens['friends'].groupby(teens['cluster']).mean()\n",
        "\n",
        "for name,group in teens.groupby('cluster'):\n",
        "    print(name)\n",
        "    print(group)\n",
        "    \n",
        "teens[teens['cluster'] == 0][teens.columns]\n",
        "col = teens.columns.difference(['gradyear','gender','age','agemean','female', 'no_gender', 'cluster','friends'])\n",
        "teens_0 = teens[teens['cluster'] == 0][col] # columns.difference([제외할 컬럼]) : 제외할 컬럼 빼고 추출\n",
        "teens_0.sum().sort_values(ascending=False)[:10]\n",
        "\n",
        "teens_0 = teens[teens['cluster'] == 1][col]\n",
        "teens_0.sum().sort_values(ascending=False)[:10]\n",
        "\n",
        "teens_0 = teens[teens['cluster'] == 2][col]\n",
        "teens_0.sum().sort_values(ascending=False)[:10]\n",
        "\n",
        "teens_0 = teens[teens['cluster'] == 3][col]\n",
        "teens_0.sum().sort_values(ascending=False)[:10]\n",
        "\n",
        "teens_0 = teens[teens['cluster'] == 4][col]\n",
        "teens_0.sum().sort_values(ascending=False)[:10]\n",
        "\n",
        "data['cluster'] = model.labels_\n",
        "data.columns[:-1] # 마지막 열은 제외\n",
        "\n",
        "data.columns[:-1][model.cluster_centers_.argsort()[:,::-1][0][:10]] # argsort() 인덱스 값을 추출\n",
        "data.columns[:-1][model.cluster_centers_.argsort()[:,::-1][1][:10]]\n",
        "data.columns[:-1][model.cluster_centers_.argsort()[:,::-1][2][:10]]\n",
        "data.columns[:-1][model.cluster_centers_.argsort()[:,::-1][3][:10]]\n",
        "data.columns[:-1][model.cluster_centers_.argsort()[:,::-1][4][:10]]\n",
        "\n",
        "ks = range(1,10)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    model = KMeans(n_clusters=k)\n",
        "    model.fit(data)\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias,'-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.ticklabel_format(style='plain')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "★ 연관성분석(association analysis)\n",
        "- 대량의 데이터에 숨겨진 항목간의 연관규칙을 찾아내는 기법으로서 장바구니분석(market basket analysis)이라고도 함\n",
        "- 실제 연관성분석은 모든 기업에서 다양한 마케팅활동에 활용하고 있으며 더 나아가 사회 네트워크 분석에도 활용할 수 있다.\n",
        "\n",
        "장점\n",
        "- 대규모 거래 데이터에 대해 작업할 수 있다.\n",
        "- 이해하기 쉬운 규칙을 생성한다\n",
        "- 데이터마이닝과 데이터베이스에서 예상치 못한 지식을 발굴하는데 유용\n",
        "\n",
        "단점\n",
        "- 작은데이터셋에는 그다지 유용하지 않다.\n",
        "- 진정한 통찰력과 상식을 분리하기 위한 노력이 필요하다.\n",
        "\n",
        "지지도(support)\n",
        "- 전체 거래중 연관성 규칙을 구성하는 항목들이 포함된 거래의 비율\n",
        "- 전체 거래중 연관성 규칙이 얼마나 빈번히 발생하는지 확인하는 지표\n",
        "\n",
        "            항목에 대한 거래 갯수\n",
        "support = ----------------------\n",
        "            전체 거래수\n",
        "            \n",
        "신뢰도(confidence)\n",
        "- 조건이 발생했을 때 동시에 일어날 확률을 의미하며 신뢰도가 1에 가까울수록 의미있는 연관성을 가지고 있다.\n",
        "- 조건을 포함하는 거래중 연관성 규칙이 얼마나 빈번히 발생하는지 확인하는 지표\n",
        "\n",
        "{조건} -> {결과}\n",
        "\n",
        "                조건과 결과 항목을 포함하는 거래수\n",
        "confidence = --------------------------\n",
        "                조건항목을 포함한 거래수\n",
        "\n",
        "                            support(x,y)\n",
        "confidence(x -> y) = -------------------------\n",
        "                            support(x)\n",
        "\n",
        "\n",
        "거래번호    구매물품\n",
        "--------    --------------------------\n",
        "1           - 우유, 버터, 시리얼\n",
        "2           - 우유, 시리얼\n",
        "3           - 우유, 빵\n",
        "4           - 버터, 맥주, 오징어\n",
        "\n",
        "support(우유 -> 시리얼) : 우유와 시리얼을 동시에 구매할 확률, {우유,시리얼}포함한 거래수 / 전체거래수 ,2/4\n",
        "confidence(우유 -> 시리얼) : 우유를 구매할 때 시리얼 같이 구매할 조건부확률, {우유,시리얼} / {우유}\n",
        "\n",
        "support(우유 -> 시리얼)  = 50%\n",
        "confidence(우유 -> 시리얼) = 66.7%\n",
        "\n",
        "support(시리얼 -> 우유) = 50%\n",
        "confidence(시리얼 -> 우유) = 100%\n",
        "\n",
        "향상도(lift)\n",
        "- 지지도와 신뢰도를 동시에 고려하는 지표\n",
        "\n",
        "                    confidence(시리얼 -> 우유)\n",
        "lift(시리얼,우유) = -------------------------------\n",
        "                            support(우유)\n",
        "                            \n",
        "또는\n",
        "                    support(시리얼 -> 우유)         \n",
        "lift(시리얼,우유) = ------------------------------- \n",
        "                    support(시리얼) * support(우유)\n",
        "                            \n",
        "                    confidence(시리얼 -> 우유)              1\n",
        "lift(시리얼,우유) = ------------------------------- =  ---------- = 1/0.75 = 1.33\n",
        "                    support(우유)                          3/4\n",
        "\n",
        "향상도값이 1인 경우 조건과 결과는 우연에 의한 관계라고 보여,1보다 클수록 우연이 아닌 의미있는 연관성을 가진 규칙으로 해석하면 된다.\n",
        "\n",
        "거래번호    구매물품\n",
        "--------    --------------------------\n",
        "1           A, C, D\n",
        "2           B, C, E\n",
        "3           A, B, C, E\n",
        "4           B, E\n",
        "\n",
        "항목      지지도(항목구매건수/전체구매건수)\n",
        "------- -------------\n",
        "A       2\n",
        "B       3\n",
        "C       3\n",
        "D       1\n",
        "E       3\n",
        "\n",
        "지지도 2이상인 항목만 추출\n",
        "\n",
        "항목      지지도(항목구매건수/전체구매건수)\n",
        "------- -------------\n",
        "A       2\n",
        "B       3\n",
        "C       3\n",
        "E       3\n",
        "\n",
        "항목      지지도\n",
        "------- --------------\n",
        "A B     1    \n",
        "A C     2\n",
        "A E     1\n",
        "B C     2\n",
        "B E     3\n",
        "C E     2\n",
        "\n",
        "지지도 2 이상인 항목 추출\n",
        "\n",
        "항목      지지도\n",
        "------- --------------\n",
        "A C     2\n",
        "B C     2\n",
        "B E     3\n",
        "C E     2\n",
        "\n",
        "각각 항목에서 첫번째 항목을 기준으로 동일한 것을 찾아보세요.\n",
        "B C E\n",
        "\n",
        "거래번호    구매물품\n",
        "--------    -------------------------- \n",
        "1           A, C, D\n",
        "2           B, C, E\n",
        "3           A, B, C, E\n",
        "4           B, E\n",
        "\n",
        "발견된 규칙항목        지지도\n",
        "----------------    ---------\n",
        "B, C, E                 2\n",
        "\n",
        "★ apriori algorithm\n",
        "- 집합의 크기가 1인경우부터 차례로 늘려가면서 처리하는 알고리즘\n",
        "- k 개인 빈도가 높은 항목을 구했다면 그 다음에는 k+1인 항목의 집합을 계산한다.\n",
        "\n",
        "\n",
        "dataset = [['우유','버터','시리얼'],\n",
        "           ['우유','시리얼'],\n",
        "           ['우유','빵'],\n",
        "           ['버터','맥주','오징어']]\n",
        "\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "import pandas as pd\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_data = te.fit_transform(dataset)\n",
        "df = pd.DataFrame(te_data,columns = te.columns_)\n",
        "f = apriori(df,min_support = 0.1, use_colnames=True)\n",
        "association_rules(f)\n",
        "association_rules(f,metric='confidence')\n",
        "\n",
        "import csv\n",
        "groceries = []\n",
        "with open('c:/data/groceries.csv','r') as file:\n",
        "    csv_data = csv.reader(file)\n",
        "    for row in csv_data:\n",
        "        groceries.append(row)\n",
        "        \n",
        "te = TransactionEncoder()\n",
        "te_data = te.fit_transform(groceries)\n",
        "df = pd.DataFrame(te_data,columns = te.columns_)\n",
        "f = apriori(df,min_support = 0.005, use_colnames=True)\n",
        "#association_rules(f)  - min_threshold값을 조정해야 값이 나옴( min_threshold값이 높으면 값들이 나올 수 없기 때문)\n",
        "rules = association_rules(f,metric='confidence', min_threshold=0.01)\n",
        "rules[rules['antecedents'] == {'whole milk','butter'}]\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "mom = pd.read_csv('c:/data/mom.txt',header=None,names=['sentens'])\n",
        "mom\n",
        "\n",
        "txt = ' '.join(mom['sentens'])\n",
        "txt\n",
        "word_tokenize(txt)\n",
        "words = word_tokenize(txt)\n",
        "pos_tag(txt) # 문장은 품사 태깅할 때 오류!\n",
        "pos_tag(words) # 단어 태깅 가능!( word_tokenize로 단어로 바꿔주고 품사 태깅하기 )\n",
        "# nltk에서 품사태깅하려면 단어로 분리한 후 수행해야 한다.\n",
        "pos_tag(word_tokenize(txt))\n",
        "\n",
        "# 품사정보를 확인\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('NNS')\n",
        "\n",
        "tagg_list = pos_tag(word_tokenize(txt))\n",
        "tagg_list\n",
        "\n",
        "nouns_list = [i[0] for i in tagg_list if i[1] == 'NN' ]\n",
        "nouns_list\n",
        "\n",
        "nouns_list = [i[0] for i in tagg_list if i[1] in ['NN','NNP'] ]\n",
        "nouns_list\n",
        "\n",
        "from nltk.tag import untag\n",
        "untag(tagg_list) # 태그 정보 빼고 출력\n",
        "\n"
      ]
    }
  ]
}
