{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_27파이썬(모형의 선형성, 잔차의 정규성, 잔차의 등분산성, 잔차의 독립성, 극단값, 다중공선성, VIF, LogisticRegression, 시그모이드).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlLmqU8Nun6WB2vgsDKnpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_27%ED%8C%8C%EC%9D%B4%EC%8D%AC(%EB%AA%A8%ED%98%95%EC%9D%98_%EC%84%A0%ED%98%95%EC%84%B1%2C_%EC%9E%94%EC%B0%A8%EC%9D%98_%EC%A0%95%EA%B7%9C%EC%84%B1%2C_%EC%9E%94%EC%B0%A8%EC%9D%98_%EB%93%B1%EB%B6%84%EC%82%B0%EC%84%B1%2C_%EC%9E%94%EC%B0%A8%EC%9D%98_%EB%8F%85%EB%A6%BD%EC%84%B1%2C_%EA%B7%B9%EB%8B%A8%EA%B0%92%2C_%EB%8B%A4%EC%A4%91%EA%B3%B5%EC%84%A0%EC%84%B1%2C_VIF%2C_LogisticRegression%2C_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icL9rnAGpreE"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "\n",
        "boston = pd.read_csv('c:/data/boston.csv')\n",
        "boston.head()\n",
        "boston.info()\n",
        "\n",
        "col = boston[boston.columns.difference(['MEDV'])].columns\n",
        "ols_lr = ols('MEDV ~ '+' + '.join(col),data=boston).fit()\n",
        "ols_lr.summary()\n",
        "\n",
        "\n",
        "★ 모형의 선형성\n",
        "- 예측값(predicted)과 잔차(residual) 비교  # 잔차 : 예측값과 실제값의 차이\n",
        "- 모든 예측값에서 잔차가 비슷하게 있어야한다. \n",
        "- 빨간 실선은 잔차의 추세를 나타내는 선\n",
        "- 빨간 실선이 점선에서 크게 벗어나면 예측값에 따라 잔차가 크게 달라진다는 것을 의미\n",
        "\n",
        "x = boston[boston.columns.difference(['MEDV'])]\n",
        "y = boston['MEDV']\n",
        "\n",
        "predicted = ols_lr.predict(x) # 예측값\n",
        "residual = y - predicted # 잔차\n",
        "sns.regplot(predicted,residual, line_kws={'color':'red'})\n",
        "plt.plot([predicted.min(),predicted.max()],[0,0],'--',color='grey')\n",
        "\n",
        "★ 잔차의 정규성\n",
        "- 잔차가 정규분포를 따른다는 가정\n",
        "- Q-Q plot으로 확인할 수 있다. (Quantile-Quantile Plot)\n",
        "- 잔차가 정규분포를 띄면 Q-Q plot에서 점들이 점선을 따라 배치되어 있어야한다.\n",
        "\n",
        "import scipy.stats as ss\n",
        "z = ss.zscore(residual)\n",
        "ss.probplot(z,plot=plt) # Q-Q plot 빨간선에 가까울수록 정규성을 따른다.\n",
        "\n",
        "sns.distplot(residual)\n",
        "sns.histplot(residual)\n",
        "\n",
        "★ 잔차의 등분산성\n",
        "- 회귀모형을 통해 예측된 값이 크던, 작던 모든 값들에 대하여 잔차의 분산이 동일하는 것으로 가정\n",
        "- 빨간색 실선이 수평선을 그리는 것이 가장 이상적이다.\n",
        "\n",
        "sns.regplot(predicted,np.sqrt(np.abs(z)), line_kws={'color':'red'})\n",
        "\n",
        "★ 잔차의 독립성\n",
        "- 자료 수집 과정에서 무작위 표본을 이용해서 모델을 학습했다면 잔차의 독립성은 만족하는 것으로 본다.\n",
        "- Durbin-Watson 검정을 이용해서 확인\n",
        "- 보통 1.5 ~ 2.5 사이이면 독립으로 판단하고 회귀모형이 적합하다고 판단하면 된다.\n",
        "- 0(양의자기상관) 또는 4(음의자기상관)에 가깝다는 것은 잔차들이 자기 상관을 가지고 있다는 의미이고 \n",
        "  이는 t값,F값, R²값이 실제보다 증가시켜 유의미하지 않는 결과를 유의미한 결과로 왜곡할 수 있다.\n",
        "\n",
        "★ 극단값\n",
        "- cook`s distance는 극단값을 나타내는 지표\n",
        "- 데이터가 특히 예측에서 많이 벗어남을 알 수 있다.\n",
        "\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "cd, _ = OLSInfluence(ols_lr).cooks_distance # 여러개 값중 하나만 받겠다 : 변수명, _\n",
        "cd.sort_values(ascending=False).head()\n",
        "\n",
        "ols_lr.predict(boston.iloc[368,])\n",
        "# 예측값368    23.800729\n",
        "# MEDV      50.000000\n",
        "# 50.00000 - 23.800729 = 잔차\n",
        "\n",
        "ols_lr.predict(boston.iloc[200,])\n",
        "# 예측200    30.643939\n",
        "# 실제값MEDV   32.90000\n",
        "\n",
        "R²(R-squared)\n",
        "-종속변수의 분산을 독립변수의 분산으로 설명하는 지표\n",
        "R² = 0 : 모델의 설명력이 0\n",
        "R² = 1 : 모델의 설명령이 100%\n",
        "\n",
        "R² (R-squared)\n",
        "- R² 분산기반으로 예측성능을 평가한다. 1에 가까울수록 예측정확도가 높다\n",
        "r2_score(y_test,y_pred)\n",
        "1- np.sum(np.power(y_test - y_pred,2)) / np.sum(np.power(y_test - np.mean(y_test),2))\n",
        "\n",
        "\n",
        "            SSR\n",
        "R² = 1 - --------\n",
        "            SST\n",
        "\n",
        "★ 다중공선성(Multicollinearity)\n",
        "- 독립변수들 끼리 상관관계를 가지고 있다.\n",
        "\n",
        "예)\n",
        "혈압(종속변수), 키, 몸무게, BMI,...\n",
        "...\n",
        "...\n",
        "\n",
        "★ VIF를 통한 다중공선성 확인\n",
        "- VIF는 Variance Inflation Factor 지표를 이용\n",
        "- 분산팽창요인\n",
        "- VIF가 10보다 크면 다중공선성이 있다고 판단한다.\n",
        "- ONE HOT ENCODING의 변수는 VIF가 3 이상이면 다중공선성이 있다고 판단 # 체크해봐야하는 기준이되는 것이고 무조건 버리는 것은 아니다.\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "vif = pd.DataFrame()\n",
        "vif['VIF Factor'] = [variance_inflation_factor(boston.values,i) for i in range(boston.shape[1])]\n",
        "vif['features'] = boston.columns\n",
        "vif\n",
        "\n",
        "data = boston[boston.columns.difference(['NOX','PTRATIO','TAX'])]\n",
        "data_vif = pd.DataFrame()\n",
        "data_vif['VIF Factor'] = [variance_inflation_factor(data.values,i) for i in range(data.shape[1])]\n",
        "data_vif['features'] = data.columns\n",
        "data_vif\n",
        "\n",
        "# 프로젝트할 때 막연하게 그냥 하는게 아니라 도메인 지식에 대해서 공부가 필요\n",
        "    \n",
        "# 상관계수로 확인(변수간의 상관계수 확인하고 불필요한 부분은 제외하기)\n",
        "corr_matrix = boston.corr().round(2)\n",
        "sns.heatmap(data=corr_matrix,annot=True)\n",
        "# 잔차, 다중공선성, P값, R² 를 체크해서 회귀분석 프로젝트 발표할 때 정확히 이야기할 수 있어야함\n",
        "# 모든피쳐를 다 넣는다고 좋은 것이 아니고 P값이 0.05보다 작은 값이 기울기가 의미가 있는 값\n",
        "\n",
        "★ Logistic Regression(로지스틱회귀분석)\n",
        "- 선형회귀방식을 분류에 적용한 알고리즘\n",
        "- 분류를 하는데 있어서 가장 흔한 경우는 이분법을 기준으로 분류하는 경우, 이진분류(0,1)\n",
        "- 기업부도예측, 주가, 환율, 금리등의 up/down 예측\n",
        "\n",
        "오즈비(odds ratio)\n",
        "- 오즈비는 확률과 관련된 의미로 P가 주어졌을 때 사건이 발생할 확률(성공)이 발생하지 않을 확률(실패)에 비해\n",
        "  몇배 더 높은 가의 의미\n",
        "- 예)\n",
        "    종속변수의 범주가 1(성공), 0(실패)인 이분형을 가정할 때 P(확률)가 0.8이라면\n",
        "    오즈비는 (0.8/(1-0.8)) = 4가 되고 이값은 성공이 될 확률이 실패가 될 확률보다 4배 높다는 의미\n",
        "              P      성공\n",
        "    odds = ------ = ------\n",
        "             1-P     실패\n",
        "\n",
        "    logit = log(odds)\n",
        "\n",
        "# log를 사용해서 값이 이분이 안되고 무한대로 가니까 최종 분류에서는 0과 1로 분류해야하기에 시그모이드 함수를 사용한다\n",
        "\n",
        "★ sigmoid(시그모이드)\n",
        "- 0 ~ 1 사이의 실수값으로 리턴하는 함수\n",
        "- e 자연상수 : 2.7182\n",
        "\n",
        "np.exp(-10)\n",
        "np.exp(10)\n",
        "                   1\n",
        "sigmoid(x) = --------------\n",
        "             1 + np.exp(-1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "sigmoid(1000)\n",
        "sigmoid(-1000)\n",
        "sigmoid(100)\n",
        "sigmoid(12)\n",
        "\n",
        "x  = np.arange(-5,5,0.1) #-5 ~ 5사이 0.1씩 단위 값\n",
        "y = sigmoid(x) \n",
        "plt.plot(x,y)\n",
        "\n",
        "sigmoid(y(예측)) = ax + b\n",
        "0 ~ 1 값만 출력할 수 있도록 시그모이드 함수를 이용한다.\n",
        "\n",
        "sigmoid(y(예측)) >= 0.5 : 1\n",
        "sigmoid(y(예측)) <  0.5 : 0\n",
        "# 신경망에서는 기본값을 지정할 수 있지만 여기 sklearn에서 제공하는 함수에서는 0.5가 기본값이 됨\n",
        "\n",
        "# 독립변수, 종속변수 둘 다 수치형으로 되야하는데 문자형도 자동으로 변환해주는 함수가 있다\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "iris = pd.read_csv('c:/data/iris.csv')\n",
        "x = iris.iloc[:,:-1]\n",
        "y = iris.Name\n",
        "\n",
        "log_lr = LogisticRegression()\n",
        "log_lr.fit(x,y)\n",
        "\n",
        "log_lr.classes_  #분류되는 종속변수값\n",
        "\n",
        "log_lr.predict([[5.1,3.5,1.4,0.2]])[0] # 2차원으로 넣어야함\n",
        "log_lr.predict([[6.9,3.2,5.4,2.2]])[0]\n",
        "\n",
        "import seaborn as sns\n",
        "sns.stripplot(x='PetalLength',y='Name',data=iris)\n",
        "sns.stripplot(x='PetalWidth',y='Name',data=iris)\n",
        "sns.stripplot(x='SepalLength',y='Name',data=iris)\n",
        "sns.stripplot(x='SepalWidth',y='Name',data=iris)\n",
        "sns.stripplot(y='PetalLength',x='Name',data=iris) # x, y 거꾸로\n",
        "\n",
        "sns.swarmplot(x='PetalLength',y='Name',data=iris) # 점들이 중복(겹처서)되서 나오지 않음\n",
        "\n",
        "# 프로젝트에서 나이브베이즈! 로지스틱회귀! 둘다 해보고 정확도를 비교해보자\n",
        "\n",
        "\n",
        "# naivebayes프로젝트----------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import pandas as pd\n",
        "from pandas import DataFrame,Series\n",
        "from collections import Counter\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "# 데이터 읽어오기\n",
        "book = pd.read_csv('c:/data/book_field1.csv',encoding='utf-8')\n",
        "book\n",
        "\n",
        "# 학습, 테스트 데이터, 학습라벨, 테스트라벨 만들기\n",
        "x_train,x_test,y_train,y_test = train_test_split(book['text'],book['field'],test_size=0.2) # test_size=0.2는 전체의 20%를 test셋으로 만들겠다는 의미, shuffle : default=True split이전에 섞을건지, stratify : target으로 지정하면 각각의 class비율을 train /validation에 유지(한쪽에 쏠려서 분배되는 것을 방지함(default=None ))\n",
        "Counter(y_train)\n",
        "Counter(y_test)\n",
        "\n",
        "# 전처리, 형태소 분석 함수\n",
        "def okt_pos(arg):\n",
        "    token = []\n",
        "    for j in okt.pos(arg):\n",
        "        if j[1] in ['Noun','Adjective','Alpha']:  # 명사, 형용사, 영어\n",
        "            token.append(j[0]) \n",
        "    token = [i for i in token if len(i)>=2]\n",
        "    return token\n",
        "\n",
        "# 불용어\n",
        "stopwords = ['지난','이후','독자','작가','있다','같은','이로','인해','있게','누구','있으며','있는','통해','있도록','저자']\n",
        "\n",
        "# 학습데이터 \n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords)\n",
        "x_train = cv.fit_transform(x_train)\n",
        "cv.get_feature_names()\n",
        "x_train.toarray()\n",
        "\n",
        "# 테스트데이터\n",
        "x_test = cv.transform(x_test)\n",
        "x_test.toarray()\n",
        "\n",
        "# 분류기 모델\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train,y_train)\n",
        "\n",
        "# 테스트 예측\n",
        "y_predict = nb.predict(x_test)\n",
        "sum(y_predict == y_test)/360 # 정답률\n",
        "accuracy_score(y_test,y_predict)\n",
        "\n",
        "# 혼동행렬\n",
        "pd.crosstab(y_test,y_predict)\n",
        "confusion_matrix(y_test,y_predict)\n",
        "#pd.set_option('display.max_columns',None)\n",
        "# 정확도, 정밀도, 재현율, f1-score\n",
        "print(classification_report(y_test,y_predict))\n",
        "\n",
        "# 분류 클래스\n",
        "nb.classes_ # 'art_popularculture', 'computer_it', 'economy', 'history_culture','humanities', 'novel', 'poem_essay', 'politics_society', 'religion', 'selfdevelope', 'sience', 'technology_engineering'\n",
        "\n",
        "# 분류 클래스 갯수\n",
        "nb.class_count_ \n",
        "\n",
        "\n",
        "# 사전확률\n",
        "import numpy as np\n",
        "np.exp(nb.class_log_prior_) \n",
        "\n",
        "# 분류 클래스별 컬럼의 값의 빈도수\n",
        "nb.feature_count_\n",
        "\n",
        "# logistic regression-----------------------------------------------------------------------------------------\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(book['text'],book['field'],test_size=0.2)\n",
        "\n",
        "# 학습데이터 \n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords)\n",
        "x_train = cv.fit_transform(x_train)\n",
        "x_test = cv.transform(x_test)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train,y_train)\n",
        "lr.classes_\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "\n",
        "정확도 - 나이브베이즈:0.76, 로지스틱회귀:0.74\n"
      ]
    }
  ]
}
