{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_11파이썬(CountVectorizer, TF-IDF).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMH65mcMHF1Y2itKcrpwOmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_11%ED%8C%8C%EC%9D%B4%EC%8D%AC(CountVectorizer%2C_TF_IDF).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi9vg1N05ZiA"
      },
      "outputs": [],
      "source": [
        "★ word embedding\n",
        "- 일상의 언어를 컴퓨터가 이해할 수 있는 벡터(숫자)로 표현하는 방법\n",
        "\n",
        "★ BOW\n",
        "- 문서를 숫자 벡터로 변환하는 가장 기본적인 방법은 BOW(Bag Of Words) 인코딩 방법\n",
        "\n",
        "★ 문서 단어 행렬(Document Term Matrix)\n",
        "- 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현\n",
        "(행이 문서들 열이 단어들)\n",
        "\n",
        "with open('c:/data/mom.txt','r') as file:\n",
        "    mom = file.read()\n",
        "\n",
        "\n",
        "[문제193] mom.txt 데이터를 가지고 화면과 같은 문서 단어행렬을 수동으로 생성해주세요.\n",
        "    am and be .... take to ...\n",
        " 0   0   1   0 ...  0    1 ...\n",
        " 1   ...\n",
        " 2   ...\n",
        "\n",
        "# \\n 단어 공백으로 변경 \n",
        "mom = mom.replace('\\n',' ')\n",
        "# ' '으로 단어 나누기\n",
        "mom_split = mom.split(' ')\n",
        "# dictionary 형태로 중복없이 추출\n",
        "mom_dict = {}\n",
        "\n",
        "for i in mom_split:\n",
        "    if i == '':\n",
        "        continue\n",
        "    elif i not in mom_dict.keys():\n",
        "        mom_dict[i] = 1\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "mom_dict\n",
        "\n",
        "# dictionary 형태로 중복없이 추출2\n",
        "mom_split.remove('') # 먼저 공백만 있는 리스트를 추출\n",
        "for i in mom_split:\n",
        "    if i not in mom_dict.keys():\n",
        "        mom_dict[i] = 1\n",
        "    else:\n",
        "        continue\n",
        "    \n",
        "'''\n",
        "# 단어들을 컬럼으로 2\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "df = DataFrame(mom_dict.keys()).T\n",
        "df.loc[1] = mom_dict.values() # df.loc 한 행 추가\n",
        "df = df.drop(0)\n",
        "df = df.reset_index()\n",
        "'''\n",
        "\n",
        "# 단어들을 컬럼으로 \n",
        "mom_dtm = DataFrame(mom_dict.values(),mom_dict.keys()).T\n",
        "\n",
        "\n",
        "# 강사님 풀이\n",
        "mom = pd.read_csv('c:/data/mom.txt',header=None,names=['sentens'])\n",
        "\n",
        "mom['sentens'] = mom['sentens'].str.lower()\n",
        "mom['sentens'].str.count('mommy').sum() # 각 문서별로 mommy가 들어있는 숫자.sum() -> 다 합쳐서 몇개있는지 확인\n",
        "mom['sentens'].str.count('i').sum() # count메소드는 단어별로 세는것이 아니라 i 철자를 모두 추출\n",
        "mom['sentens'].str.findall('i')\n",
        "\n",
        "data = ' '.join(mom['sentens'])\n",
        "words = set(data.split())\n",
        "words = [i for i in words if len(i) >= 2]\n",
        "words.sort() # sort() 오름차순 정렬 바로적용\n",
        "words\n",
        "\n",
        "freq_table = {}\n",
        "for i in words:\n",
        "    freq_table.setdefault(i,0)\n",
        "\n",
        "for j in mom['sentens'][1].split():\n",
        "    if j in freq_table.keys():\n",
        "        freq_table[j] += 1\n",
        "        \n",
        "freq_df = pd.DataFrame()\n",
        "\n",
        "temp = DataFrame(Series(freq_table)).T\n",
        "freq_df = pd.concat([freq_df, temp] ,ignore_index=True)\n",
        "\n",
        "temp = pd.DataFrame.from_dict([freq_table])\n",
        "freq_df = pd.concat([freq_df, temp] ,ignore_index=True)\n",
        "\n",
        "### 강사님 풀이 최종\n",
        "mom = pd.read_csv('c:/data/mom.txt',header=None,names=['sentens'])\n",
        "mom['sentens'] = mom['sentens'].str.lower()\n",
        "\n",
        "data = ' '.join(mom['sentens'])\n",
        "words = set(data.split())\n",
        "words = [i for i in words if len(i) >= 2]\n",
        "words.sort() # sort() 오름차순 정렬 바로적용\n",
        "words\n",
        "\n",
        "\n",
        "freq_df = pd.DataFrame()\n",
        "for i in mom['sentens']:\n",
        "    freq_table = {}\n",
        "    for w in words:\n",
        "        freq_table.setdefault(w,0)\n",
        "        \n",
        "    for j in i.split():\n",
        "        if j in freq_table.keys():\n",
        "            freq_table[j] += 1\n",
        "    temp = pd.DataFrame.from_dict([freq_table])    \n",
        "    freq_df = pd.concat([freq_df,temp],ignore_index=True)\n",
        "\n",
        "freq_df.sum(axis=0) # 컬럼별로 합\n",
        "freq_df.sum(axis=1) # 행(문서)별로 합\n",
        "\n",
        "freq_df['document'] = mom['sentens']\n",
        "del freq_df['document']\n",
        "\n",
        "# 특정한 인덱스 열 위치에 새로운 열을 추가할 때\n",
        "freq_df.insert(0,'document',mom['sentens'])\n",
        "freq_df.iloc[0]\n",
        "\n",
        "freq_df[words].sum()\n",
        "freq_df[words].sum().sort_values(ascending=False)\n",
        "\n",
        "freq_df.columns.difference(['document']) # document열만 제외하고 나머지 열만 보겠어 ( 특정한 열 제외)\n",
        "freq_df[freq_df.columns.difference(['document'])].sum().sum()\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "★ CountVectorizer\n",
        "- 문서집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW인코딩 벡터를 만든다.\n",
        "\n",
        "import operator\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "mom = pd.read_csv('c:/data/mom.txt',header=None,names=['sentens'])\n",
        "\n",
        "\n",
        "vect = CountVectorizer() #한글자 자동제거, 자동 소문자 변환\n",
        "vect.fit(mom['sentens']) # words를 만드는 작업(단어 사전)\n",
        "vect.vocabulary_ # 각단어 뒤에 value값은 인덱스(빈도수x)\n",
        "sorted(vect.vocabulary_.items(),key=operator.itemgetter(1))\n",
        "vect.get_feature_names() # 단어들만 추출\n",
        "\n",
        "feature_vector = vect.transform(mom['sentens'])\n",
        "feature_vector\n",
        "feature_vector.shape # 몇 행 몇 열인지 확인\n",
        "\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "★ 불용어\n",
        "- 의미없는 단어를 제외\n",
        "\n",
        "stopwords = ['am','are','be','and','is','the','then']\n",
        "vect = CountVectorizer(stop_words=stopwords)\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "vect = CountVectorizer(stop_words='english') # english 불용어 사전\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "★ 토큰\n",
        "\n",
        "vect = CountVectorizer(analyzer='word') # 공백문자를 기준으로 분리(기본값)\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "vect = CountVectorizer(analyzer='char') # anlayzer='char'는 알파벳 철자를 기준으로 나눔\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "vect = CountVectorizer(token_pattern='t\\w+') # token_pattern='정규표현식' , 여기서는 t로시작하는 단어들 추출, 정규표현식을 활용해서 내가 원하는 글자 패턴의 사전을 만들 수 있다.\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "★ n-gram(n그램)\n",
        "- 1개의 단어로 하나의 토큰을 생성 1-gram, monogram\n",
        "- 2개의 단어로 하나의 토큰을 생성 2-gram, bigram\n",
        "- 3개의 단어로 하나의 토큰을 생성 3-gram, trigram\n",
        "\n",
        "#vect = CountVectorizer(ngram_range=(1,1)) # 1-gram,monogram 기본값\n",
        "#vect = CountVectorizer(ngram_range=(2,2)) # 2-gram, bigram\n",
        "vect = CountVectorizer(ngram_range=(1,2)) # 1-grma, 2-gram 둘 다 생성\n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "vect = CountVectorizer(ngram_range=(1,2), token_pattern='t\\w+') \n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()m\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "★ 빈도수\n",
        "토큰의 빈도가 max_df로 지정한 값을 초과하거나 min_df로 지정한 값보다 작은 경우에는 무시한다.\n",
        "\n",
        "vect = CountVectorizer(analyzer='word', max_df=3, min_df=2) # max_df=초과, min_df=이하 \n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "★ TF-IDF(Term Frequency - Inverse Document Frequency)\n",
        "# TF(Term Frequency)\n",
        "-한 개 문서안에서 특정 단어의 등장 빈도수\n",
        "\n",
        "vect = CountVectorizer(analyzer='word') # max_df, min_df \n",
        "feature_vector = vect.fit_transform(mom['sentens']) # fit, transform 같이 사용\n",
        "feature_vector\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "df = pd.DataFrame(feature_vector.toarray(), columns=vect.get_feature_names()) # toarray() : array형태로 확인 -> dataframe으로 바로 생성가능\n",
        "df.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "# DF(Document Frequency)\n",
        "- 특정 단어가 나타나는 문서의 수\n",
        "- 문서빈도\n",
        "- DF가 클수록 여러 문서에 흔하게 사용된 일반적인 단어라고 할 수 있다.\n",
        "\n",
        "0      Mommy  I love you And I want to say thank you\n",
        "1   I am happy  everyday for the love you give to me\n",
        "2              Sometimes I am sad Sometimes I do cry\n",
        "3   Then I just remember  Mommy how much you love me\n",
        "4  Mommy you take care of me you help me to be st...\n",
        "\n",
        "mommy\n",
        "TF = ? [문서1] 1, [문서2] 0, [문서3] 1, [문서4] 1 , [문서5] 1\n",
        "DF = ? 4\n",
        "\n",
        "you\n",
        "TF = ? [문서1] 2, [문서2] 1, [문서3] 0, [문서4] 1, [문서5] 2\n",
        "DF = ? 4\n",
        "\n",
        "# IDF(Inverse Document Frequency)\n",
        "- DF에 역수로 변환을 해준 값을 의미한다. \n",
        "- DF의 역수이므로 DF가 클수록 IDF값은 작아지고 DF작을수록 IDF 커진다.\n",
        "- 여러 문서에서 자주 등장하는 단어들에 대해서 패널티를 주는 방법(a, the 같이 의미없이 자주 등장하는 단어)\n",
        "- IDF-SMOOTHING에 따라 달라진다.\n",
        "\n",
        "TF-IDF = TF * IDF\n",
        "\n",
        "IDF = log(N/DF), log(N/DF)+1, log((N+1)/(DF+1)) # 세가지 알고리즘이 사용된다.\n",
        "N : 전체 문서의 수\n",
        "DF : 특정 단어가 나타나는 문서의 수\n",
        "\n",
        "# 내 문서에는 두드러지게 나오지만 다른 문서에는 나오지 않는 것이 의미가 있다. / a, the 같은 자주나오지만 의미없는 단어들이 중요한 것처럼 많이 나오는 것이 문제(countvectorizer에서)\n",
        "# countvectorizer만 보고 하면 안되고 TF-IDF까지 봐야 의미있는 결과를 도출할 수 있다.(위에 줄 같은 문제가 생기기 때문에)\n",
        "# idf는 패널티 값으로 패널티를 준다고 생각\n",
        "# tf * idf 계산시 파급효과가 높아질 수도 있고 낮아질 수도 있다.\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tf_idf_vect = tfidf_vect.fit(mom['sentens'])\n",
        "tf_idf_vect.vocabulary_ # term을 추출하는 함수\n",
        "tf_idf_vect.get_feature_names() \n",
        "tf_idf_vect.idf_ # 각 feature의 idf 값\n",
        "\n",
        "tfidf_vect.transform(mom['sentens']).toarray()\n",
        "\n",
        "pd.DataFrame(tf_idf_vect.idf_,index=tf_idf_vect.get_feature_names(),columns=['idf'])\n",
        "\n",
        "poem = pd.read_csv('c:/data/poem.txt',header=None, names=['sentens'])\n",
        "poem\n",
        "\n",
        "vect = CountVectorizer()\n",
        "vect.fit(poem['sentens'])\n",
        "vect.vocabulary_\n",
        "vect.get_feature_names()\n",
        "\n",
        "feature_vector = vect.transform(poem['sentens'])\n",
        "feature_vector\n",
        "df = pd.DataFrame(feature_vector.toarray(),columns=vect.get_feature_names())\n",
        "df\n",
        "df.sum(axis=0)\n",
        "# 조사들, 의미있는 한 글자들이 나오지 않는 문제가 있음\n",
        "# 형태소 분석이 필요함\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def okt_pos(arg):\n",
        "    token_corpus = []\n",
        "    for i in okt.pos(arg):\n",
        "        if i[1] in ['Noun','Adjective']:\n",
        "            token_corpus.append(i[0])\n",
        "    return token_corpus\n",
        "\n",
        "[okt_pos(i) for i in poem['sentens']]\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos)\n",
        "cv_trans = cv.fit_transform(poem['sentens'])\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans.toarray()\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "df\n",
        "# 한글일 경우에는 형태소 분석을 해서 해야한다. 위처럼\n",
        "# n-gram도 생각해보기\n",
        "\n",
        "# 이 문서에만 두드러지게 나타나는 것을 찾겠다. tf-idf 이용해서\n",
        "# countvectorizer를 tf-idf로 변환하는 작업\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_trans = TfidfTransformer()\n",
        "tf_idf_vect = tfidf_trans.fit_transform(cv_trans)\n",
        "tf_idf_vect.toarray()\n",
        "\n",
        "df_tfidf = pd.DataFrame(tf_idf_vect.toarray(),columns=cv.get_feature_names())\n",
        "df_tfidf\n",
        "\n",
        "\n",
        "[문제194] 중앙일보 인공지능 뉴스 기사 검색 데이터를 이용해서 tf-idf를 생성해주세요.\n",
        "각각의 기사별로 빈도수를 확인. 다른 기사에 없는 단어들을 찾기.\n",
        "\n",
        "news = pd.read_csv('c:/data/joongang_ai_news_df.csv')\n",
        "news\n",
        "news.news\n",
        "'''\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect_t = tfidf_vect.fit(news.news)\n",
        "tfidf_vect_t.vocabulary_\n",
        "tfidf_vect_t.get_feature_names()\n",
        "tfidf_vect_t.idf_\n",
        "\n",
        "tfidf_vect_trans = tfidf_vect_t.transform(news.news).toarray()\n",
        "tfidf_vect_trans\n",
        "pd.DataFrame(tfidf_vect_trans,columns=tfidf_vect_t.get_feature_names())\n",
        "'''\n",
        "# countvectorizer\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    token_corpus = []\n",
        "    for i in okt.pos(arg):\n",
        "        if i[1] in ['Noun','Adjective']:\n",
        "            token_corpus.append(i[0])\n",
        "    return token_corpus\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos)\n",
        "cv_trans = cv.fit_transform(news.news)\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans.toarray()\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "# tfidf 변환\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_trans = TfidfTransformer()\n",
        "tf_idf_vect = tfidf_trans.fit_transform(cv_trans) # cv_trans는 countvectorizer를 만들어 놓은 변수\n",
        "tf_idf_vect.toarray()\n",
        "df_tfidf = pd.DataFrame(tf_idf_vect.toarray(),columns=cv.get_feature_names())\n",
        "df_tfidf\n",
        "\n",
        "# 10개 기사만 추출해서 수행\n",
        "data = news.news[0:10]\n",
        "cv = CountVectorizer(tokenizer=okt_pos, stop_words=['것','수','등'])\n",
        "cv_t = cv.fit_transform(data)\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_t.toarray()\n",
        "x = pd.DataFrame(cv_t.toarray(),columns=cv.get_feature_names())\n",
        "x.sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "tf_idf_trans = TfidfTransformer()\n",
        "tf_idf_trans_vect = tf_idf_trans.fit_transform(cv_t)\n",
        "tf_idf_trans_vect.toarray()\n",
        "df_tf_idf_trans_vect = pd.DataFrame(tf_idf_trans_vect.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "df_tf_idf_trans_vect.iloc[0].idxmax()\n",
        "for i in range(len(df_tf_idf_trans_vect)):\n",
        "    print(i,df_tf_idf_trans_vect.iloc[i].idxmax())\n",
        "\n",
        "\n",
        "# 강사님\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    token_corpus = []\n",
        "    for i in okt.pos(arg):\n",
        "        if i[1] in ['Noun','Adjective']:\n",
        "            token_corpus.append(i[0])\n",
        "    token_corpus = [x fo x in token_corpus if len(x) >1]\n",
        "    return token_corpus\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos)\n",
        "cv_t = cv.fit_transform(data)\n",
        "df = pd.DataFrame(cv_t.toarray(),columns=cv.get_feature_names())\n",
        "df.iloc[0].max()\n",
        "df.iloc[0].idxmax()\n",
        "df.loc[0,'연구']\n",
        "df.iloc[0].sort_values(ascending=False)[:10]\n",
        "\n",
        "df.iloc[1].max()\n",
        "df.iloc[1].idxmax() # 가장 큰 값의 인덱스구하는 함수\n",
        "df.loc[1,'연구']\n",
        "df.iloc[1].sort_values(ascending=False)[:10]\n",
        "\n",
        "for i in range(len(df)):\n",
        "    print(i, df.iloc[i].idxmax())\n",
        "    \n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib import font_manager,rc\n",
        "font_name = font_manager.FontProperties(fname = 'c:/Windows/Fonts/malgun.ttf').get_name()\n",
        "rc('font',family=font_name)\n",
        "from wordcloud import WordCloud\n",
        "w = WordCloud(font_path = 'c:/Windows/Fonts/malgun.ttf',\n",
        "              width=1000,height=1000,background_color='white').generate_from_frequencies(dict(df.sum(axis=0)))\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "w.to_file('c:/data/contents_noun_freq.jpg')\n",
        "\n",
        "import numpy as np\n",
        "\n"
      ]
    }
  ]
}
