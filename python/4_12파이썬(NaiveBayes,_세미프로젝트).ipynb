{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_13파이썬(NaiveBayes, 세미프로젝트).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxA4dELGOK0fmyvwuqXeM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_13%ED%8C%8C%EC%9D%B4%EC%8D%AC(NaiveBayes%2C_%EC%84%B8%EB%AF%B8%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngdUb3N-X9MF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "survey = pd.read_csv('c:/data/naive_survey.csv',header=None, names=['sentens','class'])\n",
        "survey\n",
        "# 레이블 기록해놓는 것이 쉽지 않을 것이다.\n",
        "\n",
        "cv = CountVectorizer(analyzer='word').fit(survey['sentens']) # analyzer='word'는 글자를 2자 이상으로 제한\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans = cv.transform(survey['sentens'])\n",
        "cv_trans.toarray()\n",
        "\n",
        "cv_trans[0:2].toarray()\n",
        "\n",
        "# 인코딩된 벡터를 실제 단어로 변환\n",
        "cv.inverse_transform(cv_trans[0:2])\n",
        "\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    return okt.morphs(arg)\n",
        "okt_pos(survey['sentens'][0])\n",
        "\n",
        "stopwords = ['은','는','이','가','에','고'] # 불용어사전 직접 만들기\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords).fit(survey['sentens'])\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans = cv.transform(survey['sentens'])\n",
        "cv_trans.toarray()\n",
        "\n",
        "cv_trans[0:2].toarray()\n",
        "\n",
        "# 인코딩된 벡터를 실제 단어로 변환\n",
        "cv.inverse_transform(cv_trans[0:2])\n",
        "\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(survey['sentens'])\n",
        "tfidf_vect.vocabulary_\n",
        "tfidf_vect.get_feature_names()\n",
        "tfidf_vect.idf_\n",
        "\n",
        "pd.DataFrame(tfidf_vect.idf_,index=tfidf_vect.get_feature_names(),columns=['idf']) # idf만 구함\n",
        "df_tfidf = pd.DataFrame(tfidf_vect.transform(survey['sentens']).toarray(),columns=tfidf_vect.get_feature_names())\n",
        "df_tfidf\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    return okt.morphs(arg)\n",
        "okt_pos(survey['sentens'][0])\n",
        "\n",
        "stopwords = ['은','는','이','가','에','고'] \n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=okt_pos, stop_words=stopwords)\n",
        "tfidf_vect.fit(survey['sentens'])\n",
        "tfidf_vect.vocabulary_\n",
        "tfidf_vect.get_feature_names()\n",
        "tfidf_vect.idf_\n",
        "\n",
        "pd.DataFrame(tfidf_vect.idf_,index=tfidf_vect.get_feature_names(),columns=['idf'])\n",
        "df_tfidf = pd.DataFrame(tfidf_vect.transform(survey['sentens']).toarray(),columns=tfidf_vect.get_feature_names())\n",
        "df_tfidf\n",
        "\n",
        "\n",
        "# CountVectorizer -> tfidf변환\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    return okt.morphs(arg)\n",
        "okt_pos(survey['sentens'][0])\n",
        "\n",
        "stopwords = ['은','는','이','가','에','고'] # 불용어사전 직접 만들기\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords).fit(survey['sentens'])\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans = cv.transform(survey['sentens'])\n",
        "cv_trans.toarray()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_trans = TfidfTransformer()\n",
        "pd.DataFrame(tfidf_trans.fit_transform(cv_trans).toarray(),columns=cv.get_feature_names())\n",
        "# tf-idf 값이 낮으면 중요도가 낮은 것 tf-idf값이 크면 중요도가 큰것\n",
        "\n",
        "[문제195] 메일안에 복권이라는 단어가 있을 경우에 스팸일 확률은?\n",
        "P(스팸) = 0.22\n",
        "P(복권|스팸) = 0.136\n",
        "P(복권|햄) = 0.025\n",
        "\n",
        "                P(복권∩스팸)        P(복권|스팸)*P(스팸)\n",
        "P(스팸|복권) = -------------- = ---------------\n",
        "                  P(복권)             P(복권)\n",
        "P(복권) = P(복권|스팸)*P(스팸) + P(복권|햄)*P(햄)\n",
        "\n",
        "P(스팸|복권) = (0.22*0.136)/(0.22*0.136 + 0.025*0.78) = 0.6\n",
        "\n",
        "# 조건부확률(conditional probability)\n",
        "- 이미 하나의 사건이 발생한 상태에서 또 다른 사건이 발생할 확률\n",
        "- 남성이라는 조건하에서 만족일 확률?\n",
        "           P(A∩B)\n",
        "P(A|B) = ----------\n",
        "            P(B) \n",
        "\n",
        "           P(남성|만족)\n",
        "P(만족|남성) = ----------\n",
        "            P(남성) \n",
        "\n",
        "                  P(여성|불만족)\n",
        "P(불만족|여성) = ---------------\n",
        "                   P(여성) \n",
        "\n",
        "★ 나이브베이즈(Naive Bayes)\n",
        "- 데이터를 나이브(단순)하게 독립적인 사건으로 가정하고 이 독립사건들을\n",
        "베이즈 이론에 대입시켜 가장 높은 확률의 레이블로 분류하는 알고리즘\n",
        "- 사전에 알고 있는 정보(예측변수)를 바탕으로 어떤 사건(결과변수)이 발생할 확률을 계산\n",
        "- 사건에 해당하는 결과변수는 범주형변수이어야하며 예측변수는 범주형 변수를 가정한다.\n",
        "\n",
        "                P(스팸∩복권)\n",
        "P(스팸|복권) = --------------\n",
        "                 P(복권)\n",
        " \n",
        "          P(A∩B)\n",
        "P(A|B) = ----------\n",
        "            P(B) \n",
        "\n",
        "P(A∩B) = P(A|B) * P(B) \n",
        "\n",
        "           P(A∩B)\n",
        "P(B|A) = ----------\n",
        "            P(A) \n",
        "\n",
        "P(B∩A) = P(B|A) * P(A) \n",
        "\n",
        "P(A∩B) = P(B∩A)\n",
        "\n",
        "P(A∩B) = P(B|A) * P(A) = P(A|B) * P(B) \n",
        "\n",
        "          P(A∩B)         P(B|A) * P(A)\n",
        "P(A|B) = ---------- = -------------------\n",
        "           P(B)              P(B)  \n",
        "P(B) = P(B|A) * P(A) + P(B|ㄱA) * P(ㄱA)\n",
        "               P(스팸∩복권)         P(복권|스팸) * 스팸\n",
        "P(스팸|복권) = -------------- = ----------------------------\n",
        "                  P(복권)             P(복권)\n",
        "\n",
        "               우도 * 사전확률\n",
        "사후확률 = --------------------\n",
        "                 주변우도\n",
        "               \n",
        "P(스팸|복권) : 사후확률, 이벤트가 발생 후 확률\n",
        "P(복권|스팸) : 우도(likelihood), 가능성(어떤 일이 있을 공산)\n",
        "p(스팸) : 사전확률, 이벤트가 발생 전 확률\n",
        "p(복권) : 주변우도(marginal likelihood), 확률변수가 아닌 상수로 고려\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    return okt.morphs(arg)\n",
        "okt_pos(survey['sentens'][0])\n",
        "\n",
        "stopwords = ['은','는','이','가','에','고'] # 불용어사전 직접 만들기\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords).fit(survey['sentens'])\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "x_train = cv.transform(survey['sentens']) # 학습데이터\n",
        "x_train.toarray()\n",
        "y_train = survey['class'] # 정답 라벨\n",
        "y_train\n",
        "\n",
        "import numpy as np\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train,y_train) # 학습데이터와 정답라벨로 모델 만들기\n",
        "\n",
        "x_test = cv.transform(Series(' '.join(okt_pos(\"함께 살고 있는 강아지가 너무 좋아\")))) # 테스트 데이터 / Series 형식으로 꼭 변경해줘야 수행이 됨\n",
        "x_test.toarray()\n",
        "cv.inverse_transform(x_test)\n",
        "nb.predict(x_test) # 테스트 데이터 예측\n",
        "\n",
        "x_test = cv.transform(Series(' '.join(okt_pos(\"오늘 하루는 기분이 우울하다.\")))) # 테스트 데이터\n",
        "x_test.toarray()\n",
        "cv.inverse_transform(x_test)\n",
        "nb.predict(x_test) # 테스트 데이터 예측\n",
        "\n",
        "\n",
        "# 분류 클래스\n",
        "nb.classes_ # 긍정, 부정\n",
        "\n",
        "# 분류 클래스 갯수\n",
        "nb.class_count_ # \n",
        "survey['class'].value_counts()\n",
        "\n",
        "# 사전확률\n",
        "np.exp(nb.class_log_prior_) # 긍정 8/15 부정 7/15\n",
        "\n",
        "# 분류 클래스별 컬럼의 값의 빈도수\n",
        "nb.feature_count_\n",
        "\n",
        "df = pd.DataFrame(nb.feature_count_,columns=cv.get_feature_names(), index=nb.classes_)\n",
        "df['짜증스러운']\n",
        "\n",
        "import numpy as np\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg):\n",
        "    return okt.morphs(arg)\n",
        "import pickle # 객체 그 자체로 저장할 수 있음\n",
        "\n",
        "# nb 객체 저장\n",
        "file = open('c:/data/classifier_1.pkl','wb')\n",
        "pickle.dump(nb,file) \n",
        "file.close()\n",
        "\n",
        "file = open('c:/data/cv.pkl','wb')\n",
        "pickle.dump(cv,file)\n",
        "file.close()\n",
        "\n",
        "# nb객체 받아오기\n",
        "file = open('c:/data/classifier_1.pkl','rb')\n",
        "classifier_new = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open('c:/data/cv.pkl','rb')\n",
        "cv_new = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "classifier_new.classes_\n",
        "classifier_new.class_count_\n",
        "np.exp(classifier_new.class_log_prior_)\n",
        "cv_new.vocabulary_\n",
        "cv_new.get_feature_names()\n",
        "\n",
        "x_test = cv_new.transform(Series(' '.join(okt_pos(\"함께 살고 있는 강아지가 너무 좋아\")))) # 테스트 데이터\n",
        "x_test.toarray()\n",
        "cv_new.inverse_transform(x_test)\n",
        "classifier_new.predict(x_test)\n",
        "\n",
        "# 학습데이터 정답라벨로 모델을 아주 잘 만들어 놓고 pickle을 이용하여 객체로 저장해둔다. 그리고 그 객체를 받아와서 사용한다. \n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------\n",
        "▶전처리 없이 R프로젝트로 구현했던 교보문고 도서분야 예측시스템 구현\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "\n",
        "# 데이터 읽어오기\n",
        "book = pd.read_csv('c:/data/book_field1.csv',encoding='utf-8')\n",
        "book\n",
        "\n",
        "# 데이터 형태소 나누기\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def okt_pos(arg): # n-gram 생각해보기, 전처리 작업 여기서\n",
        "    return okt.morphs(arg)\n",
        "\n",
        "#CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos)\n",
        "cv.fit(book.text)\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans = cv.transform(book.text)\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "#tf-idf 변환\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_t = TfidfTransformer()\n",
        "tf = tfidf_t.fit_transform(cv_trans)\n",
        "pd.DataFrame(tf.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "# 학습데이터 정답라벨 모델 만들기\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mn = MultinomialNB()\n",
        "y_label = book.field\n",
        "x_label =  cv.transform(book.text)\n",
        "mn.fit(x_label,y_label)\n",
        "\n",
        "# 테스트 데이터(정치사회에서 제목과 소개글 가져옴)\n",
        "x_test = cv.transform(Series(' '.join(okt_pos('가불 선진국, 한국은 전 세계가 놀랄 정도로 가파른 성장을 거듭해오며 선진국 반열에 들어섰다. 그러나 선진국 대한민국의 환호 뒤에는 수많은 사회적·경제적 약자의 희생이 놓여 있다. 문재인 정부의 공직자로 활동한 저자는 ‘사회권 보장’을 통해 그동안 소외돼온 약자층에 진 ‘빚’을 갚아야 한다고 주장한다. 그래야만 선진국 반열에 오르기 위해 ‘가불’했던 ‘빚’을 갚고 지속 가능한 선진국이 될 수 있다.'))))\n",
        "mn.predict(x_test)\n",
        "\n",
        "#------------------------------------------------------\n",
        "▶ 전처리 작업 포함 R프로젝트로 구현했던 교보문고 도서분야 예측시스템 구현\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "\n",
        "# 데이터 읽어오기\n",
        "\n",
        "book = pd.read_csv('c:/data/book_field1.csv',encoding='utf-8')\n",
        "book\n",
        "\n",
        "# 데이터 형태소 나누기\n",
        "def okt_pos(arg):\n",
        "    token = []\n",
        "    for j in okt.pos(arg):\n",
        "        if j[1] in ['Noun','Adjective','Alpha']:  # 명사, 형용사, 영어\n",
        "            token.append(j[0]) \n",
        "    token = [i for i in token if len(i)>=2]\n",
        "    return token\n",
        "\n",
        "# 불용어\n",
        "stopwords = ['지난','이후','독자','작가','있다','같은','이로','인해','있게','누구','있으며']\n",
        "\n",
        "#CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(tokenizer=okt_pos,stop_words=stopwords)\n",
        "cv.fit(book.text)\n",
        "cv.vocabulary_\n",
        "cv.get_feature_names()\n",
        "cv_trans = cv.transform(book.text)\n",
        "df = pd.DataFrame(cv_trans.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "#tf-idf 변환\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_t = TfidfTransformer()\n",
        "tf = tfidf_t.fit_transform(cv_trans)\n",
        "pd.DataFrame(tf.toarray(),columns=cv.get_feature_names())\n",
        "\n",
        "# 학습데이터 정답라벨 모델 만들기\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mn = MultinomialNB()\n",
        "y_label = book.field\n",
        "x_label =  cv.transform(book.text)\n",
        "mn.fit(x_label,y_label)\n",
        "\n",
        "# 테스트 데이터\n",
        "x_test = cv.transform(Series(' '.join(okt_pos('가불 선진국, 한국은 전 세계가 놀랄 정도로 가파른 성장을 거듭해오며 선진국 반열에 들어섰다. 그러나 선진국 대한민국의 환호 뒤에는 수많은 사회적·경제적 약자의 희생이 놓여 있다. 문재인 정부의 공직자로 활동한 저자는 ‘사회권 보장’을 통해 그동안 소외돼온 약자층에 진 ‘빚’을 갚아야 한다고 주장한다. 그래야만 선진국 반열에 오르기 위해 ‘가불’했던 ‘빚’을 갚고 지속 가능한 선진국이 될 수 있다.'))))\n",
        "mn.predict(x_test) # 일치(정치/사회)\n",
        "x_test = cv.transform(Series(' '.join(okt_pos('꽃을 보듯 너를 본다,만인의 심금을 울릴 수 있는 서정시의 진수 블랙핑크(BLACKPINK)의 지수와 세계적인 보컬 그룹인 방탄소년단(BTS)의 RM, 송혜교와 박보검 등은 물론 전국민의 애송시인 [풀꽃]이 수록되어있는 나태주 시집 [꽃을 보듯 너를 본다]. “자세히 보아야/ 예쁘다//오래 보아야/사랑스럽다//너도 그렇다.” 나태주 시인의 [풀꽃]은 전국민의 애송시라고 해도 과언이 아니다. 순수하고 꾸밈없는 시어들은 풀꽃의 시처럼 독자들에게 꾸준히 읽히면 더 큰 사랑을 받게 됐다. 평범한 것에 아름다움을 보는 눈, 별 볼일 없다고 생각했던 무언가를 다시보게 하는 힘이 이 시집에 있다.'))))\n",
        "mn.predict(x_test) # 일치(시/에세이)\n",
        "x_test = cv.transform(Series(' '.join(okt_pos('나 자신을 알라, 뇌과학으로 다시 태어난 소크라테스의 지혜 2500년 전 소크라테스는 델포이 신전의 어느 돌에 새겨진 “너 자신을 알라!”라는 명제를 철학함의 근본으로 삼았다. 고대 그리스 시대, 인간이 추구해야 할 가장 중요한 과제는 내가 누구인지 아는 것, 즉 자기인식이었다. 자기인식은 “자기 자신과 자신의 행동을 의식적으로 성찰할 수 있는 능력”이다. 자기인식이 발달한 사람은 자신의 상황을 정확히 인식할 수 있을 뿐 아니라 뛰어난 마음읽기 능력을 바탕으로 다른 사람의 처지와 상황, 역량도 제대로 파악한다. 세월이 흘러, 현대 과학은 인간의 뇌가 정확히 자기인식을 수행하게끔 만들어져 있음을 밝혀냈다. 최신 뇌과학 연구에 따르면, 인간의 뇌는 불확실성을 판단하고, 끊임없이 자기 자신의 상태와 행동을 모니터링한다. 이 책은 뇌과학과 심리학을 기반으로 인간의 고유한 능력인 메타인지와 자기인식이란 무엇이며, 그것을 어떻게 활용할 수 있는지를 살펴본다.'))))\n",
        "mn.predict(x_test) # 일치(인문학)\n",
        "\n",
        "# 테스트 데이터 추출\n",
        "'''\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "url = 'http://www.kyobobook.co.kr/indexKor.laf?mallGb=KOR&orderClick=c1a1'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "\n",
        "testdata_df = DataFrame(columns=['field','text'])\n",
        "for j in range(1,5):\n",
        "    driver.find_element(By.XPATH,'//*[@id=\"main_snb\"]/div[1]/ul[1]/li['+str(i)+']/a').click() # 소설 분야 버튼        \n",
        "    for i in range(1,6):\n",
        "        driver.find_element(By.XPATH,'//*[@id=\"_template1872\"]/div/ul/li['+str(i)+']/div/div[1]/a').click() # 최신작품 버튼\n",
        "        title = driver.find_element(By.XPATH,'//*[@id=\"container\"]/div[2]/form/div[1]/h1/strong').text # 제목\n",
        "        content = driver.find_element(By.XPATH,'//*[@id=\"container\"]/div[5]/div[1]/div[3]/div[2]').text # 책 소개 내용\n",
        "        testdata_df = testdata_df.append({'text':title + ',' + content},ignore_index=True)\n",
        "        driver.back()\n",
        "        time.sleep(2)\n",
        "    driver.back()\n",
        "\n",
        "'''\n",
        "# 분류 클래스\n",
        "mn.classes_ # 'art_popularculture', 'computer_it', 'economy', 'history_culture','humanities', 'novel', 'poem_essay', 'politics_society', 'religion', 'selfdevelope', 'sience', 'technology_engineering'\n",
        "\n",
        "# 분류 클래스 갯수\n",
        "mn.class_count_ \n",
        "book['field'].value_counts()\n",
        "\n",
        "# 사전확률\n",
        "import numpy as np\n",
        "np.exp(mn.class_log_prior_) # 긍정 8/15 부정 7/15\n",
        "\n",
        "# 분류 클래스별 컬럼의 값의 빈도수\n",
        "mn.feature_count_\n",
        "\n",
        "pd.DataFrame(mn.feature_count_,columns=cv.get_feature_names(), index=mn.classes_)\n",
        "\n",
        "# 분류기 모델 객체 저장\n",
        "import pickle\n",
        "with open('c:/data/classifier_book.pkl','wb') as file:\n",
        "    pickle.dump(mn,file)\n",
        "with open('c:/data/cv_book.pkl','wb') as file:\n",
        "    pickle.dump(cv,file)\n",
        "with open('c:/data/classifier_book.pkl','rb') as file:\n",
        "    classifier1 = pickle.load(file)\n",
        "classifier1.predict(x_test)\n"
      ]
    }
  ]
}
