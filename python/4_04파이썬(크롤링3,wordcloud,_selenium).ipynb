{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_04파이썬(크롤링3,wordcloud, selenium).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQIoGt45UfEV/pIyo9nOKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_04%ED%8C%8C%EC%9D%B4%EC%8D%AC(%ED%81%AC%EB%A1%A4%EB%A7%813%2Cwordcloud%2C_selenium).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeSosVtO5J2f"
      },
      "outputs": [],
      "source": [
        "[문제179] 네이버의 언론사별 랭킹뉴스 데이터를 수집해주세요.\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "from pandas import Series, DataFrame\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "# 랭킹뉴스 첫 번째 언론사의 첫 번째 기사 제목과 url test\n",
        "url = soup.select_one('div.rankingnews_box > ul.rankingnews_list > li > div.list_content > a')['href']\n",
        "\n",
        "# url 들어가서 기사 제목, text 추출 test\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "soup.select_one('h2.media_end_head_headline').text # 제목\n",
        "soup.select_one('div#dic_area')# text(내용)\n",
        "\n",
        "\n",
        "\n",
        "# 랭킹 언론사별 5개의 뉴스 url 추출 \n",
        "\n",
        "url =  'https://news.naver.com/main/ranking/popularDay.naver'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "s = soup.select('ul.rankingnews_list > li > a')\n",
        "total_url=[]\n",
        "for i in s:\n",
        "    total_url.append(i['href'])\n",
        "\n",
        "total_url\n",
        "\n",
        "# url마다 기사 제목, text 추출\n",
        "news_df = DataFrame(columns=['title','contents'])\n",
        "for i in total_url:\n",
        "    html = urlopen(i)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    \n",
        "    # 제목\n",
        "    title = soup.select_one('h2.media_end_head_headline').text \n",
        "    \n",
        "    # 내용text\n",
        "    contents = soup.select_one('div#dic_area')\n",
        "    \n",
        "    news_df = news_df.append({'title':title,'contents':contents},ignore_index=True)\n",
        "    time.sleep(2)\n",
        "    \n",
        "####### 강사님\n",
        "url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
        "html = urlopen(url)\n",
        "soup = beautifulSoup(html,'html.parser')\n",
        "div_rank = soup.select('div.rankingnews_box')\n",
        "len(div_rank)\n",
        "\n",
        "# 첫 번째 언론사\n",
        "div_rank[0].select_one('strong.rankingnews_name').text # 언론사 이름\n",
        "div_rank[0].select('ul > li > div > a')[0].string\n",
        "div_rank[0].select('ul > li > div > a')[0]['href'] #url 추출\n",
        "div_rank[0].select('ul > li > div > a')[1].string\n",
        "div_rank[0].select('ul > li > div > a')[1]['href'] #url 추출\n",
        "\n",
        "# 두 번째 언론사\n",
        "div_rank[1].select_one('strong.rankingnews_name').text # 언론사 이름\n",
        "div_rank[1].select('ul > li > div > a')[0].string\n",
        "div_rank[1].select('ul > li > div > a')[0]['href'] #url 추출\n",
        "div_rank[1].select('ul > li > div > a')[1].string\n",
        "div_rank[1].select('ul > li > div > a')[1]['href'] #url 추출\n",
        "\n",
        "html = urlopen(div_rank[0].select('ul > li > div > a')[0]['href'])\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "soup.select_one('div#newsct_article > div#dic_area').text\n",
        "\n",
        "# 결과\n",
        "url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "div_rank = soup.select('div.rankingnews_box')\n",
        "\n",
        "\n",
        "news_df = DataFrame(columns=['press_name','title','contents'])\n",
        "for i in range(0,len(div_rank)+1):\n",
        "    press_name = div_rank[i].select_one('strong.rankingnews_name').text\n",
        "    for j in range(0,6):\n",
        "        html = urlopen(div_rank[i].select('ul > li > div > a')[j]['href'])\n",
        "        soup = BeautifulSoup(html,'html.parser')\n",
        "        # 제목\n",
        "        title = soup.select_one('h2.media_end_head_headline').text \n",
        "        # 내용\n",
        "        contents = soup.select_one('div#newsct_article > div#dic_area').text\n",
        "        \n",
        "        news_df = news_df.append({'press_name':press_name,'title':title,'contents':contents},ignore_index=True)\n",
        "        time.sleep(2)\n",
        "\n",
        "# 최종 ( 2개 언론사만 추출 다 추출하려면 시간이 오래걸림)\n",
        "url = 'https://news.naver.com/main/ranking/popularDay.naver?mid=etc&sid1=111'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "news_ranking_df = DataFrame(columns=['언론사','랭킹URL','title','news'])    \n",
        "\n",
        "for i in soup.select('div.rankingnews_box')[:2]:\n",
        "    press = i.select_one('strong.rankingnews_name').text\n",
        "    news_ranking=[]\n",
        "    news_ranking_url = []\n",
        "    news = []\n",
        "    \n",
        "    for j in i.select('ul > li > div > a'):\n",
        "        news_ranking.append(j.string)\n",
        "        news_ranking_url.append(j['href'])\n",
        "        html = urlopen(j['href'])\n",
        "        soup = BeautifulSoup(html,'html.parser')\n",
        "        news.append(soup.select_one('div#newsct_article > div#dic_area').text.strip())\n",
        "        time.sleep(2)\n",
        "    news_ranking_df = news_ranking_df.append(DataFrame({'언론사':press,'랭킹URL':news_ranking_url,'title':news_ranking,'news':news}),ignore_index=True)\n",
        "    print('수집 언론사 : ',press)\n",
        "    time.sleep(2)\n",
        "    \n",
        "news_ranking_df.to_csv('c:/data/news_ranking.csv',index=False) # 파일로 저장\n",
        "\n",
        "\n",
        "[문제180] 수집한 뉴스 기사 내용에서 [내용..] 출력해주세요.\n",
        "re.findall('\\[.+\\]',news_ranking_df['news'][0])\n",
        "\n",
        "#강사님\n",
        "re.findall('\\[.+?\\]',news_ranking_df.news[0])\n",
        "\n",
        "news_ranking_df.news.apply(lambda x : re.findall('\\[.+?\\]', x))\n",
        "\n",
        "[문제181] 수집한 뉴스 기사 내용에서 <내용..> 출력해주세요.\n",
        "re.findall('\\<.+\\>',news_ranking_df['news'][0])\n",
        "\n",
        "news_ranking_df.news.apply(lambda x : re.findall('\\<.+?\\>', x))\n",
        "\n",
        "[문제182] 수집한 뉴스 기사 내용에서 (내용..) 출력해주세요.\n",
        "re.findall('\\([가-힣A-z]+[^\\)]*\\)',news_ranking_df['news'][1])\n",
        "\n",
        "news_ranking_df.news.apply(lambda x : re.findall('\\(.+?\\)', x))\n",
        "\n",
        "[문제183] 네이버 웹툰 인기순을 딕셔너리 자료형에 수집해주세요.\n",
        "comic_dict = {}\n",
        "url = 'https://comic.naver.com/index'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "li = soup.select('ol#realTimeRankFavorite >li')\n",
        "comic_dict[li[0]['class']] = li[0].select_one('a').text\n",
        "comic_dict[li[0]['class'][0]] = li[0].select_one('a').text\n",
        "comic_dict[''.join(li[0]['class'])] = li[0].select_one('a').text\n",
        "\n",
        "li[0]['class'] -> str변환?\n",
        "li[0]['class'][0]\n",
        "''.join(li[0]['class'])\n",
        "\n",
        "for i in li:\n",
        "    comic_dict[''.join(i['class'])] = i.select_one('a').text\n",
        "\n",
        "dict([(key.replace('rank',''),value)for key,value in comic_dict.items()])\n",
        "\n",
        "\n",
        "\n",
        "############################################33##########3\n",
        "★ wordcloud\n",
        "\n",
        "■ wordcloud 수동으로 받아서 설치\n",
        "'''\n",
        "https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud\n",
        "(base) C:\\WINDOWS\\system32>python --version\n",
        "Python 3.9.7\n",
        "\n",
        "(base) C:\\WINDOWS\\system32>cd C:\\Users\\ehdgh\\anaconda3\\Lib\n",
        "\n",
        "(base) C:\\Users\\ehdgh\\anaconda3\\Lib>dir *.whl\n",
        " C 드라이브의 볼륨에는 이름이 없습니다.\n",
        " 볼륨 일련 번호: FC92-4C2B\n",
        "\n",
        " C:\\Users\\ehdgh\\anaconda3\\Lib 디렉터리\n",
        "\n",
        "2022-04-04  오후 02:17           161,093 wordcloud-1.8.1-cp39-cp39-win_amd64.whl\n",
        "               1개 파일             161,093 바이트\n",
        "               0개 디렉터리  106,958,286,848 바이트 남음\n",
        "\n",
        "(base) C:\\Users\\ehdgh\\anaconda3\\Lib>pip install wordcloud-1.8.1-cp39-cp39-win_amd64.whl\n",
        "Processing c:\\users\\ehdgh\\anaconda3\\lib\\wordcloud-1.8.1-cp39-cp39-win_amd64.whl\n",
        "Requirement already satisfied: pillow in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from wordcloud==1.8.1) (8.4.0)\n",
        "Requirement already satisfied: matplotlib in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from wordcloud==1.8.1) (3.4.3)\n",
        "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from wordcloud==1.8.1) (1.20.3)\n",
        "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud==1.8.1) (1.3.1)\n",
        "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud==1.8.1) (2.8.2)\n",
        "Requirement already satisfied: cycler>=0.10 in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud==1.8.1) (0.10.0)\n",
        "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud==1.8.1) (3.0.4)\n",
        "Requirement already satisfied: six in c:\\users\\ehdgh\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud==1.8.1) (1.16.0)\n",
        "Installing collected packages: wordcloud\n",
        "Successfully installed wordcloud-1.8.1\n",
        "'''\n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib import font_manager,rc\n",
        "font_name = font_manager.FontProperties(fname='C:/Windows/Fonts/HMKMAMI.TTF').get_name()\n",
        "rc('font',family=font_name)\n",
        "from wordcloud import WordCloud\n",
        "word = {'떡볶이':100,'감자탕':50,'순대국밥':10,'치즈':70,'볶음밥':80,'김밥':20,'짜장면':300,'야끼만두':150,'치킨':100,\n",
        "        '핫도그':90,'호두':60,'알감자':50,'짬뽕':200,'탕수육':150}\n",
        "\n",
        "w = WordCloud(font_path='C:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              width=900,height=500).generate_from_frequencies(word)\n",
        "plt.imshow(w) # wordcloud 보여주기\n",
        "plt.axis('off') # 축 없애기\n",
        "\n",
        "news_ranking_df.news\n",
        "\n",
        "txt = ' '.join(news_ranking_df.news)\n",
        "txt\n",
        "w = WordCloud(font_path='C:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white',\n",
        "              width=900,height=500).generate(txt) # generate : 텍스트를 단어들로 변환해줌\n",
        "plt.imshow(w) # wordcloud 보여주기\n",
        "plt.axis('off') # 축 없애기\n",
        "\n",
        "\n",
        "# 영화 감상평 wordcloud\n",
        "movie = DataFrame(columns=['day','id','review','point'])\n",
        "for j in range(1,11):\n",
        "        \n",
        "    url = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=190695&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='+str(j) \n",
        "    # 영화 code만 바꾸면 다른 영화들의 평도 다 가져올 수 있다\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    \n",
        "    review = []\n",
        "    for i in soup.select('div.score_reple > p'):\n",
        "        if i.select_one('span.ico_viewer') == None:\n",
        "            review.append(i.text.strip())\n",
        "        else:\n",
        "            i.select_one('span.ico_viewer').extract()\n",
        "            review.append(i.text.strip())\n",
        "    \n",
        "    point = []\n",
        "    for i in soup.select('div.star_score > em'):\n",
        "        point.append(i.text)\n",
        "    \n",
        "    id = []\n",
        "    for i in soup.select('div.score_reple > dl > dt > em > a > span'):\n",
        "        id.append(i.text)\n",
        "    \n",
        "    day = []\n",
        "    for i in soup.select('div.score_reple > dl > dt > em:nth-of-type(2)'):\n",
        "        day.append(i.text)\n",
        "    \n",
        "    \n",
        "    movie = movie.append(DataFrame({'day':day,'id':id,'review':review,'point':point}))\n",
        "    \n",
        "    time.sleep(3)\n",
        "    \n",
        "txt = ' '.join(movie.review)\n",
        "w = WordCloud(font_path='C:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate(txt)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "[문제184] winter.txt 텍스트를 이용해서 워드클라우드 생성해주세요. # 겨울왕국 대사\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "alice_mask = np.array(Image.open(\"c:/data/alice_mask.png\"))\n",
        "with open('c:/data/winter.txt','r') as file:\n",
        "    winter = file.read()\n",
        "\n",
        "len(winter)\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "          background_color='white',\n",
        "          colormap='twilight',\n",
        "          mask=alice_mask,\n",
        "          contour_width=3,\n",
        "          contour_color='steelblue').generate(winter)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "###################\n",
        "★ selenium\n",
        "웹 브라우저를 컨트롤하여 웹을 자동화 하는 도구\n",
        "\n",
        "# selenium 설치 ( anaconda 관리자 실행 )\n",
        "(base) C:\\Users\\ehdgh\\anaconda3\\Lib>pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "url = \"https:/www.naver.com\"\n",
        "driver = webdriver.Chrome(\"c:/data/chromedriver.exe\") #여기서 오류가 나면 크롬 드라이버 다운받고 원하는 곳에 저장\n",
        "driver.get(url) # url에 접속\n",
        "driver.save_screenshot(\"c:/data/naver.png\") # 스크린샷 저장\n",
        "driver.quit() # 종료\n",
        "\n",
        "\n",
        "driver = webdriver.Chrome(\"c:/data/chromedriver.exe\")\n",
        "url = 'https://comic.naver.com/index'\n",
        "driver.get(url) \n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "comic_dict = {}\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "li = soup.select('ol#realTimeRankFavorite >li')\n",
        "\n",
        "for i in li:\n",
        "    comic_dict[''.join(i['class'])] = i.select_one('a').text\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "url = \"https:/www.naver.com\"\n",
        "driver = webdriver.Chrome(\"c:/data/chromedriver.exe\") #여기서 오류가 나면 크롬 드라이버 다운받고 원하는 곳에 저장\n",
        "driver.get(url) # url에 접속\n",
        "driver.implicitly_wait(2) # time.sleep과 같은 기능\n",
        "driver.find_element(By.CLASS_NAME,'id_finance').click() # 태그 요소 클릭하기\n",
        "html = driver.page_source # 현재 접속 페이지 소스 추출하기\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup.select('p')\n",
        "driver.quit() # 종료\n",
        "\n",
        "url = \"https://search.naver.com/search.naver?where=image\"\n",
        "driver = webdriver.Chrome(\"c:/data/chromedriver.exe\")\n",
        "driver.get(url) # url에 접속\n",
        "driver.implicitly_wait(2) # time.sleep과 같은 기능\n",
        "element = driver.find_element(By.CLASS_NAME,'box_window')\n",
        "element.clear()\n",
        "element.send_keys('강아지')\n",
        "driver.implicitly_wait(2)\n",
        "element.submit()\n",
        "\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n",
        "driver.find_element(By.TAG_NAME,'body').send_keys(Keys.HOME)\n",
        "driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_DOWN)\n",
        "driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_UP)\n",
        "\n",
        "for i in range(5):\n",
        "    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "soup\n",
        "img_url = []\n",
        "for i in  soup.select('img._image'):\n",
        "    img_url.append(i.attrs['src'])\n",
        "    \n",
        "len(img_url)\n",
        "img_url[0]\n",
        "\n",
        "import urllib.request as req \n",
        "# 이미지 파일 url을 이용하여 다운받는 함수 urlretrieve\n",
        "req.urlretrieve(img_url[0],'c:/img/python_img/1.jpg')\n",
        "\n",
        "\n",
        "[문제]다음 강아지 이미지 다운\n",
        "url = 'https://search.daum.net/search?w=img'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(2)\n",
        "element = driver.find_element(By.CLASS_NAME,'tf_keyword')\n",
        "element.clear()\n",
        "element.send_keys('강아지') # 검색창에 강아지 단어 입력\n",
        "element.submit() # 수행\n",
        "\n",
        "for i in range(4):\n",
        "    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "img_url = []\n",
        "for i in soup.select('img.thumb_img'):\n",
        "    img_url.append(i.attrs['src'])\n",
        "\n",
        "num = 1\n",
        "for i in img_url:\n",
        "    req.urlretrieve(i,'c:/img/python_img/dog'+str(num)+'.png')\n",
        "    num += 1\n",
        "\n",
        "#\n",
        "driver.find_element(By.CLASS_NAME,'expender open').click() # 태그 요소 클릭하기\n"
      ]
    }
  ]
}
