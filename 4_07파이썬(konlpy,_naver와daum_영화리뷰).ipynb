{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_07파이썬(konlpy, naver와daum 영화리뷰).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1fRH1kwCy/RPQmj9qq9OH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehdghks316/itwill_studyGroup_D/blob/main/4_07%ED%8C%8C%EC%9D%B4%EC%8D%AC(konlpy%2C_naver%EC%99%80daum_%EC%98%81%ED%99%94%EB%A6%AC%EB%B7%B0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0GAQTLwWYFM"
      },
      "outputs": [],
      "source": [
        "형태소분석 사이트\n",
        "https://konlpy.org/ko/latest/\n",
        "\n",
        "★konlpy\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "text = \"통찰력은 사물이나 현상의 원인과 결과를 이해하고 간파하는 능력이다 통찰력을 얻는 좋은 방법은 독서이다.\"\n",
        "\n",
        "# 문장을 분석\n",
        "kkma.sentences(text)\n",
        "\n",
        "# 명사 분석\n",
        "kkma.nouns(text)\n",
        "\n",
        "# 형태소 분석(품사태깅)\n",
        "kkma.pos(text)\n",
        "\n",
        "# 형태소 추출\n",
        "kkma.morphs(text)\n",
        "\n",
        "\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "\n",
        "# 명사 분석\n",
        "komoran.nouns(text)\n",
        "\n",
        "# 형태소 분석(품사태깅)\n",
        "komoran.pos(text)\n",
        "\n",
        "# 형태소 추출\n",
        "komoran.morphs(text)\n",
        "\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "\n",
        "# 명사 분석\n",
        "hannanum.nouns(text)\n",
        "\n",
        "# 형태소 분석(품사태깅)\n",
        "hannanum.pos(text)\n",
        "\n",
        "# 형태소 추출\n",
        "hannanum.morphs(text)\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "# 명사 분석\n",
        "okt.nouns(text)\n",
        "\n",
        "# 형태소 분석(품사태깅)\n",
        "okt.pos(text)\n",
        "\n",
        "# 형태소 추출\n",
        "okt.morphs(text)\n",
        "\n",
        "\n",
        "[문제190] 중앙일보 인공지능 기사 검색을 통해서 받아 놓은 뉴스기사를 명사만 추출해서 워드클라우드 생성해주세요.\n",
        "\n",
        "with open('c:/data/joongang1_ai_news.txt','r', encoding='utf-8') as file:\n",
        "    news = file.read()\n",
        "\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "news = news.replace('등|수|이|의|것','')\n",
        "\n",
        "komoran.nouns(news) # 오류\n",
        "text = hannanum.nouns(news) # 선택\n",
        "okt.nouns(news)\n",
        "kkma.nouns(news) # 너무 오래걸리는 듯\n",
        "\n",
        "text_list =[]\n",
        "for i in text_dic:\n",
        "    if i in ('등','것','중','이','의'):   \n",
        "        continue\n",
        "    else:\n",
        "        text_list.append(i)\n",
        "\n",
        "text_dic = collections.Counter(text_list)\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(text_dic)\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "#강사님( 텍스트 양이 너무 많을 때 문장별로 나눠서 수행해주는 것이 좋다. )\n",
        "import pandas as pd\n",
        "contents = pd.read_csv('c:/data/joongang_ai_news_df.csv')\n",
        "contents.info()\n",
        "\n",
        "news = ' '.join(contents['news'])\n",
        "len(news)\n",
        "\n",
        "news_noun = hannanum.nouns(news)\n",
        "\n",
        "contents['noun'] = contents['news'].apply(lambda x : hannanum.nouns(x))\n",
        "contents_noun = [j for i in contents['noun'] for j in i if len(j) >= 2]\n",
        "contents_noun\n",
        "\n",
        "import operator\n",
        "contents_noun_freq = collections.Counter(contents_noun)\n",
        "contents_noun_freq_sorted = sorted(contents_noun_freq.items(),key=operator.itemgetter(1),reverse=True)\n",
        "contents_noun_freq_sorted # 정렬\n",
        "\n",
        "contents_noun_freq.most_common(50) # 상위 50개만 뽑는 함수\n",
        "\n",
        "contents_noun_freq\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(contents_noun_freq) # genertae_from_frequencies는 각 단어별 빈도수가 구해진 데이터로\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "w.to_file('c:/data/contents_noun_freq.jpg')\n",
        "\n",
        "stop_words = ['분야','지난해','국내','지원','제공','대표','올해','진행','출시'] # 불용어 사전\n",
        "news_nouns = [word for word in contents_noun if word not in stop_words and len(word) >= 2]\n",
        "news_nouns_freq = collections.Counter(news_nouns)\n",
        "news_nouns_freq.most_common(50)\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(news_nouns_freq) # genertae_from_frequencies는 각 단어별 빈도수가 구해진 데이터로\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "# 딕셔너리 변수를 데이터프레임으로 변환하는 방법\n",
        "pd.DataFrame.from_dict([news_nouns_freq])\n",
        "pd.DataFrame.from_dict([news_nouns_freq]).T\n",
        "\n",
        "pd.DataFrame.from_records([news_nouns_freq])\n",
        "news_freq_df = pd.DataFrame.from_records([news_nouns_freq]).T\n",
        "news_freq_df.reset_index(inplace=True) #inplace 바로 적용\n",
        "news_freq_df.columns = ['word','freq']\n",
        "\n",
        "top50 = news_freq_df[news_freq_df['freq'] >= 50] # 빈도수가 50 이상인 단어\n",
        "top50\n",
        "\n",
        "# 데이터프레임을 딕셔너리 변수로 변환하는 방법\n",
        "top50.set_index('word') # 인덱스를 word로\n",
        "\n",
        "top50_freq = top50.set_index('word').to_dict()['freq'] # dictionary 형식으로 만들기\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(top50_freq) \n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "[문제191] okt형태소 분석기를 이용해서 중앙일보 인공지능 뉴스 기사에서 명사, 형용사를 추출해서 워드클라우드를 생성해주세요\n",
        "import pandas as pd\n",
        "contents = pd.read_csv('c:/data/joongang_ai_news_df.csv')\n",
        "contents.info()\n",
        "\n",
        "news = ' '.join(contents['news'])\n",
        "\n",
        "news_okt = okt.pos(news)\n",
        "news_okt\n",
        "'''\n",
        "news_okt_dict= {}\n",
        "for i,j in news_okt:\n",
        "    if len(i) > 2:\n",
        "        news_okt_dict[i] = j\n",
        "'''\n",
        "text = []\n",
        "for i,j in news_okt:\n",
        "    if len(i) > 2:\n",
        "        if j in ('Noun','Adjective'):\n",
        "            text.append(i)\n",
        "\n",
        "news_noun_adj = Counter(text)\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(news_noun_adj) \n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "# 강사님 \n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def okt_pos(arg):\n",
        "    token = []\n",
        "    for i in okt.pos(arg):\n",
        "        if i[1] in ['None','Adjective']:\n",
        "            token.append(i[0])\n",
        "    return token\n",
        "\n",
        "okt_pos(contents['news'][0])\n",
        "\n",
        "contents_token = [okt_pos(i) for i in contents['news']] # 리스트객체 방법\n",
        "contents_token = contents['news'].apply(lambda x : okt_pos(x)) # apply 방법\n",
        "\n",
        "stop_words = ['분야','지난해','국내','지원','제공','대표','올해','진행','출시'] # 불용어 사전 \n",
        "\n",
        "news_token = [word2 for word in contents_token for word2 in word if word2 not in stop_words and len(word2) >= 2] # 중첩리스트가 되기때문에 2중 for문 사용\n",
        "\n",
        "w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "              background_color='white').generate_from_frequencies(Counter(news_token)) \n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "\n",
        "[문제192] 네이버, 다음에서 같은 영화의 평점과 리뷰작성자, 평점, 리뷰글을 수집해주세요.(영화명 : 스파이더맨)\n",
        "# 스크래핑, 막대그래프(날짜별?), 워드클라우드, 전처리, 등 세미 프로젝트 느낌으로 고\n",
        "# 어떤 시각으로 바라볼 것인가? 평점 기준으로 나눠보기, \n",
        "\n",
        "#> 네이버 스크래핑\n",
        "https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=214553&target=after&page=1 # 리뷰 1페이지\n",
        "https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=214553&target=after&page=2 # 리뷰 2페이지\n",
        "# 리뷰페이지마다 번호가 있어서 동적으로 수집을 안 해도됨\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from pandas import Series, DataFrame\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = 'https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=214553&target=after&page=1'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "naver = soup\n",
        "    # 평점\n",
        "soup.select_one('div.list_netizen_score > em').text\n",
        "for i in soup.select('div.list_netizen_score > em'):\n",
        "    print(i.text)\n",
        "\n",
        "    # 리뷰 작성자\n",
        "soup.select_one('td.num > a').text\n",
        "for i in soup.select('td.num > a'):\n",
        "    print(i.text)\n",
        "\n",
        "    # 날짜\n",
        "date = naver.select_one('td[class=\"num\"]')\n",
        "date.select_one('a').extract()\n",
        "date.text\n",
        "for i in soup.select('td[class=\"num\"]'):\n",
        "    i.select_one('a').extract()\n",
        "    print(i.text)\n",
        "    \n",
        "    # 리뷰글\n",
        "review = naver.select_one('td.title')\n",
        "review.select_one('a').extract()\n",
        "review.select_one('div').extract()\n",
        "review.select_one('a').extract()\n",
        "review.text.strip()\n",
        "for i in naver.select('td.title'):\n",
        "    i.select_one('a').extract()\n",
        "    i.select_one('div').extract()\n",
        "    i.select_one('a').extract()\n",
        "    print(i.text)\n",
        "\n",
        "    # 한 페이지의 리뷰, 평점, 작성자, 날짜 dataframe에 저장\n",
        "import time\n",
        "url = 'https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=214553&target=after&page=1'\n",
        "html = urlopen(url)\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "naver = soup\n",
        "naver_review = DataFrame(columns=['작성자','평점','리뷰','날짜'])\n",
        "for j in range(1,11):\n",
        "    url = 'https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=208077&target=after&page='+str(j)\n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    \n",
        "    for i in soup.select('table.list_netizen > colgroup > tbody > tr'):\n",
        "        #평점\n",
        "        score = i.select_one('div.list_netizen_score > em').text\n",
        "        #작성자\n",
        "        writer = i.select_one('td.num > a').text\n",
        "        # 날짜\n",
        "        date = i.select_one('td[class=\"num\"]')\n",
        "        date.select_one('a').extract()\n",
        "        date = date.text\n",
        "        #리뷰글\n",
        "        review = i.select_one('td.title')\n",
        "        review.select_one('a').extract()\n",
        "        review.select_one('div').extract()\n",
        "        review.select_one('a').extract()\n",
        "        review = review.text.strip()\n",
        "        \n",
        "        naver_review = naver_review.append({'작성자':writer,'평점':score,'리뷰':review,'날짜':date},ignore_index=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "naver_review.to_csv('c:/data/naver_movie_spiderman.csv') # 파일 저장\n",
        "\n",
        "#> 다음 스크래핑\n",
        "    # 동적인페이지로 selenium 사용 필요\n",
        "    # 평점 더보기 클릭시 30개씩 더 추가됨\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "url = 'https://movie.daum.net/moviedb/grade?movieId=146656'\n",
        "driver = webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "\n",
        "for i in range(0,3): # 평점 더보기 버튼 3번 누르기\n",
        "    driver.find_element(By.CLASS_NAME,'link_fold').click()\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source # 페이지 소스 추출\n",
        "driver.quit()\n",
        "\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "daum = soup \n",
        "daum\n",
        "\n",
        "    # 평점\n",
        "daum.select_one('div.cmt_info > div.ratings').text\n",
        "for i in daum.select('div.cmt_info > div.ratings'):\n",
        "    print(i.text)\n",
        "    \n",
        "    # 작성자\n",
        "write = daum.select_one(\"div.cmt_info > strong.tit_nick > span.info_author > a \")\n",
        "write.select('span')[1].text\n",
        "for i in daum.select('div.cmt_info > strong.tit_nick > span.info_author > a'):\n",
        "    print(i.select('span')[1].text)\n",
        "    \n",
        "    # 날짜\n",
        "daum.select_one(\"div.cmt_info > strong.tit_nick > span.info_author >  span.txt_date\").text\n",
        "for i in daum.select(\"div.cmt_info > strong.tit_nick > span.info_author >  span.txt_date\"):\n",
        "    print(i.text)\n",
        "\n",
        "    # 리뷰 \n",
        "daum.select_one('div.cmt_info > p.desc_txt').text\n",
        "for i in daum.select('div.cmt_info > p.desc_txt'):\n",
        "    print(i.text)\n",
        "    \n",
        "    # 하나로 종합 데이터프레임에 저장\n",
        "daum_review = DataFrame(columns=['작성자','평점','리뷰','날짜'])\n",
        "for i in daum.select('ul.list_comment > li'):\n",
        "    #평점\n",
        "    score = i.select_one('div.cmt_info > div.ratings').text\n",
        "    # 작성자\n",
        "    writer = i.select_one(\"div.cmt_info > strong.tit_nick > span.info_author > a\")\n",
        "    writer = writer.select('span')[1].text\n",
        "    # 날짜\n",
        "    date = i.select_one(\"div.cmt_info > strong.tit_nick > span.info_author >  span.txt_date\").text\n",
        "    # 리뷰\n",
        "    review = i.select_one('div.cmt_info > p.desc_txt').text.strip()\n",
        "    \n",
        "    daum_review = daum_review.append({'작성자':writer,'평점':score,'리뷰':review,'날짜':date},ignore_index=True)\n",
        "    time.sleep(1)\n",
        "\n",
        "daum_review.to_csv('c:/data/daum_movie_spider.csv')\n",
        "daum_review['작성자']\n",
        "daum_review['평점']\n",
        "daum_review['리뷰']\n",
        "daum_review['날짜']\n",
        "\n",
        "#> 품사 태깅 및 전처리\n",
        "daum_review\n",
        "naver_review\n",
        "    \n",
        "    # 리스트 하나의 text로 합치기(join사용)\n",
        "naver_text = ' '.join(naver_review['리뷰'])\n",
        "daum_text = ' '.join(daum_review['리뷰'])\n",
        "    \n",
        "    # 명사와 형용사만 추출하기(okt, 함수 사용)\n",
        "naver_okt = okt.pos(naver_text)\n",
        "daum_okt = okt.pos(daum_text)\n",
        "\n",
        "def okt_pos(arg):\n",
        "    token = []\n",
        "    for i,j in arg:\n",
        "        if len(i) >= 2:\n",
        "            if j in ('Noun','Adjectiver'):\n",
        "                token.append(i)\n",
        "    return token\n",
        "\n",
        "okt_pos(naver_okt)\n",
        "okt_pos(daum_okt)\n",
        "\n",
        "#> 워드클라우드(함수 사용)\n",
        "Counter(okt_pos(naver_okt))\n",
        "Counter(okt_pos(daum_okt))\n",
        "\n",
        "def word_cloud(arg):        \n",
        "    w = WordCloud(font_path='c:/Windows/Fonts/HMKMAMI.TTF',\n",
        "                  background_color='white').generate_from_frequencies(arg)\n",
        "    plt.imshow(w)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "word_cloud(Counter(okt_pos(naver_okt)))\n",
        "plt.subplot(2,1,2)\n",
        "word_cloud(Counter(okt_pos(daum_okt)))\n",
        "\n",
        "#> 막대그래프\n",
        "from matplotlib import font_manager, rc\n",
        "font_name = font_manager.FontProperties(fname='c:/windows/fonts/HMKMAMI.TTF').get_name()\n",
        "rc('font',family=font_name)\n",
        "    # 네이버 날짜\n",
        "n_date = Counter(naver_review['날짜'])\n",
        "sorted(n_date.items(), key=lambda x : x[1]) # value값으로 정렬\n",
        "    # 다음 날짜\n",
        "d_date = Counter(daum_review['날짜']) # 시분초까지 나와있어서 년월일까지만 나오게 해야함\n",
        "re.search('[\\d+. ]+',daum_review[\"날짜\"][0]).group()\n",
        "daum_date = daum_review['날짜'].apply(lambda x : re.search('[\\d+. ]+ ',x).group())\n",
        "d_date = Counter(daum_date)\n",
        "\n",
        "plt.bar(n_date.keys(),n_date.values())\n",
        "plt.bar(d_date.keys(),d_date.values())\n",
        "plt.barh(range(0,11),n_date.values())\n",
        "plt.yticks(range(0,11),n_date.keys())\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.bar(n_date.keys(),n_date.values())\n",
        "plt.subplot(2,1,2)\n",
        "plt.bar(d_date.keys(),d_date.values())\n",
        "\n",
        "#--------------------------------------------------\n",
        "평점을 기준으로 8점 이상 , 8점 미만으로 나누어서 wordcloud로 표현해서 단어들이 어떤 식으로 나오는지 확인해보기\n",
        "naver_review\n",
        "daum_review\n",
        "\n",
        "naver_review['sentiment'] = ''\n",
        "positive = []\n",
        "negative = []\n",
        "for i in range(0,100):\n",
        "    if int(naver_review['평점'][i]) >= 8:\n",
        "        positive.append(naver_review['리뷰'][i])\n",
        "    else:\n",
        "        negative.append(naver_review['리뷰'][i])\n",
        "d_positive = []\n",
        "d_negative = []\n",
        "for i in range(0,100):\n",
        "    if int(daum_review['평점'][i]) >= 8:\n",
        "        d_positive.append(daum_review['리뷰'][i])\n",
        "    else:\n",
        "        d_negative.append(daum_review['리뷰'][i])\n",
        "\n",
        "naver_positive_text = ' '.join(positive)\n",
        "naver_negative_text = ' '.join(negative)\n",
        "\n",
        "n_positive_word = okt_pos(okt.pos(naver_positive_text))\n",
        "n_negative_word = okt_pos(okt.pos(naver_negative_text))\n",
        "Counter(n_positive_word)\n",
        "Counter(n_negative_word)\n",
        "\n",
        "daum_positive_text = ' '.join(d_positive)\n",
        "daum_negative_text = ' '.join(d_negative)\n",
        "\n",
        "d_positive_word = okt_pos(okt.pos(daum_positive_text))\n",
        "d_negative_word = okt_pos(okt.pos(daum_negative_text))\n",
        "Counter(d_positive_word)\n",
        "Counter(d_negative_word)\n",
        "\n",
        "word_cloud(Counter(n_positive_word))\n",
        "word_cloud(Counter(n_negative_word))\n",
        "\n",
        "# 구분하는데 필요없는 단어(불용어) 제거\n",
        "stop_words = ['영화', '스토리', '스파이더맨','피터','다른','지금','정도','진짜','사람','주인공','우리','한편','토비','악당','배우','마블','장면','파커','홀랜드','시리즈','스파이더','행동','자체','한번','스파','이더','파이','출연','부분','정말','위해','더맨','다시','보고','이번']\n",
        "\n",
        "n_p_w = [i for i in n_positive_word if i not in stop_words]\n",
        "n_n_w = [i for i in n_negative_word if i not in stop_words]\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "word_cloud(Counter(n_p_w))\n",
        "plt.subplot(2,1,2)\n",
        "word_cloud(Counter(n_n_w))\n",
        "▶네이버 영화리뷰 > 불용어를 제외하고 결과를 확인해보면 긍정과 부정의 단어들이 확연히 차이나는 것을 알 수 있다.\n",
        "\n",
        "d_p_w = [i for i in d_positive_word if i not in stop_words]\n",
        "d_n_w = [i for i in d_negative_word if i not in stop_words]\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "word_cloud(Counter(d_p_w))\n",
        "plt.subplot(2,1,2)\n",
        "word_cloud(Counter(d_n_w))\n",
        "▶다음 영화리뷰> 네이버와 마찬가지로 긍정평과 부정평의 단어 차이가 확연히 드러난다.\n",
        "\n",
        "# 다음 영화리뷰 json으로 받아오기 test\n",
        "\n"
      ]
    }
  ]
}